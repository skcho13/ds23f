[
  {
    "objectID": "contents/Visualize/seaborn.html",
    "href": "contents/Visualize/seaborn.html",
    "title": "The seaborn.objects interface",
    "section": "",
    "text": "The seaborn.objects interface\nThe grammer of graphics의 데이터 시각화 이론을 잘 반영하고 있으며 아직 발전 중\n기존 seaborn modules을 완전히 대체하지는 못하므로 필요시 병행하여 사용\n\nv0.12.0 (September 2022)\nIntroduction of the objects interface\nThis release debuts the seaborn.objects interface, an entirely new approach to making plots with seaborn. It is the product of several years of design and 16 months of implementation work. The interface aims to provide a more declarative, composable, and extensible API for making statistical graphics. It is inspired by Wilkinson’s grammar of graphics, offering a Pythonic API that is informed by the design of libraries such as ggplot2 and vega-lite along with lessons from the past 10 years of seaborn’s development.\n\n\n\n\n\n\n\nCaution\n\n\n\nThe objects interface is currently experimental and incomplete. It is stable enough for serious use, but there certainly are some rough edges and missing features.\n\n\n\nThe seaborn.objects interface tutorial\nAPI reference",
    "crumbs": [
      "Visualize",
      "Seaborn.objects"
    ]
  },
  {
    "objectID": "contents/Visualize/customizing.html",
    "href": "contents/Visualize/customizing.html",
    "title": "Customizing",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Visualize",
      "Customizing"
    ]
  },
  {
    "objectID": "contents/Visualize/customizing.html#scales-and-layouts",
    "href": "contents/Visualize/customizing.html#scales-and-layouts",
    "title": "Customizing",
    "section": "Scales and layouts",
    "text": "Scales and layouts\n\npenguins = sns.load_dataset(\"penguins\")\n(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"island\")\n    .facet(col=\"sex\")\n    .add(so.Dot(), so.Jitter(0.5))\n    .scale(color=\"Set2\")  # color palettes: \"Set2\"\n    .layout(size=(7, 5))  # plot size\n)\n\n\n\n\n\n\n\n\n\ndiamonds = sns.load_dataset(\"diamonds\")\n(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\", marker=\"cut\")\n    .add(so.Dots())\n    .scale(\n        color=so.Continuous(\"crest\", norm=(0, 3), trans=\"sqrt\"),\n    )\n)\n\n\n\n\n\n\n\n\nChoosing color palettes",
    "crumbs": [
      "Visualize",
      "Customizing"
    ]
  },
  {
    "objectID": "contents/Visualize/customizing.html#legends-and-ticks",
    "href": "contents/Visualize/customizing.html#legends-and-ticks",
    "title": "Customizing",
    "section": "Legends and ticks",
    "text": "Legends and ticks\n(\n    so.Plot(penguins, x=\"species\")\n    .add(so.Bar(), so.Count())\n    .scale(x=so.Nominal(order=[\"Adelie\", \"Gentoo\", \"Chinstrap\"]))  # x축의 카테고리 순서를 변경\n)\n\n\n\n\n\n\n\n\n(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\")\n    .add(so.Dots())\n    .scale(\n        x=so.Continuous().tick(every=0.5),\n        y=so.Continuous().label(like=\"${x:,.0f}\"),  # %표시: like=\"{x:.1f%}\"\n        color=so.Continuous().tick(at=[1, 2, 3, 4]),\n    )\n)",
    "crumbs": [
      "Visualize",
      "Customizing"
    ]
  },
  {
    "objectID": "contents/Visualize/customizing.html#limits-labels-and-titles",
    "href": "contents/Visualize/customizing.html#limits-labels-and-titles",
    "title": "Customizing",
    "section": "Limits, labels, and titles",
    "text": "Limits, labels, and titles\nPlot has a number of methods for simple customization, including Plot.label(), Plot.limit(), and Plot.share():\n\npenguins = sns.load_dataset(\"penguins\")\n\n(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"island\")\n    .facet(col=\"sex\")\n    .add(so.Dot(), so.Jitter(0.5))\n    .share(x=False)\n    .limit(y=(2.5, -0.5))\n    .label(\n        x=\"Body mass (g)\",\n        y=\"\",\n        color=str.capitalize,\n        title=\"{} penguins\".format,\n    )\n)",
    "crumbs": [
      "Visualize",
      "Customizing"
    ]
  },
  {
    "objectID": "contents/Visualize/customizing.html#themes",
    "href": "contents/Visualize/customizing.html#themes",
    "title": "Customizing",
    "section": "Themes",
    "text": "Themes\nMatplotlib rc parameters을 dictionary로 받음: plt.rcParams.keys()\n\nThemes 추출seaborn.axes_styleMatplotlib의 styles\n\n\n\nSeaborn의 style: sns.axes_style(): 링크\nMatplotlib의 style: plt.style.library[]\n\n\n\nsns.axes_style() # current default\nsns.axes_style(\"darkgrid\")\nsns.axes_style(\"darkgrid\")\n\n# {'figure.facecolor': 'white',\n#  'axes.labelcolor': '.15',\n#  'xtick.direction': 'out',\n#  'ytick.direction': 'out',\n#  'xtick.color': '.15',\n#  'ytick.color': '.15',\n#  'axes.axisbelow': True,\n#  'grid.linestyle': '-',\n#  'text.color': '.15',\n#  'font.family': ['sans-serif'],\n#  'font.sans-serif': ['Arial',\n#   'DejaVu Sans',\n#   'Liberation Sans',\n#   'Bitstream Vera Sans',\n#   'sans-serif'],\n#  'lines.solid_capstyle': 'round',\n#  'patch.edgecolor': 'w',\n#  'patch.force_edgecolor': True,\n#  'image.cmap': 'rocket',\n#  'xtick.top': False,\n#  'ytick.right': False,\n#  'axes.grid': True,\n#  'axes.facecolor': '#EAEAF2',\n#  'axes.edgecolor': 'white',\n#  'grid.color': 'white',\n#  'axes.spines.left': True,\n#  'axes.spines.bottom': True,\n#  'axes.spines.right': True,\n#  'axes.spines.top': True,\n#  'xtick.bottom': False,\n#  'ytick.left': False}\n\n\nplt.style.available\n\n# ['Solarize_Light2',\n#  '_classic_test_patch',\n#  '_mpl-gallery',\n#  '_mpl-gallery-nogrid',\n#  'bmh',\n#  'classic',\n#  'dark_background',\n#  'fast',\n#  'fivethirtyeight',\n#  'ggplot',\n#  'grayscale',\n#  'seaborn-v0_8',\n#  'seaborn-v0_8-bright',\n#  'seaborn-v0_8-colorblind',\n#  'seaborn-v0_8-dark',\n#  'seaborn-v0_8-dark-palette',\n#  'seaborn-v0_8-darkgrid',\n#  'seaborn-v0_8-deep',\n#  'seaborn-v0_8-muted',\n#  'seaborn-v0_8-notebook',\n#  'seaborn-v0_8-paper',\n#  'seaborn-v0_8-pastel',\n#  'seaborn-v0_8-poster',\n#  'seaborn-v0_8-talk',\n#  'seaborn-v0_8-ticks',\n#  'seaborn-v0_8-white',\n#  'seaborn-v0_8-whitegrid',\n#  'tableau-colorblind10']\n가령, matplotlib의 “seaborn-v0_8-whitegrid” 스타일의 specification을 보면,\nplt.style.library[\"seaborn-v0_8-whitegrid\"]\n\n# RcParams({'axes.axisbelow': True,\n#           'axes.edgecolor': '.8',\n#           'axes.facecolor': 'white',\n#           'axes.grid': True,\n#           'axes.labelcolor': '.15',\n#           'axes.linewidth': 1.0,\n#           'figure.facecolor': 'white',\n#           'font.family': ['sans-serif'],\n#           'font.sans-serif': ['Arial',\n#                               'Liberation Sans',\n#                               'DejaVu Sans',\n#                               'Bitstream Vera Sans',\n#                               'sans-serif'],\n#           'grid.color': '.8',\n#           'grid.linestyle': '-',\n#           'image.cmap': 'Greys',\n#           'legend.frameon': False,\n#           'legend.numpoints': 1,\n#           'legend.scatterpoints': 1,\n#           'lines.solid_capstyle': &lt;CapStyle.round: 'round'&gt;,\n#           'text.color': '.15',\n#           'xtick.color': '.15',\n#           'xtick.direction': 'out',\n#           'xtick.major.size': 0.0,\n#           'xtick.minor.size': 0.0,\n#           'ytick.color': '.15',\n#           'ytick.direction': 'out',\n#           'ytick.major.size': 0.0,\n#           'ytick.minor.size': 0.0})\n\n\n\n\np = so.Plot(penguins, x=\"body_mass_g\", y=\"species\", color=\"island\").add(so.Dot(), so.Jitter(0.5))\np.theme(\n    {\n        \"axes.facecolor\": \"white\",\n        \"axes.edgecolor\": \"0.8\",\n        \"axes.spines.top\": False,\n        \"axes.spines.right\": False,\n    }\n)\n\n\n\n\n\n\n\n\n\n# Seaborn axes_style 중 \"whitegrid\"를 적용 & grid linestyle도 조정\np.theme({**sns.axes_style(\"whitegrid\"), \"grid.linestyle\": \":\"})\n\n\n\n\n\n\n\n\nSeaborn: controlling figure aesthetics\n\n# Matplotlib의 스타일 중 \"fivethirtyeight\"을 사용\np.theme({**plt.style.library[\"fivethirtyeight\"]})\n\n\n\n\n\n\n\n\n플랏 단위가 아닌 전체 플랏에 대해 적용하려면,\nPlot.config.theme.update()을 사용: 참고 링크\n\ntheme_dict = {**sns.axes_style(\"whitegrid\"), \"grid.linestyle\": \":\"}\nso.Plot.config.theme.update(theme_dict)\n\n# Reset 하려면\nso.Plot.config.theme.reset()",
    "crumbs": [
      "Visualize",
      "Customizing"
    ]
  },
  {
    "objectID": "contents/Transform/views.html",
    "href": "contents/Transform/views.html",
    "title": "Views and Copies",
    "section": "",
    "text": "Load Packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\nWhat’s new in 2.0.0 (April 3, 2023)\n아래와 같이 copy_on_write을 적용하여 쓰는 것을 추천"
  },
  {
    "objectID": "contents/Transform/views.html#numpy",
    "href": "contents/Transform/views.html#numpy",
    "title": "Views and Copies",
    "section": "NumPy",
    "text": "NumPy\nNumPy에서 subsetting을 하는 경우 view로 나타날 수 있음.\n속도와 메모리의 효율적 관리가 가능하나 혼동의 여지 있음.\n\narr = np.array([1, 2, 3, 4, 5])\narr\n\narray([1, 2, 3, 4, 5])\n\n\n\nsub_arr = arr[1:3]\nsub_arr # view\n\narray([2, 3])\n\n\nSubset을 수정하면\n\nsub_arr[0] = 99\n\n\nprint(arr)\nprint(sub_arr)\n\n[ 1 99  3  4  5]\n[99  3]\n\n\n반대로 “original” arr를 수정하도\n\narr[2] = -11\n\n\nprint(arr)\nprint(sub_arr)\n\n[  1  99 -11   4   5]\n[ 99 -11]\n\n\n사실, arr, sub_arr는 같은 메모리 주소를 reference함\n\n\n\n\n\n\nNote\n\n\n\nView가 되지 않고 copy로 되는 경우가 있음.\nSimple indexing을 제외하면 copy가 된다고 보면 됨\n즉, arr[2] 또는 arr[2:4] 같은 경우는 view로, 그 이외에 integer array로 subsetting을 하거나 (fancy indexing); arr[[2, 3]], 또는 boolean indexing; arr[arr &gt; 2]의 경우 copy가 됨\n\n\n\narr = np.array([1, 2, 3, 4, 5])\n\n\nsub_arr = arr[[2, 3]]  # copy\nsub_arr[0] = 99\n\n\nprint(arr)\nprint(sub_arr)\n\n[1 2 3 4 5]\n[99  4]\n\n\n\nsub_arr = arr[arr &gt; 2]  # copy\nsub_arr[0] = 99\n\n\nprint(arr)\nprint(sub_arr)\n\n[1 2 3 4 5]\n[99  4  5]\n\n\n\n\n\n\n\n\nNote\n\n\n\nAssign operator의 왼편에 [:] 없이, view에서 수정된 array를 assign하면 copy로 전달\n\n\n\narr = np.array([1, 2, 3, 4, 5])\n\n\nsub_arr = arr[1:4]     # view\nsub_arr = sub_arr * 2  # copy\n\n\nprint(arr)\nprint(sub_arr)\n\n[1 2 3 4 5]\n[4 6 8]\n\n\n\narr = np.array([1, 2, 3, 4, 5])\n\n\nsub_arr = arr[1:4]        # view\nsub_arr[:] = sub_arr * 2  # view\n\n\nprint(arr)\nprint(sub_arr)\n\n[1 4 6 8 5]\n[4 6 8]\n\n\n강제로 copy: sub_arr.copy()"
  },
  {
    "objectID": "contents/Transform/views.html#pandas",
    "href": "contents/Transform/views.html#pandas",
    "title": "Views and Copies",
    "section": "pandas",
    "text": "pandas\n훨씬 복잡함…\n데이터 타입도 데이터가 어떻게 만들어졌는지도 관계가 있음.\n\ndf = pd.DataFrame(np.arange(8).reshape(4, 2), columns=[\"one\", \"two\"])\ndf\n\n   one  two\n0    0    1\n1    2    3\n2    4    5\n3    6    7\n\n\n\nsub_df = df.iloc[1:3]  # view\nsub_df\n\n   one  two\n1    2    3\n2    4    5\n\n\n\ndf.iloc[1, 1] = 99\n\n\nprint(df)\nprint(sub_df)\n\n   one  two\n0    0    1\n1    2   99\n2    4    5\n3    6    7\n   one  two\n1    2   99\n2    4    5\n\n\n\n\n\n\n\n\nNote\n\n\n\ncopy_on_write = True (v2.0) 일 때는 copy가 되어\nprint(sub_df)\n#    one  two\n# 1    2    3\n# 2    4    5\n\n\n\ndf.iloc[1, 0] = 0.9  # copy; 컬럼의 데이터 타입이 int에서 float로 바뀌면서 copy됨)\n\n\nprint(df)\nprint(sub_df)\n\n   one  two\n0  0.0    1\n1  0.9   99\n2  4.0    5\n3  6.0    7\n   one  two\n1    2   99\n2    4    5\n\n\n\ndf.iloc[2, 1] = -99  # view\n\n\nprint(df)\nprint(sub_df)\n\n   one  two\n0  0.0    1\n1  0.9   99\n2  4.0  -99\n3  6.0    7\n   one  two\n1    2   99\n2    4  -99\n\n\n\nSettingWithCopyWarning\nSubsetting된 DataFrame을 수정하려할 때 경고를 내어주지만, 항상 믿을만 한 것은 아님.\n경고가 발생할 시, 앞 어디에선가 view나 copy가 이루어진 곳을 찾아 .copy()로 수정\n\ndf = pd.DataFrame(np.arange(12).reshape(4, 3), columns=[\"one\", \"two\", \"three\"])\ndf\n\n   one  two  three\n0    0    1      2\n1    3    4      5\n2    6    7      8\n3    9   10     11\n\n\n\ndf_cols = df[[\"two\", \"three\"]]  # copy\ndf_cols\n\n   two  three\n0    1      2\n1    4      5\n2    7      8\n3   10     11\n\n\n\ndf.iloc[0, 1] = -55\n\n\nprint(df)\nprint(df_cols)\n\n   one  two  three\n0    0  -55      2\n1    3    4      5\n2    6    7      8\n3    9   10     11\n   two  three\n0    1      2\n1    4      5\n2    7      8\n3   10     11\n\n\nSubset을 수정하려하면 warning message!\ncopy_on_save가 True일 때는 copy가 되면서 경고가 발생하지 않음 (v2.0)\n\ndf_cols.iloc[0, 1] = -99\n\n/var/folders/tv/fwb_421x50z8bj5v37vw680r0000gn/T/ipykernel_1691/2609376290.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cols.iloc[0, 1] = -99\n\n\n\nprint(df_cols)\nprint(df)\n\n   two  three\n0    1    -99\n1    4      5\n2    7      8\n3   10     11\n   one  two  three\n0    0  -55      2\n1    3    4      5\n2    6    7      8\n3    9   10     11\n\n\n다음과 비교\n\ndf = pd.DataFrame(np.arange(12).reshape(4, 3), columns=[\"one\", \"two\", \"three\"])\ndf\n\n   one  two  three\n0    0    1      2\n1    3    4      5\n2    6    7      8\n3    9   10     11\n\n\n\ndf_cols_3 = df.loc[:, [\"two\", \"three\"]]  # copy\ndf_cols_3.iloc[0, 1] = -99\n\n# No warning\n\n\nprint(df_cols_3)\nprint(df)\n\n   two  three\n0    1    -99\n1    4      5\n2    7      8\n3   10     11\n   one  two  three\n0    0    1      2\n1    3    4      5\n2    6    7      8\n3    9   10     11\n\n\n\ndf = pd.DataFrame(np.arange(12).reshape(4, 3), columns=[\"one\", \"two\", \"three\"])\ndf\n\n   one  two  three\n0    0    1      2\n1    3    4      5\n2    6    7      8\n3    9   10     11\n\n\n\ndf_cols_2 = df.loc[:, \"two\":\"three\"]  # view\ndf_cols_2.iloc[0, 1] = -99\n\n/var/folders/tv/fwb_421x50z8bj5v37vw680r0000gn/T/ipykernel_1691/4245876664.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cols_2.iloc[0, 1] = -99\n\n\n\nprint(df_cols_2)\nprint(df)\n\n   two  three\n0    1    -99\n1    4      5\n2    7      8\n3   10     11\n   one  two  three\n0    0    1    -99\n1    3    4      5\n2    6    7      8\n3    9   10     11\n\n\n\n\n\n\n\n\nNote\n\n\n\ncopy_on_save가 True일 때는 copy가 되면서 경고가 발생하지 않음 (v2.0)\nprint(df)\n#    one  two  three\n# 0    0    1      2\n# 1    3    4      5\n# 2    6    7      8\n# 3    9   10     11\n\n\n강제로 copy: df_cols.copy()\n\ndf_cols_4 = df[[\"two\", \"three\"]].copy()\ndf_cols_4.iloc[0, 1] = -99\n\n\n\n\n\n\n\nTip\n\n\n\nSubset을 만들고 바로 분석을 할 것이 아니라면, 안전하게 .copy()를 쓰는 것을 추천"
  },
  {
    "objectID": "contents/Transform/subsetting.html",
    "href": "contents/Transform/subsetting.html",
    "title": "Subsetting",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nData: On-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013\n# import the dataset\nflights_data = sm.datasets.get_rdataset(\"flights\", \"nycflights13\")\nflights = flights_data.data\nflights = flights.drop(columns=\"time_hour\")  # drop the \"time_hour\" column\n# Description\nprint(flights_data.__doc__)\nflights\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0       2013      1    1    517.00             515       2.00    830.00   \n1       2013      1    1    533.00             529       4.00    850.00   \n2       2013      1    1    542.00             540       2.00    923.00   \n...      ...    ...  ...       ...             ...        ...       ...   \n336773  2013      9   30       NaN            1210        NaN       NaN   \n336774  2013      9   30       NaN            1159        NaN       NaN   \n336775  2013      9   30       NaN             840        NaN       NaN   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n0                  819      11.00      UA    1545  N14228    EWR  IAH   \n1                  830      20.00      UA    1714  N24211    LGA  IAH   \n2                  850      33.00      AA    1141  N619AA    JFK  MIA   \n...                ...        ...     ...     ...     ...    ...  ...   \n336773            1330        NaN      MQ    3461  N535MQ    LGA  BNA   \n336774            1344        NaN      MQ    3572  N511MQ    LGA  CLE   \n336775            1020        NaN      MQ    3531  N839MQ    LGA  RDU   \n\n        air_time  distance  hour  minute  \n0         227.00      1400     5      15  \n1         227.00      1416     5      29  \n2         160.00      1089     5      40  \n...          ...       ...   ...     ...  \n336773       NaN       764    12      10  \n336774       NaN       419    11      59  \n336775       NaN       431     8      40  \n\n[336776 rows x 18 columns]\nSubsetting options",
    "crumbs": [
      "Transform",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/Transform/subsetting.html#bracket",
    "href": "contents/Transform/subsetting.html#bracket",
    "title": "Subsetting",
    "section": "Bracket [ ]",
    "text": "Bracket [ ]\nBracket안에 labels이 있는 경우 columns을 select\n\nA single string: Series로 반환\n\nA list of a single string: DataFrame으로 반환\n\nA list of strings\n\n\nflights['dest']  # return as a Series\n\n0         IAH\n1         IAH\n2         MIA\n         ... \n336773    BNA\n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object\n\n\n\nflights[['dest']]  # return as a DataFrame\n\n       dest\n0       IAH\n1       IAH\n2       MIA\n...     ...\n336773  BNA\n336774  CLE\n336775  RDU\n\n[336776 rows x 1 columns]\n\n\n\nflights[['origin', 'dest']]\n\n       origin dest\n0         EWR  IAH\n1         LGA  IAH\n2         JFK  MIA\n...       ...  ...\n336773    LGA  BNA\n336774    LGA  CLE\n336775    LGA  RDU\n\n[336776 rows x 2 columns]\n\n\nBracket안에 numbers가 있는 경우 rows를 select: position-based\n\nSlicing만 허용\nFirst index는 포함, last index는 제외\n[1, 5, 8]과 같이 특정 rows를 선택하는 것은 허용안됨\n\n\nflights[2:5]\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00   \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00   \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00   \n\n   distance  hour  minute  \n2      1089     5      40  \n3      1576     5      45  \n4       762     6       0  \n\n\n 만약, 아래와 같이 index가 number일 때 out of order가 된 경우에도 row position으로 적용됨\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder[2:4]\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n Chaining with brackets\n\nflights[['origin', 'dest']][2:5]\n# 순서 바꿔어도 동일: flights[2:5][['origin', 'dest']]\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL",
    "crumbs": [
      "Transform",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/Transform/subsetting.html#dot-notation-.",
    "href": "contents/Transform/subsetting.html#dot-notation-.",
    "title": "Subsetting",
    "section": "Dot notation .",
    "text": "Dot notation .\n편리하나 주의해서 사용할 필요가 있음\n\n\n\n\n\n\nNote\n\n\n\n\nspace 또는 . 이 있는 변수명 사용 불가\nmethods와 동일한 이름의 변수명 사용 불가: 예) 변수명이 count인 경우 df.count는 df의 method로 인식\n새로운 변수를 만들어 값을 assgin할 수 없음: 예) df.new_var = 1 불가; 대신 df[\"new_var\"] = 1\n만약, 다음과 같이 변수을 지정했을 때 vars_names=[\"origin\", \"dest\"],\n\ndf[vars_names]는 \"orign\"과 \"dest\" columns을 선택\ndf.vars_names는 vars_names이라는 이름의 column을 의미\n\n\n\n\n\nflights.dest  # flihgts[\"dest\"]와 동일\n\n0         IAH\n1         IAH\n2         MIA\n         ... \n336773    BNA\n336774    CLE\n336775    RDU\nName: dest, Length: 336776, dtype: object",
    "crumbs": [
      "Transform",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/Transform/subsetting.html#loc-.iloc",
    "href": "contents/Transform/subsetting.html#loc-.iloc",
    "title": "Subsetting",
    "section": ".loc & .iloc",
    "text": ".loc & .iloc\n각각 location, integer location의 약자\ndf.(i)loc[row_indexer, column_indexer]\n\n.loc: label-based indexing\n\nIndex가 number인 경우도 label로 처리\nSlicing의 경우 first, last index 모두 inclusive\n\n\nflights.loc[2:5, ['origin', 'dest']]  # 2:5는 index의 label, not position\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n5    EWR  ORD\n\n\n다음과 같이 index가 labels인 경우는 혼동의 염려 없음\n\n\n       origin dest\nred       JFK  MIA\nblue      JFK  BQN\ngreen     LGA  ATL\nyellow    EWR  ORD\n\n\n\ndf_labels.loc[\"blue\":\"green\", :]\n\n      origin dest\nblue     JFK  BQN\ngreen    LGA  ATL\n\n\n하지만, index가 number인 경우는 혼동이 있음\n앞서 본 예에서처럼 index가 out of order인 경우 loc은 다르게 작동\n\n\n   origin dest  arr_delay\n42    LGA  DFW      48.00\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n33    EWR  MSP      29.00\n\n\n\ndf_outoforder.loc[2:14, :]  # position 아님\n\n   origin dest  arr_delay\n2     JFK  MIA      33.00\n25    EWR  ORD      32.00\n14    LGA  DFW      31.00\n\n\n\ndf_outoforder.loc[[25, 33], :]  # slicing이 아닌 특정 index 선택\n\n   origin dest  arr_delay\n25    EWR  ORD      32.00\n33    EWR  MSP      29.00\n\n\n\nflights.loc[2:5, 'dest']  # returns as a Series\n\n2    MIA\n3    BQN\n4    ATL\n5    ORD\nName: dest, dtype: object\n\n\n\nflights.loc[2:5, ['dest']]  # return as a DataFrame\n\n  dest\n2  MIA\n3  BQN\n4  ATL\n5  ORD\n\n\n\n\n\n\n\n\nTip\n\n\n\n생략 표시\nflights.loc[2:5, :]  # ':' means all\nflights.loc[2:5]\nflights.loc[2:5, ]  # flights.loc[ , ['dest', 'origin']]은 에러\n\n\n\n# select a single row\nflights.loc[2, :]  # returns as a Series, column names as its index\n\nyear        2013\nmonth          1\nday            1\n            ... \ndistance    1089\nhour           5\nminute        40\nName: 2, Length: 18, dtype: object\n\n\n\n# select a single row\nflights.loc[[2], :]  # returns as a DataFrame\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00   \n\n   distance  hour  minute  \n2      1089     5      40  \n\n\n\n\n\n.iloc: position-based indexing\n\nSlicing의 경우 as usual: first index는 inclusive, last index는 exclusive\n\n\nflights.iloc[2:5, 12:14]  # 2:5는 index의 position, last index는 미포함\n\n  origin dest\n2    JFK  MIA\n3    JFK  BQN\n4    LGA  ATL\n\n\n\nflights.iloc[2:5, 12]  # return as a Series\n\n2    JFK\n3    JFK\n4    LGA\nName: origin, dtype: object\n\n\n\nflights.iloc[2:5, :]\n# 다음 모두 가능\n# flights.iloc[2:5]\n# flights.iloc[2:5, ]\n\n# flights.iloc[, 2:5]는 에러\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2  2013      1    1    542.00             540       2.00    923.00   \n3  2013      1    1    544.00             545      -1.00   1004.00   \n4  2013      1    1    554.00             600      -6.00    812.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00   \n3            1022     -18.00      B6     725  N804JB    JFK  BQN    183.00   \n4             837     -25.00      DL     461  N668DN    LGA  ATL    116.00   \n\n   distance  hour  minute  \n2      1089     5      40  \n3      1576     5      45  \n4       762     6       0  \n\n\n\nflights.iloc[2:5, [12]]  # return as a DataFrame\n\n  origin\n2    JFK\n3    JFK\n4    LGA\n\n\n\nflights.iloc[[2, 5, 7], 12:14]  # 특정 위치의 rows 선택\n\n  origin dest\n2    JFK  MIA\n5    EWR  ORD\n7    LGA  IAD\n\n\n\n\n\n\n\n\nNote\n\n\n\n단 하나의 scalar 값을 추출할 때, 빠른 처리를 하는 다음을 사용할 수 있음\n.at[i, j], .iat[i, j]",
    "crumbs": [
      "Transform",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/Transform/subsetting.html#series의-indexing",
    "href": "contents/Transform/subsetting.html#series의-indexing",
    "title": "Subsetting",
    "section": "Series의 indexing",
    "text": "Series의 indexing\nDataFrame과 같은 방식으로 이해\nIndex가 numbers인 경우\n\n\n42    DFW\n2     MIA\n25    ORD\n14    DFW\n33    MSP\nName: dest, dtype: object\n\n\n\ns.loc[25:14]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns.iloc[2:4]\n\n25    ORD\n14    DFW\nName: dest, dtype: object\n\n\n\ns[:3]\n\n42    DFW\n2     MIA\n25    ORD\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n다음과 같은 경우 혼동스러움\ns[3] # 3번째? label 3?\n#&gt; errors occur\n\n\n Index가 lables인 경우 다음과 같이 편리하게 subsetting 가능\n\n\nred       MIA\nblue      BQN\ngreen     ATL\nyellow    ORD\nName: dest, dtype: object\n\n\n\ns[\"red\":\"green\"]\n\nred      MIA\nblue     BQN\ngreen    ATL\nName: dest, dtype: object\n\n\n\ns[[\"red\", \"green\"]]\n\nred      MIA\ngreen    ATL\nName: dest, dtype: object",
    "crumbs": [
      "Transform",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/Transform/subsetting.html#boolean-indexing",
    "href": "contents/Transform/subsetting.html#boolean-indexing",
    "title": "Subsetting",
    "section": "Boolean indexing",
    "text": "Boolean indexing\n\nBracket [ ] 이나 loc을 이용\niloc은 적용 안됨\n\n\nBracket [ ]\n\nnp.random.seed(123)\nflights_6 = flights[:100][[\"dep_delay\", \"arr_delay\", \"origin\", \"dest\"]].sample(6)\nflights_6\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n70       9.00      20.00    LGA  ORD\n82      -1.00     -26.00    JFK  SFO\n28       0.00     -21.00    JFK  SJU\n63      -2.00       2.00    JFK  LAX\n0        2.00      11.00    EWR  IAH\n\n\n\nflights_6[flights_6[\"dep_delay\"] &lt; 0]\n\n    dep_delay  arr_delay origin dest\n8       -3.00      -8.00    JFK  MCO\n82      -1.00     -26.00    JFK  SFO\n63      -2.00       2.00    JFK  LAX\n\n\n\nidx = flights_6[\"dep_delay\"] &lt; 0\nidx # bool type의 Series\n\n8      True\n70    False\n82     True\n28    False\n63     True\n0     False\nName: dep_delay, dtype: bool\n\n\n\n# Select a column with the boolean indexing\nflights_6[idx][\"dest\"]\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\n사실, boolean indexing을 할때, DataFrame/Series의 index와 match함\n대부분 염려하지 않아도 되나 다음과 같은 결과 참고\n# Reset index\nidx_reset = idx.reset_index(drop=True)\n# 0     True\n# 1    False\n# 2     True\n# 3    False\n# 4     True\n# 5    False\n# Name: dep_delay, dtype: bool\n\nflights_6[idx_reset][\"dest\"]\n#&gt; IndexingError: Unalignable boolean Series provided as indexer \n#&gt; (index of the boolean Series and of the indexed object do not match)\n\n# Index가 없는 numpy array로 boolean indexing을 하는 경우 문제없음\nflights_6[idx_reset.to_numpy()][\"dest\"]\n# 8     MCO\n# 82    SFO\n# 63    LAX\n# Name: dest, dtype: object\n\n\n\nbool_idx = flights_6[[\"dep_delay\", \"arr_delay\"]] &gt; 0\nbool_idx\n\n    dep_delay  arr_delay\n8       False      False\n70       True       True\n82      False      False\n28      False      False\n63      False       True\n0        True       True\n\n\n\nidx_any = bool_idx.any(axis=1)\nidx_any\n\n8     False\n70     True\n82    False\n28    False\n63     True\n0      True\ndtype: bool\n\n\n\nbool_idx.all(axis=1)\n\n8     False\n70     True\n82    False\n28    False\n63    False\n0      True\ndtype: bool\n\n\n\n\nnp.where() 활용\nnp.where(boolean condition, value if True, value if False)\n\nflights_6[\"delayed\"] = np.where(idx, \"delayed\", \"on-time\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed\n8       -3.00      -8.00    JFK  MCO  delayed\n70       9.00      20.00    LGA  ORD  on-time\n82      -1.00     -26.00    JFK  SFO  delayed\n28       0.00     -21.00    JFK  SJU  on-time\n63      -2.00       2.00    JFK  LAX  delayed\n0        2.00      11.00    EWR  IAH  on-time\n\n\n\nnp.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")  # str method: \"S\"로 시작하는지 여부\n\narray(['T', 'T', 'S', 'S', 'T', 'T'], dtype='&lt;U1')\n\n\n\nflights_6[\"dest_S\"] = np.where(flights_6[\"dest\"].str.startswith(\"S\"), \"S\", \"T\")\nflights_6\n\n    dep_delay  arr_delay origin dest  delayed dest_S\n8       -3.00      -8.00    JFK  MCO  delayed      T\n70       9.00      20.00    LGA  ORD  on-time      T\n82      -1.00     -26.00    JFK  SFO  delayed      S\n28       0.00     -21.00    JFK  SJU  on-time      S\n63      -2.00       2.00    JFK  LAX  delayed      T\n0        2.00      11.00    EWR  IAH  on-time      T\n\n\n\n\nloc\n\nflights_6.loc[idx, \"dest\"]  # flights_6[idx][\"dest\"]과 동일\n\n8     MCO\n82    SFO\n63    LAX\nName: dest, dtype: object\n\n\n만약 column 이름에 “time”을 포함하는 columns만 선택하고자 하면\n\nSeries/Index object는 str method 존재\nstr.contains(), str.startswith(), str.endswith()\n자세한 사항은 7.4 String Manipulation/String Functions in pandas by Wes McKinney\n\n\ncols = flights.columns.str.contains(\"time\")  # str method: \"time\"을 포함하는지 여부\ncols\n\narray([False, False, False,  True,  True, False,  True,  True, False,\n       False, False, False, False, False,  True, False, False, False])\n\n\n\n# Columns 쪽으로 boolean indexing\nflights.loc[:, cols]\n\n        dep_time  sched_dep_time  arr_time  sched_arr_time  air_time\n0         517.00             515    830.00             819    227.00\n1         533.00             529    850.00             830    227.00\n2         542.00             540    923.00             850    160.00\n...          ...             ...       ...             ...       ...\n336773       NaN            1210       NaN            1330       NaN\n336774       NaN            1159       NaN            1344       NaN\n336775       NaN             840       NaN            1020       NaN\n\n[336776 rows x 5 columns]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nChained indexing으로 값을 assign하는 경우 copy vs. view 경고 메세지\nflights[flights[\"arr_delay\"] &lt; 0][\"arr_delay\"] = 0\n/var/folders/mp/vcywncl97ml2q4c_5k2r573m0000gn/T/ipykernel_96692/3780864177.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n 경고가 제시하는데로 .loc을 이용하여 assign\nflights.loc[flights[\"arr_delay\"] &lt; 0, \"arr_delay\"] = 0",
    "crumbs": [
      "Transform",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/Transform/subsetting.html#summary",
    "href": "contents/Transform/subsetting.html#summary",
    "title": "Subsetting",
    "section": "Summary",
    "text": "Summary\n\nBracket [ ]의 경우\n\n간단히 columns을 선택하고자 할때 column labels: df[[\"var1\", \"var2\"]]\n간단히 rows를 선택하고자 할때 numerical indexing: df[:10]\n\nDot-notation은\n\npandas의 methods와 중복된 이름을 피하고,\nassignment의 왼편에는 사용을 피할 것\n\n가능하면 분명한 loc 또는 iloc을 사용\n\nloc[:, [\"var1\", \"var2\"]]는 df[[\"var1\", \"var2\"]]과 동일\niloc[:10, :]은 df[:10]와 동일\nloc의 경우, index가 숫자라 할지라도 label로 처리됨\nloc은 iloc과는 다른게 slicing(:)에서 first, last index 모두 inclusive\n\nBoolean indexing의 경우\n\nBracket [ ]: df[bool_idx]\nloc: df.loc[bool_idx, :]\niloc 불가\n\nAssignment를 할때는,\n\nchained indexing을 피하고: df[:5][\"dest\"]\nloc or iloc 사용:\n\ndf.loc[:4, \"dest\"]: index가 0부터 정렬되어 있다고 가정했을 때, slicing에서 위치 하나 차이남\ndf.iloc[:5, 13]: “dest”의 column 위치 13\n\n\n한 개의 column 혹은 row을 선택하면 Series로 반환: df[\"var1\"] 또는 df.loc[2, :]\n\n\n\n\n\n\n\nNote\n\n\n\nNumpy의 indexing에 대해서는 교재 참고\nCh.4/Basic Indexing and Slicing in Python Data Analysis by Wes McKinney",
    "crumbs": [
      "Transform",
      "Subsetting"
    ]
  },
  {
    "objectID": "contents/Transform/sol1.html",
    "href": "contents/Transform/sol1.html",
    "title": "Transform 1 Sol",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Solutions",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Transform/sol1.html#a",
    "href": "contents/Transform/sol1.html#a",
    "title": "Transform 1 Sol",
    "section": "A",
    "text": "A\n\n# 1. Had an arrival delay of two or more hours\n# Load the nycflight13 dataset\nflights = sm.datasets.get_rdataset(\"flights\", \"nycflights13\").data.drop(columns=\"time_hour\")\n\n\nflights.query('arr_delay &gt; 120').head(3)\n\n     year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n119  2013      1    1    811.00             630     101.00   1047.00   \n151  2013      1    1    848.00            1835     853.00   1001.00   \n218  2013      1    1    957.00             733     144.00   1056.00   \n\n     sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n119             830     137.00      MQ    4576  N531MQ    LGA  CLT    118.00   \n151            1950     851.00      MQ    3944  N942MQ    JFK  BWI     41.00   \n218             853     123.00      UA     856  N534UA    EWR  BOS     37.00   \n\n     distance  hour  minute  \n119       544     6      30  \n151       184    18      35  \n218       200     7      33  \n\n\n\n# 2. Flew to Houston (IAH or HOU)\nflights.query('dest == \"IAH\" | dest == \"HOU\"').head(3)\n\n    year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0   2013      1    1    517.00             515       2.00    830.00   \n1   2013      1    1    533.00             529       4.00    850.00   \n32  2013      1    1    623.00             627      -4.00    933.00   \n\n    sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n0              819      11.00      UA    1545  N14228    EWR  IAH    227.00   \n1              830      20.00      UA    1714  N24211    LGA  IAH    227.00   \n32             932       1.00      UA     496  N459UA    LGA  IAH    229.00   \n\n    distance  hour  minute  \n0       1400     5      15  \n1       1416     5      29  \n32      1416     6      27  \n\n\n\n# 3. Departed in summer (July, August, and September)\nflights.query('month in [7, 8, 9]').head(3)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n250450  2013      7    1      1.00            2029     212.00    236.00   \n250451  2013      7    1      2.00            2359       3.00    344.00   \n250452  2013      7    1     29.00            2245     104.00    151.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n250450            2359     157.00      B6     915  N653JB    JFK  SFO   \n250451             344       0.00      B6    1503  N805JB    JFK  SJU   \n250452               1     110.00      B6     234  N348JB    JFK  BTV   \n\n        air_time  distance  hour  minute  \n250450    315.00      2586    20      29  \n250451    200.00      1598    23      59  \n250452     66.00       266    22      45  \n\n\n\n# 4. Arrived more than two hours late, but didn’t leave late\nflights.query('arr_delay &gt; 120 & dep_delay &lt;= 0').head(3)\n\n       year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n22911  2013      1   27   1419.00            1420      -1.00   1754.00   \n33011  2013     10    7   1350.00            1350       0.00   1736.00   \n33019  2013     10    7   1357.00            1359      -2.00   1858.00   \n\n       sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n22911            1550     124.00      MQ    3728  N1EAMQ    EWR  ORD   \n33011            1526     130.00      EV    5181  N611QX    LGA  MSN   \n33019            1654     124.00      AA    1151  N3CMAA    LGA  DFW   \n\n       air_time  distance  hour  minute  \n22911    135.00       719    14      20  \n33011    117.00       812    13      50  \n33019    192.00      1389    13      59  \n\n\n\n# 5. Were delayed by at least an hour, but made up over 30 minutes in flight\nflights.query('dep_delay &gt; 60 & arr_delay - dep_delay &lt; -30').head(5)\n\n      year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n815   2013      1    1   2205.00            1720     285.00     46.00   \n832   2013      1    1   2326.00            2130     116.00    131.00   \n2286  2013      1    3   1503.00            1221     162.00   1803.00   \n2508  2013      1    3   1839.00            1700      99.00   2056.00   \n2522  2013      1    3   1850.00            1745      65.00   2148.00   \n\n      sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n815             2040     246.00      AA    1999  N5DNAA    EWR  MIA    146.00   \n832               18      73.00      B6     199  N594JB    JFK  LAS    290.00   \n2286            1555     128.00      UA     551  N835UA    EWR  SFO    320.00   \n2508            1950      66.00      AA     575  N631AA    JFK  EGE    239.00   \n2522            2120      28.00      AA     177  N332AA    JFK  SFO    314.00   \n\n      distance  hour  minute  \n815       1085    17      20  \n832       2248    21      30  \n2286      2565    12      21  \n2508      1747    17       0  \n2522      2586    17      45  \n\n\n\n# 6. Departed between midnight and 6am (inclusive)\nflights.query('dep_time &gt;= 0 & dep_time &lt;= 600').head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1    517.00             515       2.00    830.00   \n1  2013      1    1    533.00             529       4.00    850.00   \n2  2013      1    1    542.00             540       2.00    923.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n0             819      11.00      UA    1545  N14228    EWR  IAH    227.00   \n1             830      20.00      UA    1714  N24211    LGA  IAH    227.00   \n2             850      33.00      AA    1141  N619AA    JFK  MIA    160.00   \n\n   distance  hour  minute  \n0      1400     5      15  \n1      1416     5      29  \n2      1089     5      40  \n\n\n\n# 7. Find the fastest flights.\n(\n    flights.assign(speed=flights.distance / flights.air_time)\n    .sort_values(by=\"speed\", ascending=False)\n    .head(7)\n)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n216447  2013      5   25   1709.00            1700       9.00   1923.00   \n251999  2013      7    2   1558.00            1513      45.00   1745.00   \n205388  2013      5   13   2040.00            2025      15.00   2225.00   \n157516  2013      3   23   1914.00            1910       4.00   2045.00   \n10223   2013      1   12   1559.00            1600      -1.00   1849.00   \n70640   2013     11   17    650.00             655      -5.00   1059.00   \n129835  2013      2   21   2355.00            2358      -3.00    412.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n216447            1937     -14.00      DL    1499  N666DN    LGA  ATL   \n251999            1719      26.00      EV    4667  N17196    EWR  MSP   \n205388            2226      -1.00      EV    4292  N14568    EWR  GSP   \n157516            2043       2.00      EV    3805  N12567    EWR  BNA   \n10223             1917     -28.00      DL    1902  N956DL    LGA  PBI   \n70640             1150     -51.00      DL     315   N3768    JFK  SJU   \n129835             438     -26.00      B6     707  N779JB    JFK  SJU   \n\n        air_time  distance  hour  minute  speed  \n216447     65.00       762    17       0  11.72  \n251999     93.00      1008    15      13  10.84  \n205388     55.00       594    20      25  10.80  \n157516     70.00       748    19      10  10.69  \n10223     105.00      1035    16       0   9.86  \n70640     170.00      1598     6      55   9.40  \n129835    172.00      1598    23      58   9.29  \n\n\n\n# 8. Sort flights to find the most delayed flights. Find the flights that left earliest.\nflights.sort_values(by=\"dep_delay\", ascending=False).head(3)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n7072    2013      1    9    641.00             900    1301.00   1242.00   \n235778  2013      6   15   1432.00            1935    1137.00   1607.00   \n8239    2013      1   10   1121.00            1635    1126.00   1239.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n7072              1530    1272.00      HA      51  N384HA    JFK  HNL   \n235778            2120    1127.00      MQ    3535  N504MQ    JFK  CMH   \n8239              1810    1109.00      MQ    3695  N517MQ    EWR  ORD   \n\n        air_time  distance  hour  minute  \n7072      640.00      4983     9       0  \n235778     74.00       483    19      35  \n8239      111.00       719    16      35  \n\n\n\nflights.sort_values(by=\"dep_delay\", ascending=True).head(3)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n89673   2013     12    7   2040.00            2123     -43.00     40.00   \n113633  2013      2    3   2022.00            2055     -33.00   2240.00   \n64501   2013     11   10   1408.00            1440     -32.00   1549.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n89673             2352      48.00      B6      97  N592JB    JFK  DEN   \n113633            2338     -58.00      DL    1715  N612DL    LGA  MSY   \n64501             1559     -10.00      EV    5713  N825AS    LGA  IAD   \n\n        air_time  distance  hour  minute  \n89673     265.00      1626    21      23  \n113633    162.00      1183    20      55  \n64501      52.00       229    14      40  \n\n\n\n# 9. Which flights travelled the farthest? \nflights.sort_values(by=\"distance\", ascending=False).head(3)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n50676   2013     10   26   1004.00            1000       4.00   1435.00   \n108078  2013     12   28    933.00             930       3.00   1520.00   \n100067  2013     12   19    924.00             930      -6.00   1450.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n50676             1450     -15.00      HA      51  N386HA    JFK  HNL   \n108078            1535     -15.00      HA      51  N384HA    JFK  HNL   \n100067            1535     -45.00      HA      51  N386HA    JFK  HNL   \n\n        air_time  distance  hour  minute  \n50676     608.00      4983    10       0  \n108078    633.00      4983     9      30  \n100067    609.00      4983     9      30  \n\n\n\n# Which travelled the shortest?\nflights.sort_values(by=\"distance\", ascending=True).head(3)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n275945  2013      7   27       NaN             106        NaN       NaN   \n3083    2013      1    4   1240.00            1200      40.00   1333.00   \n16328   2013      1   19   1617.00            1617       0.00   1722.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n275945             245        NaN      US    1632     NaN    EWR  LGA   \n3083              1306      27.00      EV    4193  N14972    EWR  PHL   \n16328             1722       0.00      EV    4616  N12540    EWR  PHL   \n\n        air_time  distance  hour  minute  \n275945       NaN        17     1       6  \n3083       30.00        80    12       0  \n16328      34.00        80    16      17  \n\n\n\n# 10. 각 도착지로 출항하는 항공편이 1년 중 몇 일 있는가?\n(\n    flights.groupby([\"month\", \"day\", \"dest\"])\n    .size()\n    .reset_index(name='n')\n    .groupby(\"dest\")\n    .size()\n)\n\ndest\nABQ    254\nACK    155\nALB    260\n      ... \nTVC     37\nTYS    322\nXNA    314\nLength: 105, dtype: int64\n\n\n\n# 11. 1년 중 300일 이상 출항하는 도착지들을 구하면?\n(\n    flights.groupby([\"month\", \"day\", \"dest\"])\n    .size()\n    .reset_index(name='n')\n    .groupby(\"dest\")\n    .size()\n    .reset_index(name=\"n\")\n    .query('n &gt;= 300')\n)\n\n    dest    n\n4    ATL  365\n5    AUS  365\n10   BNA  365\n..   ...  ...\n101  TUL  314\n103  TYS  322\n104  XNA  314\n\n[78 rows x 2 columns]",
    "crumbs": [
      "Solutions",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Transform/sol1.html#b.",
    "href": "contents/Transform/sol1.html#b.",
    "title": "Transform 1 Sol",
    "section": "B.",
    "text": "B.\n\n1.\n\n# 1. Our definition of cancelled flights (dep_delay or arr_delay is missing) is slightly suboptimal. Why? Which is the most important column?\n# 예를 들어, 출발지연은 missing이 아니나 도착지연은 missing인 것이 있음\nflights.query(\"dep_delay.isna() and not arr_delay.isna()\")[\n    [\"dep_time\", \"arr_time\", \"dep_delay\", \"arr_delay\"]\n]\n\nEmpty DataFrame\nColumns: [dep_time, arr_time, dep_delay, arr_delay]\nIndex: []\n\n\n즉, 출발지연이 misssing이면 도착지연도 missing임.\n\nflights.query(\"not dep_delay.isna() and arr_delay.isna()\")[\n    [\"dep_time\", \"arr_time\", \"dep_delay\", \"arr_delay\"]\n]\n\n        dep_time  arr_time  dep_delay  arr_delay\n471      1525.00   1934.00      -5.00        NaN\n477      1528.00   2002.00      29.00        NaN\n615      1740.00   2158.00      -5.00        NaN\n...          ...       ...        ...        ...\n334495   1214.00   1801.00     -11.00        NaN\n335534   1734.00   2159.00      23.00        NaN\n335805    559.00       NaN      -1.00        NaN\n\n[1175 rows x 4 columns]\n\n\n출발지연은 missing이 아니나 도작지연은 missing인 것이 있음.\n아마도 도착지연이 missing인 경우는 결항은 아니고, 출발지연이 missing인 것이 결항된 항공편이라고 볼 수 있음. (출발지연이 missing이면 도착지연도 missing이므로)\n도착지연이 더 중요한 지표일 것임; 연결된 항공편을 놓칠 수 있기 때문에. 출발지연은 오히려 좋을 수도..\n\n\n2.\n\n# 2. Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the (daily) average delay?\n# 취소되는 항공편들이 많은 것과 관계 있는 것은 무엇이 있을까…\n\n\n# 출항한 항공편이 많을수록 결항편도 많음.. 당연? 선형관계?  &gt;&gt; 결항 비율로\ncancelled_per_day = (\n    flights.assign(\n        cancelled = lambda x: x.dep_delay.isna() | x.arr_delay.isna())  # \"|\": bitwise \"or\"\n    .groupby([\"month\", \"day\"])[\"cancelled\"]\n    .agg([\"sum\", \"count\"])  # sum(boolean) = True의 개수, mean(boolean) = True의 비율\n)\ncancelled_per_day.head(7)\n\n           sum  count\nmonth day            \n1     1     11    842\n      2     15    943\n      3     14    914\n      4      7    915\n      5      3    720\n      6      3    832\n      7      3    933\n\n\n\n(\n    so.Plot(cancelled_per_day.query('sum &lt; 300'), x='count', y='sum')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n# 항공기가 많이 지연될수록 결항비율도 큰가? 오래 지연될수록 뒤에 출발하는 항공편은 결항...\n\ndef get_delayed_positive(g):\n    return pd.Series([\n        g[\"cancelled\"].mean(),\n        g[\"dep_delay\"].apply(lambda x: 0 if x &lt; 0 else x).mean(),\n        g[\"arr_delay\"].apply(lambda x: 0 if x &lt; 0 else x).mean(),\n    ], index=[\"cancelled_prop\", \"avg_dep_delay\", \"avg_arr_delay\"])\n\ncancelled_and_delays = (\n    flights.assign(\n        cancelled = lambda x: x.dep_delay.isna() | x.arr_delay.isna())\n    .groupby([\"month\", \"day\"])\n    .apply(get_delayed_positive)\n)\n\ncancelled_and_delays\n\n           cancelled_prop  avg_dep_delay  avg_arr_delay\nmonth day                                              \n1     1              0.01          13.72          18.02\n      2              0.02          15.70          18.47\n      3              0.02          13.02          14.14\n...                   ...            ...            ...\n12    29             0.02          24.14          25.70\n      30             0.02          13.14          15.87\n      31             0.02           9.71          12.44\n\n[365 rows x 3 columns]\n\n\n\n(\n    so.Plot(\n        cancelled_and_delays.query(\"cancelled_prop &lt; .3\"),\n        x=\"avg_dep_delay\",\n        y=\"cancelled_prop\",\n    )\n    .add(so.Dots(color=\".6\"))\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(\n        cancelled_and_delays.query(\"cancelled_prop &lt; 0.05\"),\n        x=\"avg_arr_delay\",\n        y=\"cancelled_prop\",\n    )\n    .add(so.Dots(color=\".6\"))\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n\n3.\n\n# 3. What time of day should you fly if you want to avoid delays as much as possible?\n# 도착지연이 가장 적은 시간대는 언제인가?\n(\n    flights.groupby(\"sched_dep_time\")[\"arr_delay\"]\n    .mean()\n    # .reset_index()\n    .sort_values(ascending=True)\n    .head(7)\n)\n\nsched_dep_time\n712    -35.35\n626    -30.00\n505    -26.50\n2208   -26.00\n516    -25.75\n555    -25.00\n557    -23.67\nName: arr_delay, dtype: float64\n\n\n\n# minus delay는 제외하고 도착지연의 평균을 구한다면,\n(\n    flights\n    .groupby([\"sched_dep_time\"])[\"arr_delay\"]\n    # .apply(lambda x: x.map(lambda x: 0 if x &lt; 0 else x).mean())\n    .mean()\n    .sort_values(ascending=False)\n    .head(7)\n)\n\nsched_dep_time\n2207   105.33333\n1848    69.00000\n1752    68.25000\n1531    65.09091\n2339    59.00000\n1739    56.14516\n1653    55.38095\nName: arr_delay, dtype: float64\n\n\n\n# minus delay는 제외하고 도착지연의 평균을 구한다면,\n(\n    flights\n    .groupby([\"sched_dep_time\"])[\"arr_delay\"]\n    .apply(lambda x: x.map(lambda x: 0 if x &lt; 0 else x).mean())\n    .sort_values(ascending=True)\n    .head(7)\n)\n\nsched_dep_time\n555    0.00000\n538    0.00000\n1424   0.00000\n535    0.00000\n2208   0.00000\n2345   0.00000\n528    0.00000\nName: arr_delay, dtype: float64\n\n\n\ndef get_delayed_positive(g):\n    return pd.Series(\n        [\n            g[\"arr_delay\"].map(lambda x: 0 if x &lt; 0 else x).mean(),\n            g[\"arr_delay\"].count(),\n        ],\n        index=[\"avg_arr_delay\", \"n_total\"],\n    )\n\ntime_delay = (\n    flights.groupby([\"sched_dep_time\"])\n    .apply(get_delayed_positive)\n    .reset_index()\n)\ntime_delay\n\n      sched_dep_time  avg_arr_delay   n_total\n0                106            NaN   0.00000\n1                500        2.95000 340.00000\n2                501        0.00000   1.00000\n...              ...            ...       ...\n1018            2355       11.79452  73.00000\n1019            2358       16.63636  44.00000\n1020            2359       16.51111 810.00000\n\n[1021 rows x 3 columns]\n\n\n\n\n\n\n\n\nTip\n\n\n\nflights[[\"arr_delay\"]].mean()  # return a Series\n# arr_delay   6.90\n# dtype: float64\n\nflights[\"arr_delay\"].mean()  # return a scalar\n# 6.90\n\n\n\n# 시각화해서 살펴보면,\n(\n    so.Plot(time_delay, x='sched_dep_time', y='avg_arr_delay')\n    .add(so.Dots(color=\".6\"), pointsize='n_total')\n    .add(so.Line(), so.PolyFit(5))\n    .limit(y=(-5, 60))\n    .scale(pointsize=(1, 30))\n)\n\n\n\n\n\n\n\n\n\n# 이상치들은 샘플수가 작은가?\ntime_delay.sort_values(by=\"avg_arr_delay\", ascending=False).head(7)\n\n     sched_dep_time  avg_arr_delay  n_total\n991            2207         105.33     3.00\n798            1848          70.62    16.00\n742            1752          68.25     4.00\n601            1531          66.45    11.00\n683            1653          61.19    21.00\n28              558          61.00     3.00\n729            1739          59.27    62.00\n\n\n\n\n4.\n\n# 4. For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\n\n\n# For each destination, compute the total minutes of delay.\ntotal_delay = flights.groupby(\"dest\")[\"arr_delay\"].sum().reset_index(name=\"total_delay\")\ntotal_delay\n\n    dest  total_delay\n0    ABQ   1113.00000\n1    ACK   1281.00000\n2    ALB   6018.00000\n..   ...          ...\n102  TVC   1232.00000\n103  TYS  13912.00000\n104  XNA   7406.00000\n\n[105 rows x 2 columns]\n\n\n\n# For each flight, compute the proportion of the total delay for its destination.\n# Merge를 이용하면,\n(\n    flights\n    .merge(total_delay, on=\"dest\")\n    .assign(prop_delay = lambda x: x.arr_delay / x.total_delay)\n    .sort_values([\"year\", \"month\", \"day\", \"hour\", \"minute\"])\n    .head(3)\n)\n\n      year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0     2013      1    1 517.00000             515    2.00000 830.00000   \n1     2013      1    1 533.00000             529    4.00000 850.00000   \n7198  2013      1    1 542.00000             540    2.00000 923.00000   \n\n      sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n0                819   11.00000      UA    1545  N14228    EWR  IAH 227.00000   \n1                830   20.00000      UA    1714  N24211    LGA  IAH 227.00000   \n7198             850   33.00000      AA    1141  N619AA    JFK  MIA 160.00000   \n\n      distance  hour  minute  total_delay  prop_delay  \n0         1400     5      15  30046.00000     0.00037  \n1         1416     5      29  30046.00000     0.00067  \n7198      1089     5      40   3467.00000     0.00952  \n\n\n\n# transform을 이용하면,\nflights.groupby(\"dest\")[\"arr_delay\"].transform(\"sum\")\nflights[\"total_delay\"] = flights.groupby(\"dest\")[\"arr_delay\"].transform(\"sum\")\nflights.head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1 517.00000             515    2.00000 830.00000   \n1  2013      1    1 533.00000             529    4.00000 850.00000   \n2  2013      1    1 542.00000             540    2.00000 923.00000   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n0             819   11.00000      UA    1545  N14228    EWR  IAH 227.00000   \n1             830   20.00000      UA    1714  N24211    LGA  IAH 227.00000   \n2             850   33.00000      AA    1141  N619AA    JFK  MIA 160.00000   \n\n   distance  hour  minute  total_delay  \n0      1400     5      15  30046.00000  \n1      1416     5      29  30046.00000  \n2      1089     5      40   3467.00000  \n\n\n\nflights.assign(prop_delay = lambda x: x.arr_delay / x.total_delay).head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1 517.00000             515    2.00000 830.00000   \n1  2013      1    1 533.00000             529    4.00000 850.00000   \n2  2013      1    1 542.00000             540    2.00000 923.00000   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n0             819   11.00000      UA    1545  N14228    EWR  IAH 227.00000   \n1             830   20.00000      UA    1714  N24211    LGA  IAH 227.00000   \n2             850   33.00000      AA    1141  N619AA    JFK  MIA 160.00000   \n\n   distance  hour  minute  total_delay  prop_delay  \n0      1400     5      15  30046.00000     0.00037  \n1      1416     5      29  30046.00000     0.00067  \n2      1089     5      40   3467.00000     0.00952  \n\n\n\n\n5.\n\n# 5. Find all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\nflights.groupby(\"dest\")[\"carrier\"].nunique()\n\ndest\nABQ    1\nACK    1\nALB    1\n      ..\nTVC    2\nTYS    2\nXNA    2\nName: carrier, Length: 105, dtype: int64\n\n\n\ndest_carrier = flights.copy()\ndest_carrier[\"carrier_n\"] = flights.groupby(\"dest\")[\"carrier\"].transform(\"nunique\")\n\ndest_carrier.query('carrier_n &gt;= 2').head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1 517.00000             515    2.00000 830.00000   \n1  2013      1    1 533.00000             529    4.00000 850.00000   \n2  2013      1    1 542.00000             540    2.00000 923.00000   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n0             819   11.00000      UA    1545  N14228    EWR  IAH 227.00000   \n1             830   20.00000      UA    1714  N24211    LGA  IAH 227.00000   \n2             850   33.00000      AA    1141  N619AA    JFK  MIA 160.00000   \n\n   distance  hour  minute  total_delay  carrier_n  \n0      1400     5      15  30046.00000          2  \n1      1416     5      29  30046.00000          2  \n2      1089     5      40   3467.00000          3  \n\n\n\n(\n    dest_carrier.query(\"carrier_n &gt;= 2\")\n    .groupby(\"carrier\")[\"dest\"]\n    .nunique()\n    .reset_index(name=\"n_dest\")\n    .assign(rank=lambda x: x.n_dest.rank(ascending=False, method=\"min\"))\n    .sort_values(\"rank\")\n)\n\n   carrier  n_dest     rank\n5       EV      51  1.00000\n0       9E      48  2.00000\n11      UA      42  3.00000\n..     ...     ...      ...\n2       AS       1 14.00000\n6       F9       1 14.00000\n8       HA       1 14.00000\n\n[16 rows x 3 columns]",
    "crumbs": [
      "Solutions",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Transform/sol1.html#c.",
    "href": "contents/Transform/sol1.html#c.",
    "title": "Transform 1 Sol",
    "section": "C.",
    "text": "C.\n\n1.\n\n# 1. Which carrier has the worst arrival delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not?\n\n\n# Total delay by carrier within each origin, dest\narr_delay = (\n    flights.groupby([\"carrier\", \"origin\", \"dest\"])[\"arr_delay\"]\n    .agg([\"mean\", \"count\"])\n    .rename(columns={\"mean\": \"arr_delay\", \"count\": \"flights\"})\n    .reset_index()\n)\narr_delay\n\n    carrier origin dest  arr_delay  flights\n0        9E    EWR  ATL      -6.25        4\n1        9E    EWR  CVG       1.40      796\n2        9E    EWR  DTW       2.54      220\n..      ...    ...  ...        ...      ...\n436      YV    LGA  CLT      12.86      258\n437      YV    LGA  IAD      18.92      278\n438      YV    LGA  PHL     -14.38        8\n\n[439 rows x 5 columns]\n\n\n\n# Total delay within each origin dest\narr_delay_total = (\n    arr_delay.groupby([\"origin\", \"dest\"])[[\"arr_delay\", \"flights\"]]\n    .sum()\n    .reset_index()\n    .rename(columns={\"arr_delay\": \"arr_delay_total\", \"flights\": \"flights_total\"})\n)\narr_delay_total\n\n    origin dest  arr_delay_total  flights_total\n0      EWR  ALB            14.40            418\n1      EWR  ANC            -2.50              8\n2      EWR  ATL            33.79           4876\n..     ...  ...              ...            ...\n221    LGA  TVC            31.75             73\n222    LGA  TYS             3.89            265\n223    LGA  XNA           125.96            709\n\n[224 rows x 4 columns]\n\n\n\n# using `transform` instead of `merge`\narr_delay[[\"arr_delay_total\", \"flights_total\"]] = (\n    arr_delay\n    .groupby([\"origin\", \"dest\"])[[\"arr_delay\", \"flights\"]]\n    .transform(\"sum\")\n)\narr_delay\n\n    carrier origin dest  arr_delay  flights  arr_delay_total  flights_total\n0        9E    EWR  ATL      -6.25        4            33.79           4876\n1        9E    EWR  CVG       1.40      796            22.60           2513\n2        9E    EWR  DTW       2.54      220            88.35           3009\n..      ...    ...  ...        ...      ...              ...            ...\n436      YV    LGA  CLT      12.86      258            45.37           5961\n437      YV    LGA  IAD      18.92      278            30.48           1659\n438      YV    LGA  PHL     -14.38        8            -8.32            598\n\n[439 rows x 7 columns]\n\n\n\n# relative delay: average delay of each carrier - average delay of other carriers\narr_delay_relative = arr_delay.assign(\n    arr_delay_others_mean=lambda x: (x.arr_delay_total - x.arr_delay)\n    / (x.flights_total - x.flights),\n    arr_delay_mean=lambda x: x.arr_delay / x.flights,\n    arr_delay_diff=lambda x: x.arr_delay_mean - x.arr_delay_others_mean,\n)\narr_delay_relative\n\n    carrier origin dest  arr_delay  flights  arr_delay_total  flights_total  \\\n0        9E    EWR  ATL      -6.25        4            33.79           4876   \n1        9E    EWR  CVG       1.40      796            22.60           2513   \n2        9E    EWR  DTW       2.54      220            88.35           3009   \n..      ...    ...  ...        ...      ...              ...            ...   \n436      YV    LGA  CLT      12.86      258            45.37           5961   \n437      YV    LGA  IAD      18.92      278            30.48           1659   \n438      YV    LGA  PHL     -14.38        8            -8.32            598   \n\n     arr_delay_others_mean  arr_delay_mean  arr_delay_diff  \n0                     0.01           -1.56           -1.57  \n1                     0.01            0.00           -0.01  \n2                     0.03            0.01           -0.02  \n..                     ...             ...             ...  \n436                   0.01            0.05            0.04  \n437                   0.01            0.07            0.06  \n438                   0.01           -1.80           -1.81  \n\n[439 rows x 10 columns]\n\n\n\narr_delay_relative.groupby(\"carrier\")[\"arr_delay_diff\"].mean().sort_values(ascending=False)\n\ncarrier\nOO   28.79\nEV    5.35\nVX    1.83\n      ... \nDL   -1.72\nMQ   -5.25\nHA     NaN\nName: arr_delay_diff, Length: 16, dtype: float64\n\n\n\n\n2.\n\n# 2. Which plane (tailnum) has the worst on-time record?\n## on-time: 늦게 도착하지 않은 항공편의 횟수로 이해\n\n(\n    flights\n    .assign(on_time=lambda x: x.arr_delay &lt;= 0 & x.arr_time.notna())\n    .groupby(\"tailnum\")[\"on_time\"]\n    .agg([\"mean\", \"count\"])\n    .sort_values(\"mean\")\n)\n\n           mean  count\ntailnum               \nN768SK  0.00000      1\nN840MH  0.00000      1\nN838AW  0.00000      2\n...         ...    ...\nN357SW  1.00000      8\nN524AS  1.00000      9\nN834MH  1.00000      1\n\n[4043 rows x 2 columns]\n\n\n\n# 극히 작은 운항횟수를 가진 비행기가 많음... : 제거\n\non_time = (\n    flights\n    .assign(on_time=lambda x: x.arr_delay &lt;= 0)\n    .groupby(\"tailnum\")[\"on_time\"]\n    .agg([\"mean\", \"count\"])\n)\n\n(\n    so.Plot(on_time, x=\"count\", y=\"mean\")\n    .add(so.Dots(), so.Jitter(y=0.02))\n    .limit(x=(0, 100))\n)\n\n\n\n\n\n\n\n\n\non_time.query('count &gt; 20').nsmallest(3, \"mean\")\n\n         mean  count\ntailnum             \nN988AT   0.19     37\nN983AT   0.25     32\nN980AT   0.26     47\n\n\n\n## on-time: 도착 delay의 길이로 파악하는 경우\n\n(\n    flights.groupby(\"tailnum\")[\"arr_delay\"]\n    .agg([\"mean\", \"count\"])\n    .query(\"count &gt; 20\")\n    .nlargest(3, \"mean\")\n)\n\n         mean  count\ntailnum             \nN203FR  59.12     41\nN645MQ  51.00     24\nN956AT  47.65     34\n\n\n\n\n3.\n\n# 3. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error).\n\n\nflights = flights.assign(\n    mph = lambda x: x.distance / x.air_time * 60\n)\n\n\n(\n    so.Plot(flights, x='distance')\n    .add(so.Line(), so.Hist(binwidth=10))\n)\n\n\n\n\n\n\n\n\n\n# 같은 루트를 비행하는 항공편들 안에서 특이점이라면 의심해 볼만함...\n\nstandardized = (\n    flights.groupby([\"origin\", \"dest\"])[\"air_time\"]\n    .agg([(\"air_time_mean\", \"mean\"), (\"air_time_std\", \"std\"), (\"n\", \"count\")])\n    .reset_index()\n)\nstandardized\n\n    origin dest  air_time_mean  air_time_std     n\n0      EWR  ALB          31.79          3.08   418\n1      EWR  ANC         413.12         14.67     8\n2      EWR  ATL         111.99          9.99  4876\n..     ...  ...            ...           ...   ...\n221    LGA  TVC          94.60          6.49    73\n222    LGA  TYS          97.82          8.52   265\n223    LGA  XNA         173.17         15.91   709\n\n[224 rows x 5 columns]\n\n\n\ndef normalize(x):\n    return (x - x.mean()) / x.std()\n\nstandardized_flights = flights.copy()\nstandardized_flights[\"air_time_z\"] = flights.groupby([\"origin\", \"dest\"])[\n    \"air_time\"\n].transform(normalize)\n\n\n(\n    standardized_flights\n    .assign(air_time_z_abs = lambda x: x.air_time_z.abs())\n    .nlargest(5, \"air_time_z_abs\")\n)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n237716  2013      6   17   1652.00            1700      -8.00   1856.00   \n244468  2013      6   24   1932.00            1920      12.00   2228.00   \n309910  2013      9    1   2237.00            1711     326.00     41.00   \n230885  2013      6   10   1356.00            1300      56.00   1646.00   \n248839  2013      6   29    755.00             800      -5.00   1035.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n237716            1815      41.00      US    2136  N967UW    LGA  BOS   \n244468            2047     101.00      UA    1703  N37255    EWR  BOS   \n309910            1851     350.00      B6    1516  N346JB    JFK  SYR   \n230885            1414     152.00      US    2175  N745VJ    LGA  DCA   \n248839             909      86.00      B6    1491  N328JB    JFK  ACK   \n\n        air_time  distance  hour  minute  air_time_z  air_time_z_abs  \n237716    107.00       184    17       0       14.78           14.78  \n244468    112.00       200    19      20       14.56           14.56  \n309910     97.00       209    17      11       13.87           13.87  \n230885    131.00       214    13       0       13.52           13.52  \n248839    141.00       199     8       0       12.17           12.17  \n\n\n\n(\n    so.Plot(standardized_flights, x='air_time_z')\n    .add(so.Line(), so.Hist(binwidth=.1))\n)\n\n\n\n\n\n\n\n\n\n  Source: The Truthful Art by Albert Cairo\n\n\n4.\n\n# 4. Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\n# 비율의 차이\nair_time_delayed = flights.copy()\nair_time_delayed[\"air_time_delayed\"] = flights.groupby([\"origin\", \"dest\"])[\n    \"air_time\"\n].transform(lambda x: (x - x.min()) / x.min())\n\nair_time_delayed.sort_values(\"air_time_delayed\", ascending=False).head(3)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n237716  2013      6   17   1652.00            1700      -8.00   1856.00   \n230885  2013      6   10   1356.00            1300      56.00   1646.00   \n248839  2013      6   29    755.00             800      -5.00   1035.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n237716            1815      41.00      US    2136  N967UW    LGA  BOS   \n230885            1414     152.00      US    2175  N745VJ    LGA  DCA   \n248839             909      86.00      B6    1491  N328JB    JFK  ACK   \n\n        air_time  distance  hour  minute  air_time_delayed  \n237716    107.00       184    17       0              4.10  \n230885    131.00       214    13       0              3.09  \n248839    141.00       199     8       0              3.03  \n\n\n\n# 크기의 차이\nair_time_delayed = flights.copy()\nair_time_delayed[\"air_time_delayed\"] = flights.groupby([\"origin\", \"dest\"])[\n    \"air_time\"\n].transform(lambda x: x - x.min())\n\nair_time_delayed.sort_values(\"air_time_delayed\", ascending=False).head(3)\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time   \n276578  2013      7   28   1727.00            1730      -3.00   2242.00  \\\n76185   2013     11   22   1812.00            1815      -3.00   2302.00   \n24032   2013      1   28   1806.00            1700      66.00   2253.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest   \n276578            2110      92.00      DL     841  N703TW    JFK  SFO  \\\n76185             2146      76.00      DL     426  N178DN    JFK  LAX   \n24032             1950     183.00      AA     575  N5DBAA    JFK  EGE   \n\n        air_time  distance  hour  minute    mph  air_time_delayed  \n276578    490.00      2586    17      30 316.65            189.00  \n76185     440.00      2475    18      15 337.50            165.00  \n24032     382.00      1747    17       0 274.40            163.00  \n\n\n\n\n5.\n\n# 5. For each plane, count the number of flights before the first delay of greater than 1 hour.\n\nflights_first_delay = (\n    flights\n    .query(\"arr_delay.notna()\")\n    .loc[:, [\"tailnum\", \"year\", \"month\", \"day\", \"arr_delay\"]]\n    .assign(delay_1h=lambda x: x.arr_delay &gt; 60)\n    .sort_values([\"tailnum\", \"year\", \"month\", \"day\"])\n)\nflights_first_delay.head(7)\n\n       tailnum  year  month  day  arr_delay  delay_1h\n120316  D942DN  2013      2   11      91.00      True\n157233  D942DN  2013      3   23      44.00     False\n157799  D942DN  2013      3   24       2.00     False\n254418  D942DN  2013      7    5     -11.00     False\n523     N0EGMQ  2013      1    1      67.00      True\n792     N0EGMQ  2013      1    1      17.00     False\n1025    N0EGMQ  2013      1    2      -6.00     False\n\n\n\n(\n    flights_first_delay.groupby(\"tailnum\")[\"delay_1h\"]\n    .apply(np.cumsum)\n    .reset_index(level=0)\n    .groupby(\"tailnum\")\n    .apply(lambda x: np.sum(x.delay_1h &lt; 1))\n    .sort_values(ascending=False)\n)\n\ntailnum\nN717TW    119\nN765US     97\nN705TW     97\n         ... \nN376AA      0\nN378AA      0\nD942DN      0\nLength: 4037, dtype: int64\n\n\n\npd.options.display.max_rows = 25\n\n\nflights_first_delay[\"cumulative_sum\"] = (\n    flights_first_delay.groupby(\"tailnum\")[\"delay_1h\"]\n    .transform(\"cumsum\")\n)\nflights_first_delay.iloc[353:377]\n\n       tailnum  year  month  day  arr_delay  delay_1h  cumulative_sum\n109925  N0EGMQ  2013     12   30      32.00     False              29\n110289  N0EGMQ  2013     12   30      27.00     False              29\n111144  N0EGMQ  2013     12   31     122.00      True              30\n7956    N10156  2013      1   10       2.00     False               0\n8238    N10156  2013      1   10      40.00     False               0\n8513    N10156  2013      1   10      47.00     False               0\n8898    N10156  2013      1   11     -12.00     False               0\n9174    N10156  2013      1   11      -8.00     False               0\n9594    N10156  2013      1   11      27.00     False               0\n9907    N10156  2013      1   12     -20.00     False               0\n10127   N10156  2013      1   12      -5.00     False               0\n10926   N10156  2013      1   13      20.00     False               0\n11240   N10156  2013      1   13     123.00      True               1\n11444   N10156  2013      1   14      20.00     False               1\n12973   N10156  2013      1   15      12.00     False               1\n13536   N10156  2013      1   16      37.00     False               1\n13902   N10156  2013      1   16     112.00      True               2\n14385   N10156  2013      1   17       7.00     False               2\n14734   N10156  2013      1   17     123.00      True               3\n14910   N10156  2013      1   17      15.00     False               3\n15106   N10156  2013      1   18      15.00     False               3\n18286   N10156  2013      1   22      13.00     False               3\n18540   N10156  2013      1   22     101.00      True               4\n18950   N10156  2013      1   22      78.00      True               5",
    "crumbs": [
      "Solutions",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html",
    "href": "contents/Transform/nycflights.html",
    "title": "NYC Flights",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html#q1",
    "href": "contents/Transform/nycflights.html#q1",
    "title": "NYC Flights",
    "section": "Q1",
    "text": "Q1\n각 도착지에 따른 비행거리(distance)와 도착지연시간(arr_delay)와의 관계를 알아보고자 함.\n\nGroup flights by destination.\nSummarise to compute distance, average delay, and number of flights.\nFilter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport.\n\n\n# Load the nycflight13 dataset\nflights = sm.datasets.get_rdataset(\"flights\", \"nycflights13\").data.drop(columns=\"time_hour\")\n\n\n# grouping by destinations\nby_dest = flights.groupby(\"dest\")\n\n\ndelay = (\n      by_dest\n      .agg(\n            count=(\"distance\", \"count\"), \n            dist=(\"distance\", \"mean\"), \n            delays=(\"arr_delay\", \"mean\")\n      )\n      .reset_index()\n)\n\n\ndelay\n\n    dest  count       dist   delays\n0    ABQ    254 1826.00000  4.38189\n1    ACK    265  199.00000  4.85227\n2    ALB    439  143.00000 14.39713\n..   ...    ...        ...      ...\n102  TVC    101  652.38614 12.96842\n103  TYS    631  638.80983 24.06920\n104  XNA   1036 1142.50579  7.46573\n\n[105 rows x 4 columns]\n\n\n\n(\n    so.Plot(delay, x=\"dist\", y=\"delays\")\n    .add(so.Dots(), pointsize=\"count\")\n    .add(so.Line(), so.PolyFit(5))\n    .scale(pointsize=(2, 20))\n)\n\n\n\n\n\n\n\n\n불필요한 자료를 제거하고 시각화하는 것이 유리\n\n# Filter to remove noisy points and Honolulu airport\ndelay_sub = delay.query('count &gt; 20 & dest != \"HNL\"')\n\n(\n    so.Plot(delay_sub, x=\"dist\", y=\"delays\")\n    .add(so.Dots(), pointsize=\"count\")\n    .add(so.Line(), so.PolyFit(5))\n    .scale(pointsize=(2, 20))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n다음과 같이 제외되는 자료에 대해 label 혹은 True/False을 붙여 관리하는 것도 방법\nidx = (delay[\"count\"] &gt; 20) & (delay[\"dest\"] != \"HNL\")\n# 또는\nidx = delay.eval('count &gt; 20 and dest != \"HNL\"')  # query와 동일한 문법\n\ndelay[\"incl\"] = np.where(idx, \"out\", \"in\")  # idx가 True인 곳은 \"out\", False인 곳은 \"in\"\ndelay\n#   dest  count    dist  delay incl\n# 0  ABQ    254 1826.00   4.38  out\n# 1  ACK    265  199.00   4.85  out\n# 2  ALB    439  143.00  14.40  out\n# 3  ANC      8 3370.00  -2.50   in\n# 4  ATL  17215  757.11  11.30  out\n...\n\n# 제외되는 데이터를 같이 볼 수 있음\n(\n    so.Plot(delay, x=\"dist\", y=\"delay\", color=\"incl\")\n    .add(so.Dots(), pointsize=\"count\")\n    .add(so.Line(), so.PolyFit(5))\n    .scale(pointsize=(5, 20))\n)",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html#q2",
    "href": "contents/Transform/nycflights.html#q2",
    "title": "NYC Flights",
    "section": "Q2",
    "text": "Q2\n평균적으로 가장 연착시간이 큰 항공기(tail number로 구분)를 살펴보는데,\n우선, count()를 사용하여 샘플 수가 극히 작은 케이스들 혹은 극단치들을 제거해서 살펴보는 것이 유리함.\n\ndelays = flights.groupby(\"tailnum\")[[\"arr_delay\"]].mean() # as DataFrame\ndelays\n\n         arr_delay\ntailnum           \nD942DN    31.50000\nN0EGMQ     9.98295\nN10156    12.71724\n...            ...\nN998DL    16.39474\nN999DN    14.31148\nN9EAMQ     9.23529\n\n[4043 rows x 1 columns]\n\n\n\n(\n    so.Plot(delays, x=\"arr_delay\")\n    .add(so.Line(), so.Hist())\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\npandas DataFrame method .hist()를 간단히 이용할 수 있음\ndelays.hist(bins=100)\n\n\n300분이 넘는 delay도 있음을 보이는데, 각 평균 delay값이 몇 개 항목의 평균인지 살펴보면 흥미로운 사실을 발견할 수 있음. 즉,\n\ndelays = (\n    flights\n    .groupby(\"tailnum\")[\"arr_delay\"]\n    .agg([(\"delay\", \"mean\"), (\"n\", \"count\")])\n)\ndelays\n\n           delay    n\ntailnum              \nD942DN  31.50000    4\nN0EGMQ   9.98295  352\nN10156  12.71724  145\n...          ...  ...\nN998DL  16.39474   76\nN999DN  14.31148   61\nN9EAMQ   9.23529  238\n\n[4043 rows x 2 columns]\n\n\n\n(\n    so.Plot(delays, x=\"n\", y=\"delay\")\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".4\"), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n비행횟수가 작을수록 편차가 크게 나타나는데,\n일반적으로 샘플 수가 클수록 평균들의 편차가 줄어드는 현상이 나타남.\n위와 같은 플랏을 볼 때, 샘플 수가 매우 작은 그룹들은 제외하고 살펴보는 것이 패턴을 파악하는데 종종 도움이 됨.\n간단하게, query() method를 이용하면 편리\n\n(\n    so.Plot(delays.query('n &gt; 25'), x=\"n\", y=\"delay\")\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".4\"), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n연속적인 함수 적용시 .pipe method를 권장; method chaining\n(\n    delays\n    .query('n &gt; 25')\n    .pipe(so.Plot, x=\"n\", y=\"delay\")  # 함수 so.Plot의 첫 번째 인자로 DataFrame을 받음\n    .add(so.Dots(alpha=.1))\n)",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html#q3",
    "href": "contents/Transform/nycflights.html#q3",
    "title": "NYC Flights",
    "section": "Q3",
    "text": "Q3\n\n# Lahman's Baseball Database\nbatting = pd.read_csv(\"https://raw.githubusercontent.com/beanumber/baseball_R/master/data/Batting.csv\") \n\n\nbatting\n\n        playerID  yearID  stint teamID lgID    G  G_batting     AB     R  \\\n0      aardsda01    2004      1    SFN   NL   11         11   0.00  0.00   \n1      aardsda01    2006      1    CHN   NL   45         43   2.00  0.00   \n2      aardsda01    2007      1    CHA   AL   25          2   0.00  0.00   \n...          ...     ...    ...    ...  ...  ...        ...    ...   ...   \n95192  zwilldu01    1914      1    CHF   FL  154        154 592.00 91.00   \n95193  zwilldu01    1915      1    CHF   FL  150        150 548.00 65.00   \n95194  zwilldu01    1916      1    CHN   NL   35         35  53.00  4.00   \n\n           H  ...    SB   CS    BB    SO  IBB  HBP    SH   SF  GIDP  G_old  \n0       0.00  ...  0.00 0.00  0.00  0.00 0.00 0.00  0.00 0.00  0.00  11.00  \n1       0.00  ...  0.00 0.00  0.00  0.00 0.00 0.00  1.00 0.00  0.00  45.00  \n2       0.00  ...  0.00 0.00  0.00  0.00 0.00 0.00  0.00 0.00  0.00   2.00  \n...      ...  ...   ...  ...   ...   ...  ...  ...   ...  ...   ...    ...  \n95192 185.00  ... 21.00  NaN 46.00 68.00  NaN 1.00 10.00  NaN   NaN 154.00  \n95193 157.00  ... 24.00  NaN 67.00 65.00  NaN 2.00 18.00  NaN   NaN 150.00  \n95194   6.00  ...  0.00  NaN  4.00  6.00  NaN 0.00  2.00  NaN   NaN  35.00  \n\n[95195 rows x 24 columns]\n\n\n\n# AB: At Bats 타석에 나선 횟수, H: Hits; times reached base 출루한 횟수\nbatters = batting.groupby(\"playerID\")[[\"H\", \"AB\"]].sum()\nbatters = batters.assign(\n    BA = lambda x: x.H / x.AB   # BA: batting average 타율\n)\nbatters\n\n                   H          AB      BA\nplayerID                                \naardsda01    0.00000     3.00000 0.00000\naaronha01 3771.00000 12364.00000 0.30500\naaronto01  216.00000   944.00000 0.22881\n...              ...         ...     ...\nzuvelpa01  109.00000   491.00000 0.22200\nzuverge01   21.00000   142.00000 0.14789\nzwilldu01  364.00000  1280.00000 0.28437\n\n[17661 rows x 3 columns]\n\n\n\n# filtering없이 보았을 때와 비교해서 어느 정도 제외할지 고민\n(\n    so.Plot(batters.query('AB &gt; 100'), x=\"AB\", y=\"BA\")\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(color=\".4\"), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n# 1번 기회를 얻은 타자... 타율 100%\nbatters.sort_values(\"BA\", ascending=False).head(10)\n\n             H   AB   BA\nplayerID                \npaciojo01 3.00 3.00 1.00\ngallaja01 1.00 1.00 1.00\nsellsda01 1.00 1.00 1.00\n...        ...  ...  ...\nkehnch01  2.00 2.00 1.00\ndevinha01 2.00 2.00 1.00\nliddeda01 1.00 1.00 1.00\n\n[10 rows x 3 columns]",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html#exercises",
    "href": "contents/Transform/nycflights.html#exercises",
    "title": "NYC Flights",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html#a",
    "href": "contents/Transform/nycflights.html#a",
    "title": "NYC Flights",
    "section": "A",
    "text": "A\n다음 조건을 만족하는 항공편을 필터링 해보세요. (1~6)\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n출발할 때 예정시간보다 1시간 이상 지연되어 출발하였으나 빠르게 비행하여 출발 지연된 시간보다 도착 지연이 30분이상 단축된 항공편들입니다. (예를 들어, 1시간 늦게 출발했는데, 도착은 28분 지연된 항공편)\nDeparted between midnight and 6am (inclusive)\n\n\n\nFind the fastest flights.\nSort flights to find the most delayed flights. Find the flights that left earliest (예정시간보다 가장 일찍 출발한).\nWhich flights travelled the farthest? Which travelled the shortest?\n각 도착지 별로, 뉴욕에서 출항한 항공편이 1년 중 몇 일 있었는가?\n뉴욕에서 1년 중 300일 이상 출항하는 도착지들을 구하면?",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html#b",
    "href": "contents/Transform/nycflights.html#b",
    "title": "NYC Flights",
    "section": "B",
    "text": "B\n\nOur definition of cancelled flights (dep_delay or arr_delay is missing) is slightly suboptimal. Why? Which is the most important column?\n\n예를 들어, 출발지연은 missing이 아니나 도착지연은 missing인 것이 있음\n\nLook at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the (daily) average delay?\n\n취소되는 항공편들이 많은 것과 관계 있는 것은 무엇이 있을까…\n\nWhat time of day should you fly if you want to avoid delays as much as possible?\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nFind all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\n즉, 적어도 두 항공사가 출항하는 도착지들도 한정한 후,\n다양한 곳으로 출항할수록 높은 순위의 항공사라고 보고, 항공사들의 순위를 정해봄",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/nycflights.html#c",
    "href": "contents/Transform/nycflights.html#c",
    "title": "NYC Flights",
    "section": "C",
    "text": "C\nChallenges:\n\nWhich carrier has the worst arrival delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not?\n\n항공사(carrier)마다 취항하는 곳에 차이가 날 수 있다면, 그건 그 노선 혹은 공항의 문제이지 항공사의 문제는 아닐 수도 있음을 암시하는 것임\n\nWhich plane (tailnum) has the worst on-time record?\n\non-time을 적절히 정의한 후에 진행; 여러 방식이 있을 수 있음\n예를 들어, 늦게 도착하지 않은 항공편의 “갯수”로 보거나\n도착지연의 평균값을 기준으로 볼 수도 있음\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error).\n\n빠르게 비행한 이유: 제트 기류? 정체가 심한 공항?…\n같은 루트를 비행하는 항공편들 안에서 특이점이라면 의심해 볼만함…\n서로 다른 루트를 비행하는 항공편들과의 비교는?\n빠르다는 것을 비교하려면 동일한 루트에서 비교해야 적절함\n다른 루트의 항공편들까지 같이 비교하려면 어떤 방식이 있겠는가?\n\nCompute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\n“상대적”의 의미가 값의 차이로 볼지 비율의 차이로 볼지도 고려해 볼 것\n\n** For each plane, count the number of flights before the first delay of greater than 1 hour.\n\nnp.cumsum을 활용",
    "crumbs": [
      "Transform",
      "Transforming I",
      "Examples"
    ]
  },
  {
    "objectID": "contents/Transform/exercise_wrangle.html",
    "href": "contents/Transform/exercise_wrangle.html",
    "title": "Exercises",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\n\n\nThe nycflight13 datasets\nCombine 섹션에서 다른 nycflight13의 4개의 relational data를 이용하세요.\n\nAdd the location of the origin and destination (i.e. the lat and lon in airports) to flights.\nIs there a relationship between the age of a plane and its delays?\nWhat weather conditions make it more likely to see a delay?\nflights 테이블에서 하루 평균 도착지연(arr_delay)가 가장 큰 10일에 해당하는 항공편을 선택\nflights 테이블의 도착지(dest)에 대한 공항정보가 airports 테이블에 없는 그러한 도착지(dest)를 구하면?\nFilter flights (항공편) in flights to only show flights with planes that have flown at least 100 flights.\nFind the 48 hours (over the course of the whole year) that have the worst (departure) delays. Cross-reference it with the weather data. Can you see any patterns?\n\nflights의 hour 열을 이용할 것\n\nYou might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above.\n\n즉, 각 비행기는 특정 항공사에서만 운행되는가의 질문임. 2개 이상의 항공사에서 운항되는 비행기가 있는지 확인해 볼 것\n그리고, 2개 이상의 항공사에서 운항되는 비행기들만 포함하고, 그 항공사들의 full name을 함께 포함하는 테이블을 만들어 볼 것",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Exercises"
    ]
  },
  {
    "objectID": "contents/Transform/crosstab.html",
    "href": "contents/Transform/crosstab.html",
    "title": "Cross-Tabulation and Pivot Tables",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n# Load a datdset\ntips = sns.load_dataset(\"tips\")\ntips\n\n     total_bill  tip     sex smoker   day    time  size\n0         16.99 1.01  Female     No   Sun  Dinner     2\n1         10.34 1.66    Male     No   Sun  Dinner     3\n2         21.01 3.50    Male     No   Sun  Dinner     3\n..          ...  ...     ...    ...   ...     ...   ...\n241       22.67 2.00    Male    Yes   Sat  Dinner     2\n242       17.82 1.75    Male     No   Sat  Dinner     2\n243       18.78 3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Pivot Table"
    ]
  },
  {
    "objectID": "contents/Transform/crosstab.html#pd.crosstab",
    "href": "contents/Transform/crosstab.html#pd.crosstab",
    "title": "Cross-Tabulation and Pivot Tables",
    "section": "pd.crosstab()",
    "text": "pd.crosstab()\n두 카테고리 변수의 모든 level 쌍에 대한 count.\n\nnormalize: 비율을 계산\n\nmargins: 행과 열로 합\n\ngroupby()를 적용해 구할 수 있으나 좀 더 간결.\n\npd.crosstab(tips[\"day\"], tips[\"time\"])\n\ntime  Lunch  Dinner\nday                \nThur     61       1\nFri       7      12\nSat       0      87\nSun       0      76\n\n\n\n# groupby를 이용\ntips.groupby([\"day\", \"time\"], observed=False).size()  # observed: categorical type의 변수에 대한 처리 (default: True)\n\nday   time  \nThur  Lunch     61\n      Dinner     1\nFri   Lunch      7\n                ..\nSat   Dinner    87\nSun   Lunch      0\n      Dinner    76\nLength: 8, dtype: int64\n\n\n\n# groupby & unstack를 이용\ntips.groupby([\"day\", \"time\"], observed=False).size().unstack()\n\ntime  Lunch  Dinner\nday                \nThur     61       1\nFri       7      12\nSat       0      87\nSun       0      76\n\n\nParameters\n\nnormalize: 비율을 계산 (index, columns, all)\n\nmargins: 행과 열로 합산\n\n\npd.crosstab(tips[\"day\"], tips[\"time\"], normalize='all', margins=True)\n\ntime  Lunch  Dinner  All\nday                     \nThur   0.25    0.00 0.25\nFri    0.03    0.05 0.08\nSat    0.00    0.36 0.36\nSun    0.00    0.31 0.31\nAll    0.28    0.72 1.00\n\n\n\npd.crosstab(tips[\"day\"], tips[\"time\"], normalize='columns')\n\ntime  Lunch  Dinner\nday                \nThur   0.90    0.01\nFri    0.10    0.07\nSat    0.00    0.49\nSun    0.00    0.43",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Pivot Table"
    ]
  },
  {
    "objectID": "contents/Transform/crosstab.html#pivot_table",
    "href": "contents/Transform/crosstab.html#pivot_table",
    "title": "Cross-Tabulation and Pivot Tables",
    "section": "pivot_table()",
    "text": "pivot_table()\n\ncount 대신 mean\n\nmean 외에 다른 aggregation 함수를 지정할 수 있음\n\ngrouping을 할 변수들을 (index and/or columns)과 aggregate할 변수 지정 (values)\n\n\n# grouping을 할 변수들을 index에 지정, aggregate할 변수 지정\ntips.pivot_table(index=[\"day\", \"time\"], values=\"tip\")  # dropna=True\n\n             tip\nday  time       \nThur Lunch  2.77\n     Dinner 3.00\nFri  Lunch  2.38\n     Dinner 2.94\nSat  Dinner 2.99\nSun  Dinner 3.26\n\n\n\n# grouping을 할 변수들을 index & columns에 지정, aggregate할 변수 지정\ntips.pivot_table(index=\"day\", columns=\"time\", values=\"tip\")\n\ntime  Lunch  Dinner\nday                \nThur   2.77    3.00\nFri    2.38    2.94\nSat     NaN    2.99\nSun     NaN    3.26\n\n\n\n# groupby를 이용\ntips.groupby([\"day\", \"time\"], observed=True)[\"tip\"].mean()\n\nday   time  \nThur  Lunch    2.77\n      Dinner   3.00\nFri   Lunch    2.38\n      Dinner   2.94\nSat   Dinner   2.99\nSun   Dinner   3.26\nName: tip, dtype: float64\n\n\n\n# groupby를 이용\ntips.groupby([\"day\", \"time\"], observed=True)[\"tip\"].mean().unstack()\n\ntime  Lunch  Dinner\nday                \nThur   2.77    3.00\nFri    2.38    2.94\nSat     NaN    2.99\nSun     NaN    3.26\n\n\n\n# 두 개 이상의 변수에 대한 aggregation\ntips.pivot_table(index=\"day\", columns=\"time\", values=[\"tip\", \"total_bill\"])\n\n       tip        total_bill       \ntime Lunch Dinner      Lunch Dinner\nday                                \nThur  2.77   3.00      17.66  18.78\nFri   2.38   2.94      12.85  19.66\nSat    NaN   2.99        NaN  20.44\nSun    NaN   3.26        NaN  21.41\n\n\n\n# groupby를 이용\ntips.groupby([\"day\", \"time\"])[[\"tip\", \"total_bill\"]].mean().unstack()  # observed=True\n\n       tip        total_bill       \ntime Lunch Dinner      Lunch Dinner\nday                                \nThur  2.77   3.00      17.66  18.78\nFri   2.38   2.94      12.85  19.66\nSat    NaN   2.99        NaN  20.44\nSun    NaN   3.26        NaN  21.41\n\n\n\n# margins 추가\ntips.pivot_table(index=\"day\", columns=\"time\", values=\"tip\", margins=True)\n\ntime  Lunch  Dinner  All\nday                     \nThur   2.77    3.00 2.77\nFri    2.38    2.94 2.73\nSat     NaN    2.99 2.99\nSun     NaN    3.26 3.26\nAll    2.73    3.10 3.00",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Pivot Table"
    ]
  },
  {
    "objectID": "contents/Transform/babynames.html",
    "href": "contents/Transform/babynames.html",
    "title": "Baby Names",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Exploratory Analysis II",
      "Baby Names"
    ]
  },
  {
    "objectID": "contents/Transform/babynames.html#us-baby-names-18802010",
    "href": "contents/Transform/babynames.html#us-baby-names-18802010",
    "title": "Baby Names",
    "section": "US Baby Names 1880–2010",
    "text": "US Baby Names 1880–2010\nSource: McKinney’s: 13. Data Analysis Examples\n다음 데이터는 미국의 출생아 이름에 대한 데이터입니다. 링크\n\n2021년 업데이트된 버전으로 1880년부터 2021년까지의 데이터가 있으며,\n각 연도별로 남자아이와 여자아이의 이름별 출생아 수를 나타내고 있습니다.\n\n\nnames = pd.read_csv(\"data/babynames.csv\")\nnames\n\n          name sex  births  year\n0         Mary   F    7065  1880\n1         Anna   F    2604  1880\n2         Emma   F    2003  1880\n...        ...  ..     ...   ...\n2052778  Zyian   M       5  2021\n2052779  Zylar   M       5  2021\n2052780    Zyn   M       5  2021\n\n[2052781 rows x 4 columns]\n\n\n\n1. 다음과 같이 년도와 성별에 따른 출생아 수의 합계를 구해보세요.\n\ngroupby와 sum을 사용하세요.\n\n\n\ncode\nnames_sex_year = names.groupby([\"year\", \"sex\"])[\"births\"].sum().reset_index()\n\n\n\nnames_sex_year\n\n     year sex   births\n0    1880   F    90994\n1    1880   M   110490\n2    1881   F    91953\n..    ...  ..      ...\n281  2020   M  1718248\n282  2021   F  1627098\n283  2021   M  1734277\n\n[284 rows x 3 columns]\n\n\n\n\n2. 이를 이용해 다음과 같이 연도에 따른 출생아 수의 변화를 성별에 따라 그려보세요.\n\n\ncode\n(\n    so.Plot(names_sex_year, x='year', y='births', color=\"sex\")\n    .add(so.Line())\n)\n\n\n\n\n\n\n\n\n\n\n\n3. names 데이터프레임에 각 이름별 출생아 수의 비율을 나타내는 컬럼 prop을 추가하세요.\n\n예를 들어, 아래 결과 테이블 첫 행을 보면, 1880년 Mary라는 이름의 여자 출생아 수는 7065명인데, 이는 1880년에 태어난 총 여자 아이 중 7.764%를 차지합니다.\nyear, sex로 그룹핑을 한 후\n이 비율을 계산하는 함수를 세워서 apply method를 이용하여 prop 컬럼을 추가해 보세요.\n\n\n\ncode\ndef add_prop(group):\n    group = group.assign(prop = lambda x: x.births / x.births.sum())\n    return group\n\n# 또는\ndef add_prop(group):\n    group[\"prop\"] = group[\"births\"] / group[\"births\"].sum()\n    return group\n\nnames = names.groupby([\"year\", \"sex\"], group_keys=False).apply(add_prop)\n\n\n\nnames\n\n          name sex  births  year  prop\n0         Mary   F    7065  1880  0.08\n1         Anna   F    2604  1880  0.03\n2         Emma   F    2003  1880  0.02\n...        ...  ..     ...   ...   ...\n2052778  Zyian   M       5  2021  0.00\n2052779  Zylar   M       5  2021  0.00\n2052780    Zyn   M       5  2021  0.00\n\n[2052781 rows x 5 columns]\n\n\n\n\n4. 이제 names에서 각 연도에서 남녀별 인기있는 이름 1000개씩을 추립니다.\n\n즉, 1880년 인기있는 여자아이 1000개, 남자아이 1000개를 추리고, 1881년에도 마찬가지…\n인기있다는 것은 그 이름의 출생아 수가 많다는 의미입니다.\n3번과 마찬가지로, year, sex로 그룹핑을 한 후\ngrouped object에 sort_values method를 적용하여 상위 1000개를 추출하는 함수 세워 apply로 적용해 보세요.\n\n\n\ncode\ndef get_top1000(group):\n    return group.sort_values(\"births\", ascending=False)[:1000]\n\ntop1000 = names.groupby([\"year\", \"sex\"], group_keys=False).apply(\n    get_top1000\n)  # group_keys=False: 그룹핑 한 변수들을 인덱스로 사용하지 않음\n\n\n\ntop1000\n\n            name sex  births  year  prop\n0           Mary   F    7065  1880  0.08\n1           Anna   F    2604  1880  0.03\n2           Emma   F    2003  1880  0.02\n...          ...  ..     ...   ...   ...\n2039793   Ronnie   M     217  2021  0.00\n2039792  Merrick   M     217  2021  0.00\n2039791   Mayson   M     217  2021  0.00\n\n[283876 rows x 5 columns]",
    "crumbs": [
      "Exploratory Analysis II",
      "Baby Names"
    ]
  },
  {
    "objectID": "contents/Transform/babynames.html#a-sample-trend",
    "href": "contents/Transform/babynames.html#a-sample-trend",
    "title": "Baby Names",
    "section": "A sample trend",
    "text": "A sample trend\n\n5. 각 아기 이름들의 시간에 따른 트렌드를 살펴보는데,\n\n각 년도별, 이름별 총 출생아 수를 구한 후; total_birth\n몇 개의 이름 (John, Harry, Mary, Marilyn)에 대해서, 시간에 따른 이름의 변화를 살펴봅니다.\n\n\n\ncode\ntotal_births = top1000.groupby([\"year\", \"name\"])[\"births\"].sum()\n\n\n\ntotal_births\n\nyear  name  \n1880  Aaron     102\n      Abbie      71\n      Abby        6\n               ... \n2021  Zyair     324\n      Zyaire    828\n      Zyon      240\nName: births, Length: 269016, dtype: int64\n\n\n\n\ncode\n# total_births의 MultiIndex를 직접 이용해 subsetting을 하려면\nsubset = total_births.loc[:, [\"John\", \"Harry\", \"Mary\", \"Marilyn\"]]  # .loc[첫번째 인덱스, 두번째 인덱스]\nsubset = subset.reset_index()  # seaborn을 이용하기 위해서 DataFrame으로 변환\n\n# 또는 DataFrame으로 먼저 변환한 후에 query를 이용\nsubset = total_births.reset_index().query(\n    'name in [\"John\", \"Harry\", \"Mary\", \"Marilyn\"]'\n)\n\n# visualization\n(\n    so.Plot(subset, x='year', y='births', color='name')\n    .add(so.Line())\n)",
    "crumbs": [
      "Exploratory Analysis II",
      "Baby Names"
    ]
  },
  {
    "objectID": "contents/Transform/babynames.html#measuring-the-increase-in-naming-diversity",
    "href": "contents/Transform/babynames.html#measuring-the-increase-in-naming-diversity",
    "title": "Baby Names",
    "section": "Measuring the increase in naming diversity",
    "text": "Measuring the increase in naming diversity\n\n6. 이제 이름의 다양성이 시간에 따라 어떻게 변화하는지를 살펴봅니다.\n\ntop1000은 이미 각 년도별, 성별에 따른 인기있는 이름 1000개씩을 추려놓은 데이터프레임입니다.\n이제 년도별, 성별에 따른 비율(prop)의 합의 의미를 잘 생각해보면,\n이는 인기있는 1000개의 이름을 가진 아이들의 총 비율이 되는데,\n이 비율이 적다는 것은 더 다양한 이름이 사용되고 있다는 것을 의미합니다.\n아래 시각화에서 나타나듯이, 이름의 다양성은 시간이 지남에 따라 증가하며, 특히 여자 이름이 더욱 그러한 것을 알 수 있습니다.\n\n\ntop1000\n\n            name sex  births  year  prop\n0           Mary   F    7065  1880  0.08\n1           Anna   F    2604  1880  0.03\n2           Emma   F    2003  1880  0.02\n...          ...  ..     ...   ...   ...\n2039793   Ronnie   M     217  2021  0.00\n2039792  Merrick   M     217  2021  0.00\n2039791   Mayson   M     217  2021  0.00\n\n[283876 rows x 5 columns]\n\n\n\n\ncode\ntable = top1000.groupby([\"year\", \"sex\"])[\"prop\"].sum().reset_index()\n\n(\n    so.Plot(table, x='year', y='prop', color=\"sex\")\n    .add(so.Line())\n)\n\n\n\n\n\n\n\n\n\n\n\n7. 이름의 다양성의 변화를 다른 방식으로 살펴봅니다.\n\n예를 들어, 2010년에 한정해서 살펴보면, 이 해에 인기있는 이름 순으로 나열한 후,\n그 비율(prop)의 누적합이 0.5가 되는 지점을 찾아보면,\n인기 상위 50%의 이름이 몇 개나 되는지 알 수 있습니다.\n이 개수를 지표로 이용하면, 이름의 다양성이 어떻게 변화하는지를 살펴볼 수 있습니다.\n\n\ndf = top1000.query('sex == \"M\" and year == 2010')  # 2010년도에 태어난 남자아이들의 이름\nprop_cumsum = df[\"prop\"].sort_values(ascending=False).cumsum()  # 인기순서로 비율의 누적합\nprop_cumsum\n\n1678130   0.01\n1678131   0.02\n1678132   0.03\n          ... \n1679127   0.84\n1679128   0.84\n1679129   0.84\nName: prop, Length: 1000, dtype: float64\n\n\n\n# 누적 비율 50%까지의 이름의 개수\nprop_cumsum[prop_cumsum &lt;= 0.5].size + 1  # .size: Series의 열의 개수\n\n117\n\n\n이제 위 예를 이용하여, 각 연도별, 성별에 따른 인기 상위 50% 이름의 개수를 구해보세요.\n\nyear, sex로 그룹핑을 한 후\ngrouped object에 인기 상위 50% 이름의 개수를 구하는 함수를 세워 apply로 적용해 보세요.\n이를 diversity라는 이름의 데이터프레임으로 저장하면, 다음과 같고,\n이를 이용해 아래와 같은 시각화를 해보세요.\n\n\n\ncode\ndef get_quantile_count(group, q=0.5):\n    prop_cumsum = group[\"prop\"].sort_values(ascending=False).cumsum()\n    return prop_cumsum[prop_cumsum &lt;= q].size + 1\n\ndiversity = top1000.groupby([\"year\", \"sex\"]).apply(get_quantile_count)\ndiversity = diversity.reset_index(name=\"count\")\n\n\n\ndiversity\n\n     year sex  count\n0    1880   F     38\n1    1880   M     14\n2    1881   F     38\n..    ...  ..    ...\n281  2020   M    163\n282  2021   F    276\n283  2021   M    167\n\n[284 rows x 3 columns]\n\n\n\n\ncode\n(\n    so.Plot(diversity, x='year', y='count', color=\"sex\")\n    .add(so.Line())\n)",
    "crumbs": [
      "Exploratory Analysis II",
      "Baby Names"
    ]
  },
  {
    "objectID": "contents/Transform/babynames.html#the-last-letter-revolution",
    "href": "contents/Transform/babynames.html#the-last-letter-revolution",
    "title": "Baby Names",
    "section": "The “last letter” revolution",
    "text": "The “last letter” revolution\n\n“In 2007, baby name researcher Laura Wattenberg pointed out that the distribution of boy names by final letter has changed significantly over the last 100 years.” (p. 452)\n\n이름의 마지막 철자는 이름의 느낌을 좌우해서인지 특히 자주 쓰이는 철자가 있으며, 그 트렌드도 시간에 따라 크게 변화하는 것 같습니다.\n이제, 이름의 마지막 철자가 시간에 따라 어떻게 변화하는지 살펴봅니다.\n\n8. 아래와 같이 이름의 마지막 철자를 last_letter라는 칼럼에 추가합니다.\n\n문자열의 마지막 철자를 구하는 함수를 구한 후,\nnames데이터의 name 칼럼을 Series로 추출 후 .map Series method를 이용하여 적용합니다.\n\n\n\ncode\n# 함수를 이용해 이름의 마지막 글자를 추출\ndef get_last_letter(x):\n    return x[-1]\n\nlast_letters = names[\"name\"].map(get_last_letter)\n\n# 또는 간단히 lambda 함수를 이용\nlast_letters = names[\"name\"].map(lambda x: x[-1])\n\n# DataFrame에 새로운 열로 추가\nnames[\"last_letter\"] = last_letters\n\n\n\nnames\n\n          name sex  births  year  prop last_letter\n0         Mary   F    7065  1880  0.08           y\n1         Anna   F    2604  1880  0.03           a\n2         Emma   F    2003  1880  0.02           a\n...        ...  ..     ...   ...   ...         ...\n2052778  Zyian   M       5  2021  0.00           n\n2052779  Zylar   M       5  2021  0.00           r\n2052780    Zyn   M       5  2021  0.00           n\n\n[2052781 rows x 6 columns]\n\n\n\n\n9. 이제 아래와 같이 각 연도별, 성별에 따른 last_letter로 끝나는 이름을 가진 출생아 수를 구해보세요.\n\n\ncode\ntable = names.groupby([\"year\", \"sex\", \"last_letter\"])[\"births\"].sum().reset_index()\n\n\n\ntable\n\n      year sex last_letter  births\n0     1880   F           a   31446\n1     1880   F           d     609\n2     1880   F           e   33381\n...    ...  ..         ...     ...\n6734  2021   M           x   21014\n6735  2021   M           y   82625\n6736  2021   M           z    3519\n\n[6737 rows x 4 columns]\n\n\n\n\n10. 우선 3개 년도 1910, 1960, 2010에 한해 살펴보는데, births의 비율을 다음과 같이 시각화 해보세요.\n\n1910, 1960, 2010년도로 필터링한 후\n각 년도별, 성별에 따른 last_letter로 철자가 끝나는 출생아 수의 비율을 구합니다.\n이를 이용해 다음과 같이 시각화 해보세요.\n이 플랏에서 남자 이름의 경우, “d”, “n”, “y”로 끝나는 이름이 많고 변화가 크다는 것을 볼 수 있습니다.\n\n\n\ncode\n# Filtering\nsubtable = table.query('year in [1910, 1960, 2010]')\n\n# transform을 이용해서 비율을 구하거나\nsubtable[\"total\"] = subtable.groupby([\"year\", \"sex\"])[\"births\"].transform(\"sum\")\nsubtable = subtable.assign(prop = lambda x: x[\"births\"] / x[\"total\"])\n\n# 앞서와 같이 함수를 이용해서 비율을 구할 수도 있음\ndef add_prop(group):\n    group = group.assign(prop = lambda x: x.births / x.births.sum())\n    return group\n\nsubtable = subtable.groupby([\"year\", \"sex\"], group_keys=False).apply(add_prop)\ndisplay(subtable)\n\n# Visualization\n(\n    so.Plot(subtable, x='last_letter', y='prop', color=\"year\")\n    .add(so.Bar(), so.Dodge())\n    .facet(row=\"sex\")\n    .layout(size=(8, 6))\n    .scale(color=so.Nominal()) # year가 연속형 변수라 범주형으로 바꾸어 구별되는 색으로 표현되도록 함\n)\n\n\n      year sex last_letter  births    total  prop\n1260  1910   F           a  108399   396505  0.27\n1261  1910   F           c       5   396505  0.00\n1262  1910   F           d    6751   396505  0.02\n...    ...  ..         ...     ...      ...   ...\n6162  2010   M           x   16487  1917416  0.01\n6163  2010   M           y  111600  1917416  0.06\n6164  2010   M           z    3507  1917416  0.00\n\n[145 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n11. 이제 ‘d’, ‘n’, ’y’로 끝나는 남자 이름의 시간에 따른 변화를 다음과 같이 시각화 해보세요.\n\n위에 table 데이터를 이용해 모든 년도에 대해 last_letter의 비율을 구해보세요.\n남자아이이고 ‘d’, ‘n’, ’y’로 끝나는 이름으로 필터링 한 후 시각화 합니다.\n\n\n\ncode\n# 앞서 정의한 `add_prop` 함수를 이용해서 비율을 구함\ntable = table.groupby([\"year\", \"sex\"], group_keys=False).apply(add_prop)\n\n# visualization\n(\n    so.Plot(\n        table.query('sex == \"M\" and last_letter in [\"d\", \"n\", \"y\"]'),\n        x=\"year\",\n        y=\"prop\",\n        color=\"last_letter\",\n    )\n    .add(so.Line())\n)",
    "crumbs": [
      "Exploratory Analysis II",
      "Baby Names"
    ]
  },
  {
    "objectID": "contents/Transform/babynames.html#boy-names-that-became-girl-names-and-vice-versa",
    "href": "contents/Transform/babynames.html#boy-names-that-became-girl-names-and-vice-versa",
    "title": "Baby Names",
    "section": "Boy names that became girl names (and vice versa)",
    "text": "Boy names that became girl names (and vice versa)\n남자 아이의 이름이였다가 여자 아이의 이름으로 점차 변하고 있는 경우가 있습니다. (혹은 반대)\n\n아래와 같이 Leslie와 비슷한 철자를 가진 발음이 비슷한 이름들을 살펴보면,\n\n\n# 유닉크한 이름을 Series로 추출\nall_names = pd.Series(top1000[\"name\"].unique())  # .unique(): numpy array로 반환됨\n\n# `Lesl` 문자열을 포함하는 이름들 추출\nlesley_like = all_names[all_names.str.contains(\"Lesl\")]  # Series의 method로 .str.contains()는 문자열 포함 여부를 판단\n\n\nlesley_like\n\n632     Leslie\n2293    Lesley\n4263    Leslee\n4731     Lesli\n6106     Lesly\ndtype: object\n\n\n\n12. 이 lesley_like에 해당하는 이름들로 top1000 데이터를 필터링하여 filtered에 저장하세요.\n\n\ncode\n# query를 이용하거나\nfiltered = top1000.query('name in @lesley_like')  # @: 외부 변수를 참조할 때 사용\n\n# .isin()을 이용\nfiltered = top1000[top1000[\"name\"].isin(lesley_like)]\n\n\n\nfiltered\n\n           name sex  births  year  prop\n654      Leslie   F       8  1880  0.00\n1108     Leslie   M      79  1880  0.00\n2522     Leslie   F      11  1881  0.00\n...         ...  ..     ...   ...   ...\n1958254  Leslie   F     571  2019  0.00\n1990386  Leslie   F     486  2020  0.00\n2021863  Leslie   F     476  2021  0.00\n\n[415 rows x 5 columns]\n\n\n\n\n13. filtered 데이터에서 Leslie_like에 포함된 이름들이 각 연도별로, 남자로 쓰인 비율과 여자로 쓰인 비율을 구해보세요.\n\n우선, filtered 데이터에서 연도별, 성별에 따른 출생아수의 합를 구한 후\n이를 연도에 따른 비율을 계산하면, 각 연도에서 Leslie와 같은 이름들이 남자로 쓰인 비율과 여자로 쓰인 비율을 구할 수 있습니다.\n즉, 아래 플랏에서 보면 각 연도에서 남자로 쓰인 비율과 여자로 쓰인 비율의 합이 1이 됩니다.\n\n\n\ncode\n# 연도별, 성별에 따른 출생아수의 합\ntable = filtered.groupby([\"year\", \"sex\"])[\"births\"].sum().reset_index()\n\n# 긱 연도에 따른 비율\ntable = table.groupby(\"year\", group_keys=False).apply(add_prop)\n\n# Visualization\n(\n    so.Plot(table, x='year', y='prop', color=\"sex\")\n    .add(so.Line())\n)",
    "crumbs": [
      "Exploratory Analysis II",
      "Baby Names"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling_misc.html",
    "href": "contents/Modelling/modelling_misc.html",
    "title": "Model Buidling Misc.",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nbikeshare = pd.read_csv(\"../data/hour.csv\")\nbikeshare_daily = pd.read_csv(\"../data/day.csv\")\n\n\ndef clean_data(df):\n    df.rename({\"dteday\": \"date\", \"cnt\": \"count\"}, axis=1, inplace=True)\n\n    df = df.assign(\n        date=lambda x: pd.to_datetime(x[\"date\"]),  # datetime type으로 변환\n        year=lambda x: x[\"date\"].dt.year.astype(str),  # year 추출\n        day=lambda x: x[\"date\"].dt.day_of_year,  # day of the year 추출\n        month=lambda x: x[\"date\"].dt.month_name().str[:3],  # month 추출\n        wday=lambda x: x[\"date\"].dt.day_name().str[:3],  # 요일 추출\n    )\n\n    df[\"season\"] = (\n        df[\"season\"]\n        .map({1: \"winter\", 2: \"spring\", 3: \"summer\", 4: \"fall\"})  # season을 문자열로 변환\n        .astype(\"category\")  # category type으로 변환\n        .cat.set_categories(\n            [\"winter\", \"spring\", \"summer\", \"fall\"], ordered=True\n        )  # 순서를 지정\n    )\n    return df\n\n\nbikes = clean_data(bikeshare)\nbikes_daily = clean_data(bikeshare_daily)\nbikes_daily.head(3)\n\n   instant       date  season  yr  mnth  holiday  weekday  workingday  \\\n0        1 2011-01-01  winter   0     1        0        6           0   \n1        2 2011-01-02  winter   0     1        0        0           0   \n2        3 2011-01-03  winter   0     1        0        1           1   \n\n   weathersit  temp  atemp  hum  windspeed  casual  registered  count  year  \\\n0           2  0.34   0.36 0.81       0.16     331         654    985  2011   \n1           2  0.36   0.35 0.70       0.25     131         670    801  2011   \n2           1  0.20   0.19 0.44       0.25     120        1229   1349  2011   \n\n   day month wday  \n0    1   Jan  Sat  \n1    2   Jan  Sun  \n2    3   Jan  Mon",
    "crumbs": [
      "Modelling",
      "Misc."
    ]
  },
  {
    "objectID": "contents/Modelling/modelling_misc.html#변수의-상대적-중요도",
    "href": "contents/Modelling/modelling_misc.html#변수의-상대적-중요도",
    "title": "Model Buidling Misc.",
    "section": "변수의 상대적 중요도",
    "text": "변수의 상대적 중요도\n다양한 방식이 제안되어 있고 각각 한계가 있음. 예를 들어,\n\n선형모형의 경우 표준화된 회귀계수(또는 T value), 부분 상관계수(partial, semi-partial correlation), 위계적 모형의 비교\nSensitivity analysis; SALib\nPermutation importance; scikit-learn의 permutation_importance\nTree 기반 feature importance; interpretML\nSHAP value\n\n\n위계적 모형의 비교\n\nimport statsmodels.formula.api as smf\n\nmod_r_year = smf.ols(\"registered ~ year\", data=bikes_daily).fit()\nmod_r_temp = smf.ols(\"registered ~ year + temp + I(temp**2)\", data=bikes_daily).fit()\nmod_r_month = smf.ols(\"registered ~ year + temp + I(temp**2) + month\", data=bikes_daily).fit()\nmod_r_wday = smf.ols(\"registered ~ year + temp + I(temp**2) + month + wday\", data=bikes_daily).fit()\n\nprint(\n    f\" year: {mod_r_year.rsquared:.3f}\\n\",\n    f\"year + temperature: {mod_r_temp.rsquared:.3f}\\n\",\n    f\"year + temperature + months: {mod_r_month.rsquared:.3f}\\n\",\n    f\"year + temperature + months + days of the week: {mod_r_wday.rsquared:.3f}\",\n)\n\n year: 0.353\n year + temperature: 0.653\n year + temperature + months: 0.686\n year + temperature + months + days of the week: 0.757\n\n\n\n\nT value / 표준화된 회귀계수\n\nprint(mod_r_wday.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             registered   R-squared:                       0.757\nModel:                            OLS   Adj. R-squared:                  0.750\nMethod:                 Least Squares   F-statistic:                     110.6\nDate:                Mon, 09 Jun 2025   Prob (F-statistic):          1.66e-202\nTime:                        03:42:48   Log-Likelihood:                -5894.3\nNo. Observations:                 731   AIC:                         1.183e+04\nDf Residuals:                     710   BIC:                         1.193e+04\nDf Model:                          20                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept      -74.2636    343.767     -0.216      0.829    -749.185     600.658\nyear[T.2012]  1777.6982     58.315     30.484      0.000    1663.208    1892.189\nmonth[T.Aug]   777.0600    185.742      4.184      0.000     412.390    1141.730\nmonth[T.Dec]   113.2666    153.852      0.736      0.462    -188.793     415.326\nmonth[T.Feb]  -438.5490    162.694     -2.696      0.007    -757.968    -119.130\nmonth[T.Jan]  -433.1165    180.380     -2.401      0.017    -787.258     -78.975\nmonth[T.Jul]   710.4538    212.142      3.349      0.001     293.953    1126.954\nmonth[T.Jun]   829.2689    177.396      4.675      0.000     480.986    1177.552\nmonth[T.Mar]  -245.4253    145.140     -1.691      0.091    -530.381      39.530\nmonth[T.May]   420.7856    151.166      2.784      0.006     124.000     717.571\nmonth[T.Nov]   446.4927    148.101      3.015      0.003     155.725     737.260\nmonth[T.Oct]   716.5870    141.358      5.069      0.000     439.057     994.117\nmonth[T.Sep]   876.4995    156.341      5.606      0.000     569.554    1183.445\nwday[T.Mon]   -270.8906    107.946     -2.509      0.012    -482.823     -58.958\nwday[T.Sat]   -794.1801    107.955     -7.357      0.000   -1006.129    -582.231\nwday[T.Sun]  -1012.2886    107.987     -9.374      0.000   -1224.300    -800.277\nwday[T.Thu]    122.9289    108.255      1.136      0.257     -89.609     335.467\nwday[T.Tue]     -9.2864    108.269     -0.086      0.932    -221.851     203.278\nwday[T.Wed]     32.8857    108.244      0.304      0.761    -179.630     245.402\ntemp          1.019e+04   1268.580      8.034      0.000    7701.355    1.27e+04\nI(temp ** 2) -8061.8141   1317.345     -6.120      0.000   -1.06e+04   -5475.456\n==============================================================================\nOmnibus:                      255.792   Durbin-Watson:                   0.990\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1060.594\nSkew:                          -1.584   Prob(JB):                    4.95e-231\nKurtosis:                       7.979   Cond. No.                         86.1\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nPermutation importance\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nfrom patsy import dmatrices\ny, X = dmatrices(\"registered ~ year + temp + I(temp**2) + month + wday\", data=bikes_daily, return_type=\"dataframe\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.5)\n\nlr = LinearRegression(fit_intercept=False)\nlr.fit(X_train, y_train)\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\n\nfrom sklearn.inspection import permutation_importance\n\nimp_permu = permutation_importance(lr, X_test, y_test, n_repeats=30, scoring=\"r2\")  # default: R-squared\n\n\nimp_permu_df2 = pd.DataFrame({\"mean\": imp_permu.importances_mean, \"std\": imp_permu.importances_std}, index=X.columns)\n\n# plot imp_permu_df2 with error bars\nsns.set_theme(style=\"whitegrid\")\nimp_permu_df2[\"mean\"].sort_values().plot(kind=\"barh\", xerr=imp_permu_df2[\"std\"]);\n\n\n\n\n\n\n\n\n\nimp_permu_df2.query(\"mean &lt; 0.2\")[\"mean\"].sort_values().plot(kind=\"barh\", xerr=imp_permu_df2[\"std\"]);",
    "crumbs": [
      "Modelling",
      "Misc."
    ]
  },
  {
    "objectID": "contents/Modelling/modelling_misc.html#ml-모형을-이용한-fitted-lines",
    "href": "contents/Modelling/modelling_misc.html#ml-모형을-이용한-fitted-lines",
    "title": "Model Buidling Misc.",
    "section": "ML 모형을 이용한 fitted lines",
    "text": "ML 모형을 이용한 fitted lines\n\n# 2012년 데이터만 이용\nbikes_daily_2012 = bikes_daily.query('year == \"2012\"')\n\n\nSpline Fits\nB-spline을 이용한 모형\n\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\ndef spline_df(df, x, y, n_knots=10, **kwargs):\n    \"\"\"\n    Build a dataframe of Spline regression.\n    \"\"\"\n\n    df.dropna(subset=[x, y], inplace=True)\n    X_, y_ = df[[x]].values, df[y].values\n\n    B_basis = SplineTransformer(n_knots=n_knots)\n    lr = LinearRegression()\n    spline = make_pipeline(B_basis, lr).fit(X_, y_)\n\n    # make a grid for prediction\n    XX = np.linspace(df[x].min(), df[x].max(), 100).reshape(-1, 1)\n    yy = spline.predict(XX)\n\n    return pd.DataFrame({\"x_ax\": XX[:, 0], \"y_ax\": yy})\n\n\nspline_fitted = spline_df(bikes_daily_2012, x=\"temp\", y=\"registered\", n_knots=8)\n\n(\n    so.Plot(bikes_daily_2012, x='temp', y='registered')\n    .add(so.Dots(color=\".6\"))\n    .add(so.Line(), so.PolyFit(2))\n    .add(so.Line(color=\"darkorange\"), x=spline_fitted[\"x_ax\"], y=spline_fitted[\"y_ax\"])\n)\n\n\n\n\n\n\n\n\n\n\nGeneralized Additive Model (GAM)\n참고 pyGAM\n2025/5/12 scipy 1.13.1 (python 3.12)\n\nfrom pygam import LinearGAM, s\n\ndef pspline_df(df, x, y, width=0.90, lam=0, **kwargs):\n    \"\"\"\n    Build a dataframe of penalized spline regression.\n    \"\"\"\n\n    df.dropna(subset=[x, y], inplace=True)\n    X_, y_ = df[[x]], df[y]\n    gam = LinearGAM(s(0, lam=lam, **kwargs)).fit(X_, y_)  # spline regression\n\n    # make a grid for prediction\n    XX = gam.generate_X_grid(term=0)\n    yy = gam.predict(XX)\n    yy_se = gam.prediction_intervals(XX, width=width)  # 90% prediction interval\n\n    return pd.DataFrame(\n        {\"x_ax\": XX[:, 0], \"y_ax\": yy, \"ymin\": yy_se[:, 0], \"ymax\": yy_se[:, 1]}\n    )\n\n\npsplines = pspline_df(bikes_daily_2012, x=\"temp\", y=\"registered\", n_splines=20, lam=5)\n\n(\n    so.Plot(bikes_daily_2012, x='temp', y='registered')\n    .add(so.Dots(color=\".6\"))\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\"darkorange\"), x=psplines[\"x_ax\"], y=psplines[\"y_ax\"])  # penallized spline fit\n    .add(so.Band(color=\".3\"), x=psplines[\"x_ax\"], ymin=psplines[\"ymin\"], ymax=psplines[\"ymax\"], y=None)  # 90% prediction interval\n)\n\n\n\n\n\n\n\n\n\n\nSeaborn.objects의 객체로 생성\nso.PolyFit()을 수정\n\nso.PolyFit?\n\n\nfrom __future__ import annotations\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\n\nfrom seaborn._stats.base import Stat\n\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n@dataclass\nclass SplineFit(Stat):\n    \"\"\"\n    Fit a spline of the given n_knots and resample data onto predicted curve.\n    \"\"\"\n    # This is a provisional class that is useful for building out functionality.\n    # It may or may not change substantially in form or dissappear as we think\n    # through the organization of the stats subpackage.\n\n    n_knots: int = 5\n    gridsize: int = 100\n\n    def _fit_predict(self, data):\n\n        x = data[\"x\"]\n        y = data[\"y\"]\n        if x.nunique() &lt;= self.n_knots:\n            # TODO warn?\n            xx = yy = []\n        else:\n            B_basis = SplineTransformer(n_knots=self.n_knots)\n            lr = LinearRegression()\n            spline = make_pipeline(B_basis, lr).fit(x.values.reshape(-1, 1), y)\n            xx = np.linspace(x.min(), x.max(), self.gridsize)\n            yy = spline.predict(xx.reshape(-1, 1))\n\n        return pd.DataFrame(dict(x=xx, y=yy))\n\n    # TODO we should have a way of identifying the method that will be applied\n    # and then only define __call__ on a base-class of stats with this pattern\n\n    def __call__(self, data, groupby, orient, scales):\n\n        return (\n            groupby\n            .apply(data.dropna(subset=[\"x\", \"y\"]), self._fit_predict)\n        )\n\n\n# seaborn.objects의 객체로 변환\nso.Spline = SplineFit\n\n\n(\n    so.Plot(bikes_daily_2012, x='temp', y='registered', color=\"season\")\n    .add(so.Dots())\n    .add(so.Line(), so.Spline(3)) # Spline fit with 3 knots\n)",
    "crumbs": [
      "Modelling",
      "Misc."
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2.html",
    "href": "contents/Modelling/modelling2.html",
    "title": "Model building II",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Modelling",
      "Model building II"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2.html#day-of-week-요일",
    "href": "contents/Modelling/modelling2.html#day-of-week-요일",
    "title": "Model building II",
    "section": "1. Day of week (요일)",
    "text": "1. Day of week (요일)\n항공편의 개수는 요일에 따른 효과가 크게 미치기 때문에 우선 요일의 효과를 살펴보면서 장기간의 트렌드를 이해하고자 함\n\n# Add a column for the day of the week\ndaily[\"wday\"] = pd.Categorical(\n    daily[\"date\"].dt.day_name().str[:3],\n    categories=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"],\n    ordered=True,\n)\n\n\ndaily\n\n          date    n wday\n0   2013-01-01  842  Tue\n1   2013-01-02  943  Wed\n2   2013-01-03  914  Thu\n..         ...  ...  ...\n362 2013-12-29  888  Sun\n363 2013-12-30  968  Mon\n364 2013-12-31  776  Tue\n\n[365 rows x 3 columns]\n\n\n\nfrom sbcustom import boxplot\nboxplot(daily, \"wday\", \"n\", alpha=.5)\n\n\n\n\n\n\n\n\n매우 강한 주말 효과를 제거하기 위해 모델을 만들어 residuals을 얻음\n\nimport statsmodels.formula.api as smf\n\nmod = smf.ols(\"n ~ wday\", data=daily).fit()\nmod.params\n\nIntercept      891.48\nwday[T.Mon]     83.33\nwday[T.Tue]     59.88\nwday[T.Wed]     71.21\nwday[T.Thu]     74.27\nwday[T.Fri]     75.98\nwday[T.Sat]   -146.87\ndtype: float64\n\n\n\n# Prediction\ngrid = pd.DataFrame({\"wday\": [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]})\ngrid[\"pred\"] = mod.predict(grid)\ngrid\n\ndaily.groupby(\"wday\")[\"n\"].mean()\n\n\n\n\n\n\n\n  wday   pred\n0  Sun 891.48\n1  Mon 974.81\n2  Tue 951.36\n3  Wed 962.69\n4  Thu 965.75\n5  Fri 967.46\n6  Sat 744.62\n\n\nwday\nSun   891.48\nMon   974.81\nTue   951.36\nWed   962.69\nThu   965.75\nFri   967.46\nSat   744.62\nName: n, dtype: float64\n\n\n\n\n(\n    boxplot(daily, \"wday\", \"n\", alpha=.5)\n    .add(so.Dot(color=\"blue\", marker=\"&lt;\"), x=grid.wday, y=grid.pred)  # prediction 추가\n    .scale(x=so.Nominal(order=[\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]))\n)\n\n\n\n\n\n\n\n\nResiduals: 요일의 효과를 제거한 후의 날짜와 항공편의 개수의 관계\n\n# Add a column of residuals from mod\ndaily[\"resid\"] = mod.resid\n\n\n(\n    so.Plot(daily, x='date', y='resid')\n    .add(so.Line())\n    .add(so.Line(color=\"red\", linestyle=\":\"), so.Agg(lambda x: 0))\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n살펴볼 것들\n\n6월부터 모델이 잡아내지 못한 패턴들이 심해짐. 이를 위해 우선, 요일별로 잔차를 들여다 보면,\n\n\n(\n    so.Plot(daily, x='date', y='resid', color=\"wday\")\n    .add(so.Line())\n    .add(so.Line(color=\".6\", linestyle=\":\"), so.Agg(lambda x: 0))\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n토요일의 패턴을 잡아내지 못했음:\n토요일 여름에 모델의 기대치보다(즉, 토요일 1년 평균보다) 더 많은 항공편이 있고, 가을에는 반대로 더 적은 항공편이 있음. 뒤에 이를 고려한 새로운 모델을 세워 봄\n\n특히나 적은 항공편이 있는 날들이 있어서 그 날들을 살펴보면\n\n\ndaily.query('resid &lt; -100')\n\n          date    n wday   resid\n0   2013-01-01  842  Tue -109.36\n19  2013-01-20  786  Sun -105.48\n145 2013-05-26  729  Sun -162.48\n184 2013-07-04  737  Thu -228.75\n185 2013-07-05  822  Fri -145.46\n243 2013-09-01  718  Sun -173.48\n331 2013-11-28  634  Thu -331.75\n332 2013-11-29  661  Fri -306.46\n357 2013-12-24  761  Tue -190.36\n358 2013-12-25  719  Wed -243.69\n364 2013-12-31  776  Tue -175.36\n\n\n미국 공휴일에 해당하는 날들로 보임: 새해, 독립기념일(7/4), 추수감사절, 크리스마스 등등\n\n\n공휴일 확인\nimport holidays\n\nus_holidays = holidays.UnitedStates()\ndf = daily.query('resid &lt; -100')\ndf['holiday_name'] = df['date'].apply(lambda x: us_holidays.get(x, \"Not a holiday\"))\n\ndf\n\n\n          date    n wday   resid      holiday_name\n0   2013-01-01  842  Tue -109.36    New Year's Day\n19  2013-01-20  786  Sun -105.48     Not a holiday\n145 2013-05-26  729  Sun -162.48     Not a holiday\n184 2013-07-04  737  Thu -228.75  Independence Day\n185 2013-07-05  822  Fri -145.46     Not a holiday\n243 2013-09-01  718  Sun -173.48     Not a holiday\n331 2013-11-28  634  Thu -331.75      Thanksgiving\n332 2013-11-29  661  Fri -306.46     Not a holiday\n357 2013-12-24  761  Tue -190.36     Not a holiday\n358 2013-12-25  719  Wed -243.69     Christmas Day\n364 2013-12-31  776  Tue -175.36     Not a holiday\n\n\n\n일년에 걸쳐 나타나는 장기간의 트렌드가 있어보임: so.Polyfit(5)을 이용해 시각화해보면\n\n\n(\n    so.Plot(daily, x='date', y='resid')\n    .add(so.Line(color=\".5\"))\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\"red\", linestyle=\":\"), so.Agg(lambda x: 0))\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n다항함수 fit 보다 좀 더 flexible한 natural spline으로 fit을 하면,\npatsy 패키지의 cr 함수를 이용: spline regression\n\n# 일년 중 몇 일째인지 수치형 변수로 추가: cr함수가 datetime을 처리하지 못함.\ndaily[\"day\"] = daily[\"date\"].dt.day_of_year\n\n\n# 10개의 조각으로 이루어진 cubic(3차) natural spline\nfit_spline = smf.ols(\"resid ~ cr(day, df=10)\", data=daily).fit()\ndaily[\"ns_fitted\"] = fit_spline.fittedvalues\n\n\n\n\n\n\n\nNote\n\n\n\nNatual spline은 일반적인 B-splines에 비해 양 끝단에서 flat하도록 제한을 둔 것\nGeneral B-splines의 예: patsy 패키지의 bs 함수를 이용\n다음은 1차 함수(degree=1) 즉, 직선의 10개(df=10)로 나누어진 piecewise polynomial로 fit,\nfit_bspline = smf.ols(\"resid ~ bs(day, df=10, degree=1)\", data=daily).fit()\ndaily[\"bs_fitted\"] = fit_bspline.fittedvalues\n\n\n\ndaily.head(7)\n\n        date    n wday   resid  day  ns_fitted\n0 2013-01-01  842  Tue -109.36    1     -56.91\n1 2013-01-02  943  Wed  -19.69    2     -56.86\n2 2013-01-03  914  Thu  -51.75    3     -56.80\n3 2013-01-04  915  Fri  -52.46    4     -56.75\n4 2013-01-05  720  Sat  -24.62    5     -56.68\n5 2013-01-06  832  Sun  -59.48    6     -56.61\n6 2013-01-07  933  Mon  -41.81    7     -56.54\n\n\n\np = (\n    so.Plot(daily, x='day', y='resid')\n    .add(so.Line(color=\".6\"))\n    .add(so.Line(color=\"red\", linestyle=\":\"), so.Agg(lambda x: 0))\n    .add(so.Line(), y=daily.ns_fitted)  # predicted line by natural spline model\n    .layout(size=(8, 4))\n)\np\n\n\n\n\n\n\n\n\n겨울에 좀 적으며, 여름에 많음: 여러 해의 데이터가 있다면 이 패턴을 구체화 할 수 있으나 2013년의 데이터만 있으므로 우리의 지식에 의존해서 설명해 볼 수 밖에 없음\n참고로 위에서 언급한 B-splines를 이용한 모델에 의한 예측값을 추가로 그리면,\n\n# Predicted line by B-spline model: 1차 함수(직선), 10개의 pieces로 나누어짐\nfit_bspline = smf.ols(\"resid ~ bs(day, df=10, degree=1)\", data=daily).fit()\ndaily[\"bs_fitted\"] = fit_bspline.fittedvalues\n\np.add(so.Line(color=\"blue\"), y=daily.bs_fitted)",
    "crumbs": [
      "Modelling",
      "Model building II"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2.html#seasonal-saturday-effect-계절",
    "href": "contents/Modelling/modelling2.html#seasonal-saturday-effect-계절",
    "title": "Model building II",
    "section": "2. Seasonal Saturday effect (계절)",
    "text": "2. Seasonal Saturday effect (계절)\n앞서 말한 살펴볼 3가지 중에 첫번째 부분, 즉 토요일에 대해 나타나는 패턴을 잡아내기 위해\n\n(\n    so.Plot(daily.query('wday == \"Sat\"'), x='date', y='n')\n    .add(so.Line(marker=\".\"))\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n여름에는 토요일에 여행하는 걸 마다하지 않는 것은 아마도 여름 방학이나 휴가철이라 그런 것일 듯,\nstate’s school terms: summer break in 2013 was Jun 26–Sep 9\n가을에 토요일 항공편이 적은 것은 미국에서는 추수감사절이나 크리스마스와 같은 큰 공휴일들이 있어 가족여행을 계획하지 않는다고 하는데 추측해 볼 뿐임.\n\n대략 3개의 school terms으로 나누어 우리의 추측을 확인해보고자 함\n\ndates_cut = pd.to_datetime([\"2013-01-01\", \"2013-06-05\", \"2013-08-25\", \"2014-01-01\"])\n\n# cut의 나눌 위치를 직접 지정\ndaily[\"term\"] = pd.cut(\n    daily[\"date\"], dates_cut, right=False, labels=[\"spring\", \"summer\", \"fall\"]\n)\n\n\n(\n    so.Plot(daily.query('wday == \"Sat\"'), x='date', y='n', color=\"term\")\n    .add(so.Line(marker=\".\"))\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n이 term 변수가 토요일이 아닌 다른 요일에는 어떻게 작용하는지 살펴보면\n\nboxplot(daily, x=\"wday\", y=\"n\", color=\"term\", alpha=.5)\n\n\n\n\n\n\n\n\n3개의 term에 따라 각 요일에서의 항공편의 개수가 큰 차이가 나는 것으로 보이며, 그 패턴이 요일마다 다른 것으로 보이므로, 각 term에 따라 요일의 효과를 분리해서 보는 것이 타당해보임.",
    "crumbs": [
      "Modelling",
      "Model building II"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2.html#model-building",
    "href": "contents/Modelling/modelling2.html#model-building",
    "title": "Model building II",
    "section": "Model building",
    "text": "Model building\n앞에서 탐색한 결과를 바탕으로, 하루에 출항하는 항공편의 개수를 예측하는데\n\n요일로만 예측하는 모델 (mod1)과\n요일과 term, 그리고 그 둘의 상호작용을 함께 고려한 모델 (mod2)을 세워 비교해보면,\n\n\nmod1 = smf.ols(\"n ~ wday\", data=daily).fit()  # 요일로만 예측\nmod2 = smf.ols(\"n ~ wday * term\", data=daily).fit()  # 요일과 term, 그리고 요일과 term의 interaction으로 예측\n\n\nfrom patsy import dmatrices\ny, X = dmatrices(\"n ~ wday * term\", data=daily, return_type=\"dataframe\")\nX.columns\n\nIndex(['Intercept', 'wday[T.Mon]', 'wday[T.Tue]', 'wday[T.Wed]', 'wday[T.Thu]',\n       'wday[T.Fri]', 'wday[T.Sat]', 'term[T.summer]', 'term[T.fall]',\n       'wday[T.Mon]:term[T.summer]', 'wday[T.Tue]:term[T.summer]',\n       'wday[T.Wed]:term[T.summer]', 'wday[T.Thu]:term[T.summer]',\n       'wday[T.Fri]:term[T.summer]', 'wday[T.Sat]:term[T.summer]',\n       'wday[T.Mon]:term[T.fall]', 'wday[T.Tue]:term[T.fall]',\n       'wday[T.Wed]:term[T.fall]', 'wday[T.Thu]:term[T.fall]',\n       'wday[T.Fri]:term[T.fall]', 'wday[T.Sat]:term[T.fall]'],\n      dtype='object')\n\n\n\ndaily = daily.assign(\n    without_term = mod1.resid,\n    with_term = mod2.resid,\n)\n\n\ndaily\n\n          date    n wday   resid  day  ns_fitted  bs_fitted    term  \\\n0   2013-01-01  842  Tue -109.36    1     -56.91     -55.93  spring   \n1   2013-01-02  943  Wed  -19.69    2     -56.86     -55.78  spring   \n2   2013-01-03  914  Thu  -51.75    3     -56.80     -55.63  spring   \n..         ...  ...  ...     ...  ...        ...        ...     ...   \n362 2013-12-29  888  Sun   -3.48  363     -39.96     -37.80    fall   \n363 2013-12-30  968  Mon   -6.81  364     -41.10     -38.74    fall   \n364 2013-12-31  776  Tue -175.36  365     -42.24     -39.68    fall   \n\n     without_term  with_term  \n0         -109.36     -98.26  \n1          -19.69      -8.64  \n2          -51.75     -51.36  \n..            ...        ...  \n362         -3.48      -7.00  \n363         -6.81     -11.26  \n364       -175.36    -167.58  \n\n[365 rows x 10 columns]\n\n\n\ndaily_models = daily.melt(id_vars=[\"date\"], value_vars=[\"without_term\", \"with_term\"], var_name=\"model\", value_name=\"residual\")\n\n\n(\n    so.Plot(daily_models, x='date', y='residual', color=\"model\")\n    .add(so.Line(alpha=.75))\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n시즌(term)을 포함한 모델이 약간 나아보이나 좀 더 살펴보고자, 원래의 데이터와 함께 플랏을 그려보면,\n\nwday_grid = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\nterm_grid = [\"spring\", \"summer\", \"fall\"]\n\n# make a grid of wday and term\nfrom itertools import product\ngrid = pd.DataFrame(\n    list(product(wday_grid, term_grid)),\n    columns=[\"wday\", \"term\"],\n)\ngrid\n\n   wday    term\n0   Sun  spring\n1   Sun  summer\n2   Sun    fall\n..  ...     ...\n18  Sat  spring\n19  Sat  summer\n20  Sat    fall\n\n[21 rows x 2 columns]\n\n\n\ngrid[\"pred_mod2\"] = mod2.predict(grid[[\"wday\", \"term\"]])\ngrid\n\n   wday    term  pred_mod2\n0   Sun  spring     872.23\n1   Sun  summer     923.91\n2   Sun    fall     895.00\n..  ...     ...        ...\n18  Sat  spring     737.32\n19  Sat  summer     800.92\n20  Sat    fall     716.00\n\n[21 rows x 3 columns]\n\n\n\ndaily_plot = daily.merge(grid)\ndaily_plot\n\n          date    n wday   resid  day  ns_fitted  bs_fitted    term  \\\n0   2013-01-01  842  Tue -109.36    1     -56.91     -55.93  spring   \n1   2013-01-02  943  Wed  -19.69    2     -56.86     -55.78  spring   \n2   2013-01-03  914  Thu  -51.75    3     -56.80     -55.63  spring   \n..         ...  ...  ...     ...  ...        ...        ...     ...   \n362 2013-12-29  888  Sun   -3.48  363     -39.96     -37.80    fall   \n363 2013-12-30  968  Mon   -6.81  364     -41.10     -38.74    fall   \n364 2013-12-31  776  Tue -175.36  365     -42.24     -39.68    fall   \n\n     without_term  with_term  pred_mod2  \n0         -109.36     -98.26     940.26  \n1          -19.69      -8.64     951.64  \n2          -51.75     -51.36     965.36  \n..            ...        ...        ...  \n362         -3.48      -7.00     895.00  \n363         -6.81     -11.26     979.26  \n364       -175.36    -167.58     943.58  \n\n[365 rows x 11 columns]\n\n\n\n(\n    boxplot(daily_plot, x=\"wday\", y=\"n\")\n    .add(so.Dot(color=\"blue\"), y=\"pred_mod2\")  # prediction 값을 추가\n    .facet(\"term\")\n    .layout(size=(7.5, 5))\n)\n\n\n\n\n\n\n\n\n이상치(ourliers)가 많아 평균(mean)의 값이 중앙값 (median)보다 많이 내려와 있는 경향을 보이고 있음.\n이는 이상치가 모델에 큰 영향을 미치기 때문인데, 이상치의 영향을 줄이는 방법 중에 하나인 robust regression 모형 (mod3)을 세워 해결해보면,\n\n# Robust Linear Model (RLM) in statsmodels\nmod3 = sm.RLM.from_formula(\"n ~ wday * term\", data=daily).fit()\n\n\n## Predictions\n# mod2: smf.ols(\"n ~ wday * term\", data=daily)\ngrid[\"with_term_ols\"] = mod2.predict(grid[[\"wday\", \"term\"]])\n\n# mod3: robust mod2\ngrid[\"with_term_rlm\"] = mod3.predict(grid[[\"wday\", \"term\"]])\n\n\n# Median 값 추가\ndaily_median = daily.groupby([\"wday\", \"term\"])[\"n\"].median().reset_index(name=\"median\")\n\n\ngrid = grid.merge(daily_median, on=[\"wday\", \"term\"])\ngrid\n\n   wday    term  pred_mod2  with_term_ols  with_term_rlm  median\n0   Sun  spring     872.23         872.23         893.50  901.00\n1   Sun  summer     923.91         923.91         923.91  929.00\n2   Sun    fall     895.00         895.00         900.09  902.00\n..  ...     ...        ...            ...            ...     ...\n18  Sat  spring     737.32         737.32         746.82  747.00\n19  Sat  summer     800.92         800.92         801.22  808.00\n20  Sat    fall     716.00         716.00         696.90  690.00\n\n[21 rows x 6 columns]\n\n\n\ngrid_long = grid.melt(id_vars=[\"wday\", \"term\"], value_vars=[\"with_term_ols\", \"with_term_rlm\", \"median\"], var_name=\"model\", value_name=\"n\")\n\n(\n    so.Plot(grid_long, x='wday', y='n', color=\"model\")\n    .add(so.Dot(edgecolor=\"w\", pointsize=8))\n    .facet(\"term\")\n    .layout(size=(7.5, 5))\n)\n\n\n\n\n\n\n\n\nFinal model\nmod3 = sm.RLM.from_formula(\"n ~ wday * term\", data=daily).fit()\n\nmod3.params\n\nIntercept                     893.50\nwday[T.Mon]                    72.65\nwday[T.Tue]                    59.34\nwday[T.Wed]                    66.27\nwday[T.Thu]                    77.63\nwday[T.Fri]                    73.70\nwday[T.Sat]                  -146.68\nterm[T.summer]                 30.40\nterm[T.fall]                    6.58\nwday[T.Mon]:term[T.summer]     -1.92\nwday[T.Tue]:term[T.summer]      6.69\nwday[T.Wed]:term[T.summer]      2.32\nwday[T.Thu]:term[T.summer]     -7.51\nwday[T.Fri]:term[T.summer]     -5.57\nwday[T.Sat]:term[T.summer]     23.99\nwday[T.Mon]:term[T.fall]        8.89\nwday[T.Tue]:term[T.fall]        1.72\nwday[T.Wed]:term[T.fall]        1.21\nwday[T.Thu]:term[T.fall]        3.82\nwday[T.Fri]:term[T.fall]        8.65\nwday[T.Sat]:term[T.fall]      -56.51\ndtype: float64\n\n\n\n(\n    so.Plot(grid, x='wday', y='with_term_rlm', color=\"term\")\n    .add(so.Line(marker=\".\"))\n    .layout(size=(7, 4))\n)\n\n\n\n\n\n\n\n\nResiduals for mod3\n\ndaily[\"with_term_rlm\"] = mod3.resid\n\n\ndaily_models = daily.melt(\n    id_vars=[\"date\"],\n    value_vars=[\"with_term\", \"with_term_rlm\"],\n    var_name=\"model\",\n    value_name=\"residual\",\n)\n\n\ndaily_models\n\n          date          model  residual\n0   2013-01-01      with_term    -98.26\n1   2013-01-02      with_term     -8.64\n2   2013-01-03      with_term    -51.36\n..         ...            ...       ...\n727 2013-12-29  with_term_rlm    -12.09\n728 2013-12-30  with_term_rlm    -13.63\n729 2013-12-31  with_term_rlm   -185.15\n\n[730 rows x 3 columns]\n\n\n\n(\n    so.Plot(daily_models, x='date', y='residual', color=\"model\")\n    .add(so.Line())\n    .layout(size=(7, 4))\n)\n\n\n\n\n\n\n\n\n이제 1년에 걸친 장기 트렌드를 더 잘 볼 수 있고, positive한 이상치와 negative한 이상치도 확연히 들어남",
    "crumbs": [
      "Modelling",
      "Model building II"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2.html#time-of-year-an-alternative-approach",
    "href": "contents/Modelling/modelling2.html#time-of-year-an-alternative-approach",
    "title": "Model building II",
    "section": "Time of year: an alternative approach",
    "text": "Time of year: an alternative approach\n\n위에서는 모델을 향상시키기 위해 우리의 지식을 이용하였는데, 이에 반해\n좀 더 유동적인 모델을 이용하여 데이터의 패턴을 잘 잡아내주도록 적절한 모델을 생성하는 방식도 있음\n위의 예에서는 선형모델로는 부족하니 natural (cubic) spline을 이용해서 부드러운 곡선에 fit해볼 수 있음\n\n\nmod_spline = sm.RLM.from_formula(\"n ~ wday * cr(day, df=5)\", data=daily).fit()\n\n\ndaily[\"spline_fit\"] = mod_spline.fittedvalues\n\n\ndaily\n\n          date    n wday   resid  day  ns_fitted  bs_fitted    term  \\\n0   2013-01-01  842  Tue -109.36    1     -56.91     -55.93  spring   \n1   2013-01-02  943  Wed  -19.69    2     -56.86     -55.78  spring   \n2   2013-01-03  914  Thu  -51.75    3     -56.80     -55.63  spring   \n..         ...  ...  ...     ...  ...        ...        ...     ...   \n362 2013-12-29  888  Sun   -3.48  363     -39.96     -37.80    fall   \n363 2013-12-30  968  Mon   -6.81  364     -41.10     -38.74    fall   \n364 2013-12-31  776  Tue -175.36  365     -42.24     -39.68    fall   \n\n     without_term  with_term  with_term_rlm  spline_fit  \n0         -109.36     -98.26        -110.84      865.91  \n1          -19.69      -8.64         -16.78      888.63  \n2          -51.75     -51.36         -57.14      913.36  \n..            ...        ...            ...         ...  \n362         -3.48      -7.00         -12.09      885.37  \n363         -6.81     -11.26         -13.63      967.40  \n364       -175.36    -167.58        -185.15      935.67  \n\n[365 rows x 12 columns]\n\n\n\n(\n    so.Plot(daily, x='date', y='spline_fit', color=\"wday\")\n    .add(so.Line(marker=\".\"))\n    .layout(size=(7, 5))\n)\n\n\n\n\n\n\n\n\nResiduals for mod_spline\n\ndaily[\"spline_resid\"] = mod_spline.resid\n\n\n(\n    so.Plot(daily, x='date', y='spline_resid')\n    .add(so.Line())\n    .layout(size=(7, 4))\n)\n\n\n\n\n\n\n\n\n앞서 두 개의 명목변수의 예측한 선형모형, sm.RLM.from_formula(\"n ~ wday * term\", data=daily) 과 그 잔차를 비교해 보았을 때 (아래 그림), 크게 다르지 않는 다는 점에서 우리의 모형이 만족스럽다고 볼 수 있음.\nRegression vs. natural spline\n\ndaily_models2 = daily.melt(\n    id_vars=[\"date\"],\n    value_vars=[\"with_term_rlm\", \"spline_resid\"],\n    var_name=\"model\",\n    value_name=\"residual\",\n)\n\n\n(\n    so.Plot(daily_models2, x='date', y='residual', color=\"model\")\n    .add(so.Line())\n    .layout(size=(7, 4))\n)\n\n\n\n\n\n\n\n\n참고: 좀 더 복잡한 모델; degree of freedom=10\n\nmod_spline2 = sm.RLM.from_formula(\"n ~ wday * cr(day, df=10)\", data=daily).fit()\ndaily[\"spline_fit2\"] = mod_spline2.fittedvalues\n\n\n\nNatural spline fit with df=10\n(\n    so.Plot(daily, x='day', y='spline_fit2', color=\"wday\")\n    .add(so.Line())\n    .add(so.Dots(color='.5'), y=\"n\")\n    .facet(\"term\")\n    .share(x=False)\n    .layout(size=(9, 6))\n)\n\n\n\n\n\n\n\n\n\n\n\nNatural spline fit with df=5\n(\n    so.Plot(daily, x='day', y='spline_fit', color=\"wday\")\n    .add(so.Line())\n    .add(so.Dots(color='.5'), y=\"n\")\n    .facet(\"term\")\n    .share(x=False)\n    .layout(size=(7, 5))\n)\n\n\n\n\n\n\n\n\n\nQ: 왜 앞서 지식에 기반해 패턴을 찾아 복잡한 과정을 거쳐 모형을 만들었나?",
    "crumbs": [
      "Modelling",
      "Model building II"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2.html#연습문제",
    "href": "contents/Modelling/modelling2.html#연습문제",
    "title": "Model building II",
    "section": "연습문제",
    "text": "연습문제\n\nWithout interactions: what would you expect the model n ~ wday + cr(date, df=5)to look like? Knowing what you know about the data, why would you expect it to be not particularly effective?\n\n다음과 같은 Interaction이 있는 모델과 없는 모델을 비교하기 위해 residual plot을 함께 그려본 후 interaction이 없는 모델에서 상대적으로 어떻게 예측이 다른지 살펴보세요.\nmod5 = smf.ols(\"n ~ wday + cr(day, df=5)\", data=daily)\nmod6 = smf.ols(\"n ~ wday * cr(day, df=5)\", data=daily)\n\nWhat happens if you fit a day of week effect that varies by month (i.e. n ~ wday * month)? Why is this not very helpful?\n\nn ~ wday * month를 이용해 모델을 세워보고, 이 모델이 어떤 문제를 가지고 있는지 살펴보세요.\n즉, 위의 예제에서는 term을 이용 (3개의 변수)하여 요일의 효과를 분리하였는데, month를 이용 (12개의 변수를 요구)하여 요일의 효과를 분리하는 것이 의미있을까?",
    "crumbs": [
      "Modelling",
      "Model building II"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1.html",
    "href": "contents/Modelling/modelling1.html",
    "title": "Model Building I",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Modelling",
      "Model Building I"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1.html#price-and-carat",
    "href": "contents/Modelling/modelling1.html#price-and-carat",
    "title": "Model Building I",
    "section": "Price and carat",
    "text": "Price and carat\n다이아몬드의 퀄리티(cut, color, clarity)가 좋을수록 가벼워짐\n\nprice = (\n    so.Plot(diamonds, x='carat', y='price')\n    .add(so.Dots(alpha=.1, color=\".6\"))\n)\ncut = rangeplot(diamonds, x=\"cut\", y=\"carat\")\ncolor = rangeplot(diamonds, x=\"color\", y=\"carat\")\nclarity = rangeplot(diamonds, x=\"clarity\", y=\"carat\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 다이아몬드의 퀄리티가 가격에 주는 영향/예측을 정확히 파악하기 위해 다음과 같은 절차를 통해 가격 대신 “캐럿으로 설명되지 않는 가격”(residuals)으로 종속변수를 대체함\n\n우선, 2.5캐럿 이하로 제한하고,\n가격과 캐럿을 log-transform하여 선형모형을 세움\n이 모형으로 잔차를 구하고,\n다이아몬드의 퀄리티와 이 잔차와의 관계를 살펴봄\n\n\ndiamonds2 = diamonds.query(\"carat &lt; 2.5\").assign(\n    lprice=lambda x: np.log2(x.price), \n    lcarat=lambda x: np.log2(x.carat)\n)\n\n\nlog_plot = (\n    so.Plot(diamonds2, x=\"lcarat\", y=\"lprice\")\n    .add(so.Dots(color=\".6\", alpha=0.1))\n    .add(so.Line(), so.PolyFit(5))\n)\n\nplot = (\n    so.Plot(diamonds2, x=\"carat\", y=\"price\")\n    .add(so.Dots(color=\".6\", alpha=0.1))\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 캐럿으로 가격을 예측하는 선형모형\nimport statsmodels.formula.api as smf\n\nmod_diamonds = smf.ols(\"lprice ~ lcarat\", data=diamonds2).fit()\n\n\n# Parmeters\nmod_diamonds.params\n\nIntercept   12.19\nlcarat       1.68\ndtype: float64\n\n\nModel: \\(\\displaystyle lprice = 1.68 \\cdot lcarat + 12.19 + e\\)\n또는 \\(\\displaystyle \\widehat{lprice} = 1.68 \\cdot lcarat + 12.19\\)\n\nPrediction\n\n# Data range from the carat variable\ngrid = pd.DataFrame({\"carat\": []})\ngrid[\"carat\"]= np.linspace(diamonds2.carat.min(), diamonds2.carat.max(), 20)\ngrid = grid.assign(\n    lcarat=lambda x: np.log2(x.carat),  # 모형의 변수와 동일하게 log 변환\n    lprice=lambda x: mod_diamonds.predict(x.lcarat),  # prediction\n    price=lambda x: 2**x.lprice  # 원래 단위로 되돌리기\n)\n\n\ngrid\n\n    carat  lcarat  lprice    price\n0    0.20   -2.32    8.29   312.79\n1    0.32   -1.64    9.43   691.47\n2    0.44   -1.18   10.21  1182.86\n..    ...     ...     ...      ...\n17   2.25    1.17   14.16 18318.33\n18   2.37    1.24   14.29 19999.52\n19   2.49    1.32   14.41 21740.08\n\n[20 rows x 4 columns]\n\n\n\n(\n    so.Plot(grid, x=\"carat\", y=\"price\")\n    .add(so.Line())\n    .add(so.Dots(color=\".6\", alpha=0.1), x=diamonds2.carat, y=diamonds2.price)\n)\n\n\n\n\n\n\n\n\n\n캐럿과 가격은 비선형적인 관계에 있으며, 이를 log-transform하여 선형적인 관계로 만들어줌\n또한, variability는 캐럿이 증가함에 따라 비례해서 커지는 양상을 보임; 이 또한 log-transform을 통해 해결되었음\n\n\n\nResiduals 분석\n위 모형은 충분히 좋은가?\n\ndiamonds2[\"lresid\"] = mod_diamonds.resid\n\n(\n    so.Plot(diamonds2, x='lcarat', y='lresid')\n    .add(so.Dots(alpha=.1))\n)\n\n\n\n\n\n\n\n\n이제, 다이아몬드의 퀄리티와 위에서 구한 가격의 residuals과의 관계를 살펴보면,\n\ny축은 log2 scale로 변환된 것이므로, 원래 단위로 이해하면,\n\nresidual +1은 캐럿으로 예측되는 가격(residual = 0)보다 가격이 2배 비싸다는 것을 의미\nresidual -1은 캐럿으로 예측되는 가격(residual = 0)보다 가격이 1/2배 낮다는 것을 의미\n\n이는 캐럿의 영향을 고려한 후에, 다이아몬드의 퀄리티 각각이 가격에 (상대적으로) 얼마나 영향을 주는지를 가늠할 수 있음\n\n\ncut = rangeplot(diamonds2, x=\"cut\", y=\"lresid\")\ncolor = rangeplot(diamonds2, x=\"color\", y=\"lresid\")\nclarity = rangeplot(diamonds2, x=\"clarity\", y=\"lresid\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod1 = smf.ols(\"lprice ~ lcarat + cut\", data=diamonds2).fit()\nmod2 = smf.ols(\"lprice ~ lcarat + color\", data=diamonds2).fit()\nmod3 = smf.ols(\"lprice ~ lcarat + clarity\", data=diamonds2).fit()\n\n# mod.params\n\n\n\n\n\n\n\nIntercept          11.84\ncut[T.Good]         0.23\ncut[T.Very Good]    0.34\ncut[T.Premium]      0.33\ncut[T.Ideal]        0.45\nlcarat              1.70\ndtype: float64\n\n\nIntercept    12.37\ncolor[T.E]   -0.04\ncolor[T.F]   -0.05\ncolor[T.G]   -0.08\ncolor[T.H]   -0.27\ncolor[T.I]   -0.41\ncolor[T.J]   -0.61\nlcarat        1.73\ndtype: float64\n\n\n\n\nIntercept         11.24\nclarity[T.SI2]     0.66\nclarity[T.SI1]     0.87\nclarity[T.VS2]     1.08\nclarity[T.VS1]     1.15\nclarity[T.VVS2]    1.38\nclarity[T.VVS1]    1.45\nclarity[T.IF]      1.58\nlcarat             1.81\ndtype: float64\n\n\n\n다이아몬드의 3가지 퀄리티가 서로 연관되어 있다면?",
    "crumbs": [
      "Modelling",
      "Model Building I"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1.html#a-more-complicated-model",
    "href": "contents/Modelling/modelling1.html#a-more-complicated-model",
    "title": "Model Building I",
    "section": "A more complicated model",
    "text": "A more complicated model\n\n다이아몬드의 3가지 퀄리티와 carat이 모두 연관되어 있어, 각각의 고유한 효과를 보기 위해 다음과 같이 모든 예측변수들을 포함하는 모형을 세울 수 있음\n\n\nmod_full = smf.ols('lprice ~ lcarat + cut + color + clarity', data=diamonds2).fit()\n\n\nPrediction\n첫번째로 이 모형에 의한, cut에 따른 예측값을 구하기 위해, cut을 제외한 다른 값들을 고정한 grid를 구해보면,\n\n연속변수에 대해서는 보통 평균(mean)이나 중앙값(median)을\n카테고리 변수에 대해서는 보통 최빈값(mode)을 사용\n\n\ngrid = pd.DataFrame({\"cut\": [\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]})\ngrid[\"color\"] = diamonds2.color.mode()[0]  # mode: 최빈값\ngrid[\"clarity\"] = diamonds2.clarity.mode()[0]\ngrid[\"lcarat\"] = diamonds2.lcarat.median()\n\n\ngrid\n\n         cut color clarity  lcarat\n0       Fair     G     SI1   -0.51\n1       Good     G     SI1   -0.51\n2  Very Good     G     SI1   -0.51\n3    Premium     G     SI1   -0.51\n4      Ideal     G     SI1   -0.51\n\n\n즉, G 컬러이고, SI1의 투명도와, 캐럿(로그) -0.51인 다이아몬드에 대해서, cut이 좋아질수록 가격이 얼마나 올라가는지를 예측해 본다면,\n\ngrid[\"lpred\"] = mod_full.predict(grid)\ngrid[\"pred\"] = 2**grid.lpred  # 원래 단위인 price로 되돌리기\ngrid\n\n         cut color clarity  lcarat  lpred    pred\n0       Fair     G     SI1   -0.51  10.99 2035.36\n1       Good     G     SI1   -0.51  11.10 2202.21\n2  Very Good     G     SI1   -0.51  11.16 2285.37\n3    Premium     G     SI1   -0.51  11.19 2337.24\n4      Ideal     G     SI1   -0.51  11.22 2388.52\n\n\n(\n    so.Plot(grid, x='cut', y='pred')\n    .add(so.Line(marker=\"o\"))\n    .limit(y=(1300, 4100))  # 아래 플랏과 비교하기 위해 y축 범위를 고정\n).show()\n\n\n\n\n\n\n\n\n# 모형의 파라미터를 해석해 보면\n2**mod_full.params[1:5]\n\ncut[T.Good]        1.08\ncut[T.Very Good]   1.12\ncut[T.Premium]     1.15\ncut[T.Ideal]       1.17\ndtype: float64\n\n\ncut 대신 color와 clarity에 대해서도 그려볼 것\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals 분석\n\ndiamonds2[\"lresid_full\"] = mod_full.resid\n\n\n(\n    so.Plot(diamonds2, x='lcarat', y='lresid_full')\n    .add(so.Dots(alpha=.1))\n)\n\n\n\n\n\n\n\n\n이상치들만 자세히 들여다보면,\n\nfrom numpy import abs\n\ndiamonds2.query(\"@abs(lresid_full) &gt; 1\").assign(\n    pred_full=lambda x: 2 ** mod_full.predict(x[[\"lcarat\", \"cut\", \"color\", \"clarity\"]]),\n    resid_full=lambda x: x.price - x.pred_full,\n).sort_values(\"resid_full\")\n\n       carat      cut color clarity  depth  table  price    x    y    z  \\\n22440   2.46  Premium     E     SI2  59.70  59.00  10470 8.82 8.76 5.25   \n41918   1.03     Fair     E      I1  78.20  54.00   1262 5.72 5.59 4.42   \n38153   0.25     Fair     F     SI2  54.40  64.00   1013 4.30 4.23 2.32   \n...      ...      ...   ...     ...    ...    ...    ...  ...  ...  ...   \n5325    0.61     Good     F     SI2  62.50  65.00   3807 5.36 5.29 3.33   \n8203    0.51     Fair     F    VVS2  60.70  66.00   4368 5.21 5.11 3.13   \n21935   1.01     Fair     D     SI2  64.60  58.00  10011 6.25 6.20 4.02   \n\n       lprice  lcarat  lresid  lresid_full  pred_full  resid_full  \n22440   13.35    1.30   -1.02        -1.17   23630.26   -13160.26  \n41918   10.30    0.04   -1.96        -1.07    2650.65    -1388.65  \n38153    9.98   -2.00    1.15         1.94     264.51      748.49  \n...       ...     ...     ...          ...        ...         ...  \n5325    11.89   -0.71    0.90         1.31    1539.74     2267.26  \n8203    12.09   -0.97    1.53         1.36    1706.07     2661.93  \n21935   13.29    0.01    1.07         1.30    4052.40     5958.60  \n\n[16 rows x 16 columns]\n\n\n좀 더 체계적으로 다음과 같이 모형의 복잡성이 올라감에 따라 예측의 정확성이 어떻게 변하는지 알아보면\n\ndiamonds2 = diamonds.query(\"carat &lt; 2.5\").assign(\n    lprice=lambda x: np.log2(x.price), \n    lcarat=lambda x: np.log2(x.carat)\n)\n\n# Nested models\ndiamonds2_mod1 = smf.ols(\"lprice ~ lcarat\", data=diamonds2).fit()\ndiamonds2_mod2 = smf.ols(\"lprice ~ lcarat + clarity\", data=diamonds2).fit()\ndiamonds2_mod3 = smf.ols(\"lprice ~ lcarat + cut + color + clarity\", data=diamonds2).fit()\n\n\n\ncode\ndiamonds2_mods = diamonds2.assign(\n    mod1=diamonds2_mod1.resid,\n    mod2=diamonds2_mod2.resid,\n    mod3=diamonds2_mod3.resid,\n)\n\ndiamonds2_mods = diamonds2_mods.melt(\n    id_vars=[\"lcarat\", \"lprice\"],\n    value_vars=[\"mod1\", \"mod2\", \"mod3\"],\n    var_name=\"model\",\n    value_name=\"resid\",\n)\n\n(\n    so.Plot(diamonds2_mods, x='lcarat', y='resid', color='model')\n    .add(so.Dots(alpha=.1))\n    .facet(\"model\")\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\n\n\ncode\n(\n    so.Plot(diamonds2_mods, x='resid', color='model')\n    .add(so.Line(), so.Hist(bins=50))\n    .layout(size=(5, 3))\n)\n\n\n\n\n\n\n\n\n\n\n\ncode\nfrom statsmodels.tools.eval_measures import rmse, meanabs\n\nmods = [diamonds2_mod1, diamonds2_mod2, diamonds2_mod3]\ny = diamonds2.price\nprint(\"The prediction accuracy of the models (original unit except R-squared):\\n\")\n\nfor i, mod in enumerate(mods):\n    y_hat = 2**mod.fittedvalues\n    R2 = np.var(y_hat) / np.var(y)\n    R2_log = mod.rsquared\n\n    print(\n        f\"Model {i+1} &gt;&gt; R-squared(log): {R2_log:.2f}, R_squared {R2:.2f}, \"\n        f\"RMSE: {rmse(y, y_hat):.2f}, MAE:{meanabs(y, y_hat):.2f}\"\n    )\n\n\nThe prediction accuracy of the models (original unit except R-squared):\n\nModel 1 &gt;&gt; R-squared(log): 0.93, R_squared 0.88, RMSE: 1507.04, MAE:817.34\nModel 2 &gt;&gt; R-squared(log): 0.97, R_squared 0.98, RMSE: 1164.81, MAE:623.65\nModel 3 &gt;&gt; R-squared(log): 0.98, R_squared 0.95, RMSE: 732.70, MAE:390.61\n\n\n\n\n\n\n\n\nModel evaluation: R-squared, RMSE, MAE\n\n\n\nThe prediction accuracy of the models can be evaluated by the following metrics:\n(The strength of association)\n변량의 비율로 해석하고 싶다면,\n\\(\\displaystyle\\frac{V(predictions)}{V(Y)} + \\frac{V(residuals)}{V(Y)} = 1,\\)   (OLS estimate)\n즉, “모형에 의해 설명되는 \\(Y\\) 변량의 비율” + “모형에 의해 설명되지 않는 \\(Y\\) 변량의 비율” = 1\n첫 항을 \\(R^2\\) 라고 하고, 결정계수 혹은 R squared라고 부름\n따라서, \\(1-R^2\\) 는 설명되지 않는 변량의 비율이라고 할 수 있음.\n\\(또한, R\\)은 multiple correlation coefficient라고 부르는데, 이는 \\(Y\\)와 예측값 \\(\\hat Y\\)의 상관계수(Pearson’s correlation coefficient)를 의미함.\n비율이 아닌 \\(Y\\)의 단위와 동일한 단위로 해석하고 싶다면,\n\nRoot-mean-squared deviation/error: \\(RMSE = \\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(y_i -\\hat y_i)^2}}\\)\nMean absolute error: \\(MAE = \\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}{|~y_i -\\hat y_i~|}\\) : 이상치에 덜 민감함\n\n동일한 관계를 갖지만 예측정확성/설명력이 다른 경우의 예",
    "crumbs": [
      "Modelling",
      "Model Building I"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1.html#interactions",
    "href": "contents/Modelling/modelling1.html#interactions",
    "title": "Model Building I",
    "section": "Interactions",
    "text": "Interactions\n무게(carat)과 투명도(clarity)가 상호작용하여 가격에 영향을 준다는 가정하에, 즉, 투명도의 레벨에 따라 무게와 가격의 관계가 바뀔 수 있다는 가정\n\ndiamonds2_mod2 = smf.ols(\"lprice ~ lcarat + clarity\", data=diamonds2).fit()\ndiamonds2_mod2_interact = smf.ols(\"lprice ~ lcarat * clarity\", data=diamonds2).fit()\n\nPrediction 비교\n\n\ncode\ndiamonds2_mods = diamonds2.assign(\n    pred_add=diamonds2_mod2.fittedvalues,\n    pred_interact=diamonds2_mod2_interact.fittedvalues,\n)\n\ndiamonds2_mods = diamonds2_mods.melt(\n    id_vars=[\"lcarat\", \"lprice\", \"clarity\"],\n    value_vars=[\"pred_add\", \"pred_interact\"],\n    var_name=\"model\",\n    value_name=\"pred\",\n)\n\n(\n    so.Plot(diamonds2_mods, x='lcarat', y='pred', color='clarity')\n    .add(so.Line())\n    .scale(color=so.Nominal(order=diamonds2.clarity.cat.categories.tolist()))\n    .facet(\"model\")\n    .layout(size=(8, 4.5))\n)\n\n\n\n\n\n\n\n\n\nResiduals 비교\n\n\ncode\ndiamonds2_mods = diamonds2.assign(\n    resid_add=diamonds2_mod2.resid,\n    resid_interact=diamonds2_mod2_interact.resid,\n)\n\ndiamonds2_mods = diamonds2_mods.melt(\n    id_vars=[\"lcarat\", \"lprice\", \"clarity\"],\n    value_vars=[\"resid_add\", \"resid_interact\"],\n    var_name=\"model\",\n    value_name=\"resid\",\n)\n\n(\n    so.Plot(diamonds2_mods, x=\"lcarat\", y=\"resid\", color=\"clarity\")\n    .add(so.Dots(alpha=0.1))\n    .layout(size=(12, 5))\n    .facet(\"clarity\", \"model\")\n)",
    "crumbs": [
      "Modelling",
      "Model Building I"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1.html#모델-파라미터의-해석",
    "href": "contents/Modelling/modelling1.html#모델-파라미터의-해석",
    "title": "Model Building I",
    "section": "모델 파라미터의 해석",
    "text": "모델 파라미터의 해석\n변수와 변수간의 관계성에 초점\n변수들을 “동시”에 고려해서 봄으로써 각 변수들의 “고유한 impact”의 방향과 크기를 해석하고자 함\nex. ols(lprice ~ lcarat + color + cut + clarity, data = diamonds2)\n다이아몬드 투명도(clarity)의 레벨이 하나씩 올라감에 따라 가격형성에 어떻게 혹은 얼마나 영향을 주는가?\n\n표현에 주의할 것: 인과관계가 있는 듯한 표현… lprice ~ clarity의 관계는?\n변수들을 개별적으로 보았을 때의 impact는 다른 변수들을 함께 고려하면 바뀌는 것이 일반적임 (서로 독립이 아니라면)\n\nex. 다이아몬드 투명도가 가격과 맺는 관계는 다른 변수를 고려하면 바뀜\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclarity[T.SI2]    0.59\nclarity[T.SI1]    0.82\nclarity[T.VS2]    1.04\n                  ... \nclarity[T.VVS1]   1.44\nclarity[T.IF]     1.58\nlcarat            1.89\nLength: 8, dtype: float64\n\n\n\n또한, 변수간의 상호작용을 고려하여, 각 변수들의 “고유한 impact”의 방향과 크기에 대해 정교한 분석이 가능\n\n\n\n\n\n\n\n\n\n\n\nlcarat                   1.60\nlcarat:clarity[T.SI2]    0.20\nlcarat:clarity[T.SI1]    0.22\nlcarat:clarity[T.VS2]    0.18\nlcarat:clarity[T.VS1]    0.23\nlcarat:clarity[T.VVS2]   0.25\nlcarat:clarity[T.VVS1]   0.23\nlcarat:clarity[T.IF]     0.27\ndtype: float64",
    "crumbs": [
      "Modelling",
      "Model Building I"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1.html#모델의-예측-정확성과-특성",
    "href": "contents/Modelling/modelling1.html#모델의-예측-정확성과-특성",
    "title": "Model Building I",
    "section": "모델의 예측 정확성과 특성",
    "text": "모델의 예측 정확성과 특성\nResiduals의 분석\n변수의 개수가 증가하면, 즉 모델이 복잡할수록 샘플에 대한 예측력은 높아짐.\n하지만, 미래 데이터(모집단, 일반화)에 대한 예측력은 떨어질 수 있음.\n한편, 변수들 간의 복잡한 correlation들이 모델의 해석에 오류를 가져올 수 있음.\n\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff\n\nSource: p.31, An Introduction to Statistical Learning (2e) by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani",
    "crumbs": [
      "Modelling",
      "Model Building I"
    ]
  },
  {
    "objectID": "contents/Introduction/vis.html",
    "href": "contents/Introduction/vis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Python의 시각화 라이브러리는 다양하게 개발되어지고 있으며, 각기 특성이 달라 하나로만 쓰기 어려운 상황임\n한편, R은 ggplot2라는 매우 강력한 시각화 라이브러리가 존재하고, 이는 grammar of graphics를 충실히 따라 설계되었고, 매우 flexible함\n\n\nMatplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly: 다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Introduction",
      "Data visualization"
    ]
  },
  {
    "objectID": "contents/Introduction/vis.html#대표적-도구들",
    "href": "contents/Introduction/vis.html#대표적-도구들",
    "title": "Data Visualization",
    "section": "",
    "text": "Matplotlib\n\n가장 오래된 Python과 잘 통합된 널리 사용되는 라이브러리\n거의 가능한 모든 플랏을 그릴 수 있음\n한편, 디테일한 부분을 모두 specify해야 함으로써 많은 코딩이 요구되며, interactive 또는 web graphs에 취약함\n\npandas\n\nMatplotlib로 구현된 DataFrame의 method로 간략하게 시각화가 가능하며, 빠르게 데이터를 들여다볼 수 있음\n\nSeaborn & the seaborn.objects interface\n\nMatplotlib 위에 개발된 간결한 문법의 high-level 언어\nDecalative: 변수들이 어떤 시각화 속성과 위치를 지니는지만 specify\n“Grammer of graphics”라는 시각화 문법에 충실하고자 the seaborn.objects로 새롭게 변화 중\n\n\n\n\nAltair\n\n“Grammer of graphics”를 충실히 따라 설계됨\n각 plot이 이미지가 아닌 data + specification으로 이루어짐: 이미지가 저장되지 않고, 브라우저에서 이미지로 complie되어 생성됨\nWeb-based interactive 시각화인 D3에 그 모체를 두며, Vega/Vega-Lite로부터 파생됨\njavascript-based로 interactive 시각화에 용이하나 Python과 연계가 부족한 부분이 있고, 지원/커뮤니티가 미흡함\n\nBokeh\nPlotly: 다양한 언어(R, Python, Julia)을 지원하며, 기업 수준의 상용화 제품들도 있으며, 지원군 많음\nJake VanderPlas의 2017년 발표 자료 중: The Python Visualization Landscape\n\nSource: Jake VanderPlas - The Python Visualization Landscape PyCon 2017",
    "crumbs": [
      "Introduction",
      "Data visualization"
    ]
  },
  {
    "objectID": "contents/Introduction/vis.html#the-grammer-of-graphics",
    "href": "contents/Introduction/vis.html#the-grammer-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammer of Graphics",
    "text": "The Grammer of Graphics\nA coherent system for describing and building graphs\nAesthetics and types of data:\n\n데이터의 값을 특정 aesthetics에 mapping\n\n\nSource: Fundamentals of Data Visualization by Claus O. Wilke\n\n데이터의 타입은 다음과 같이 나누어짐\n\ncontinuous / discrete\nquatitative / qualitative\ncategorical unordered (nominal) / categorical ordered (ordinal)\n\n성별, 지역 / 등급, 랭킹\nordinal: 등간격을 가정\n\n퀄리티 good, fair, poor는 등간격이라고 봐야하는가?\n랭킹은?\n선호도 1, 2, …, 8; continuous?\n임금 구간?\n\n\n\n데이터 타입에 따라 좀 더 적절한 aesthetic mapping이 있으며,\n같은 정보를 품고 있는 시각화라도 더 적절한 representation이 존재\nBertin’s Semiology of Graphics (1967)\nLevels of organization\n\nSource: Jake VanderPlas’ presentation at PyCon 2018\n\nCase 1\n예를 들어, 다음과 같이 1) 지역별로 2) 날짜에 따른 3) 온도의 변화를 나타낸다면,\n즉, x축의 위치에 날짜 정보를, y축의 위치에 온도 정보를, 색깔에 지역 정보를 할당했음.\n\n한편, 아래는 x축의 위치에 압축된 날짜 정보를, y축의 위치에 지역 정보를, 색깔에 압축된 온도 정보를 할당했음.\n\n\n\nCase 2\n다음은 GDP, mortality, population, region의 네 정보를 다른 방식으로 mapping한 결과임.",
    "crumbs": [
      "Introduction",
      "Data visualization"
    ]
  },
  {
    "objectID": "contents/Introduction/vis.html#탐색적-exploratory-vs.-정보전달-communicative",
    "href": "contents/Introduction/vis.html#탐색적-exploratory-vs.-정보전달-communicative",
    "title": "Data Visualization",
    "section": "탐색적 (Exploratory) vs. 정보전달 (Communicative)",
    "text": "탐색적 (Exploratory) vs. 정보전달 (Communicative)\n\n분석도구: 현미경, 연장도구\n강점이자 약점",
    "crumbs": [
      "Introduction",
      "Data visualization"
    ]
  },
  {
    "objectID": "contents/Introduction/vis.html#interative-plots",
    "href": "contents/Introduction/vis.html#interative-plots",
    "title": "Data Visualization",
    "section": "Interative Plots",
    "text": "Interative Plots\nAltair\n\n\n\n\n\n\n\nPlotly",
    "crumbs": [
      "Introduction",
      "Data visualization"
    ]
  },
  {
    "objectID": "contents/Introduction/inspection.html",
    "href": "contents/Introduction/inspection.html",
    "title": "Inspecting data",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Introduction",
      "NumPy and pandas",
      "Inspecting data"
    ]
  },
  {
    "objectID": "contents/Introduction/inspection.html#useful-method",
    "href": "contents/Introduction/inspection.html#useful-method",
    "title": "Inspecting data",
    "section": "Useful method",
    "text": "Useful method\n.head(), .tail(), .sample()\n.info(), .describe(),\n.value_counts(),\n.sort_values(), .nlargest(), .nsmallest()\nData: Tips\n일정기간 한 웨이터가 얻은 팁에 대한 데이터\n\n# load a dataset\ntips = sns.load_dataset(\"tips\")\ntips\n\n     total_bill  tip     sex smoker   day    time  size\n0         16.99 1.01  Female     No   Sun  Dinner     2\n1         10.34 1.66    Male     No   Sun  Dinner     3\n2         21.01 3.50    Male     No   Sun  Dinner     3\n3         23.68 3.31    Male     No   Sun  Dinner     2\n..          ...  ...     ...    ...   ...     ...   ...\n240       27.18 2.00  Female    Yes   Sat  Dinner     2\n241       22.67 2.00    Male    Yes   Sat  Dinner     2\n242       17.82 1.75    Male     No   Sat  Dinner     2\n243       18.78 3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\n# DataFrame의 값들: ndarray\ntips.values # or tips.to_numpy()\n\narray([[16.99, 1.01, 'Female', ..., 'Sun', 'Dinner', 2],\n       [10.34, 1.66, 'Male', ..., 'Sun', 'Dinner', 3],\n       [21.01, 3.5, 'Male', ..., 'Sun', 'Dinner', 3],\n       ...,\n       [22.67, 2.0, 'Male', ..., 'Sat', 'Dinner', 2],\n       [17.82, 1.75, 'Male', ..., 'Sat', 'Dinner', 2],\n       [18.78, 3.0, 'Female', ..., 'Thur', 'Dinner', 2]], dtype=object)\n\n\n\ntips.head() # 처음 N개 나열\n\n   total_bill  tip     sex smoker  day    time  size\n0       16.99 1.01  Female     No  Sun  Dinner     2\n1       10.34 1.66    Male     No  Sun  Dinner     3\n2       21.01 3.50    Male     No  Sun  Dinner     3\n3       23.68 3.31    Male     No  Sun  Dinner     2\n4       24.59 3.61  Female     No  Sun  Dinner     4\n\n\n\ntips.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\ntips.describe() # numerical type만 나열\n\n\n\n       total_bill    tip   size\ncount      244.00 244.00 244.00\nmean        19.79   3.00   2.57\nstd          8.90   1.38   0.95\nmin          3.07   1.00   1.00\n25%         13.35   2.00   2.00\n50%         17.80   2.90   2.00\n75%         24.13   3.56   3.00\nmax         50.81  10.00   6.00\n\n\n\n\ntips.describe(include=\"all\") # all types 나열\n\n        total_bill    tip   sex smoker  day    time   size\ncount       244.00 244.00   244    244  244     244 244.00\nunique         NaN    NaN     2      2    4       2    NaN\ntop            NaN    NaN  Male     No  Sat  Dinner    NaN\nfreq           NaN    NaN   157    151   87     176    NaN\n...            ...    ...   ...    ...  ...     ...    ...\n25%          13.35   2.00   NaN    NaN  NaN     NaN   2.00\n50%          17.80   2.90   NaN    NaN  NaN     NaN   2.00\n75%          24.13   3.56   NaN    NaN  NaN     NaN   3.00\nmax          50.81  10.00   NaN    NaN  NaN     NaN   6.00\n\n[11 rows x 7 columns]\n\n\ntips.describe(include=\"category\")\n\n\n\n         sex smoker  day    time\ncount    244    244  244     244\nunique     2      2    4       2\ntop     Male     No  Sat  Dinner\nfreq     157    151   87     176\n\n\n\n\ns1 = tips.value_counts(\"day\") # \"day\" 칼럼에 대한 각 카테고리별 counts\ns2 = tips.value_counts(\"day\", sort=False) # default: sort is true\ns3 = tips.value_counts(\"day\", ascending=True) # default: ascending is False\ns4 = tips.value_counts(\"day\", normalize=True) # 카테고리별 비율\ns5 = tips.value_counts([\"sex\", \"smoker\"]) # \"sex\", \"smoker\" 칼럼에 대한 유니크한 카테고리별 counts\n\n\n\n\n\n\n\n\n\nday\nSat     87\nSun     76\nThur    62\nFri     19\nName: count, dtype: int64\n\n\n(a) s1\n\n\n\n\n\n\n\n\nday\nThur    62\nFri     19\nSat     87\nSun     76\nName: count, dtype: int64\n\n\n(b) s2\n\n\n\n\n\n\n\n\n\n\nday\nFri     19\nThur    62\nSun     76\nSat     87\nName: count, dtype: int64\n\n\n(c) s3\n\n\n\n\n\n\n\n\nday\nSat    0.36\nSun    0.31\nThur   0.25\nFri    0.08\nName: proportion, dtype: float64\n\n\n(d) s4\n\n\n\n\n\n\n\n\n\n\nsex     smoker\nMale    No        97\n        Yes       60\nFemale  No        54\n        Yes       33\nName: count, dtype: int64\n\n\n(e) s5\n\n\n\n\n\n\n\nFigure 1: value_count()의 arguments\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.value_count()의 결과는 Series이며 그 이름은 ‘count’ 또는 ’proportion’임 (pandas 2.0)\nMissing(NA)을 count하지 않으나 dropna=False을 이용해 나타낼 수 있음\ntips.value_counts(\"day\", dropna=False)\nSeries에 대해서도 적용되며, DataFrame으로 컬럼을 선택해 적용할 수 있음\ntips[\"day\"].value_counts()  # tips[\"day\"]: Series object\ntips[[\"sex\", \"smoker\"]].value_counts()\n\n\n\nData: palmerpenguins\n\n# load a dataset\npenguins = sns.load_dataset(\"penguins\")\npenguins\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex  \n0         3750.0    Male  \n1         3800.0  Female  \n2         3250.0  Female  \n3            NaN     NaN  \n4         3450.0  Female  \n..           ...     ...  \n339          NaN     NaN  \n340       4850.0  Female  \n341       5750.0    Male  \n342       5200.0  Female  \n343       5400.0    Male  \n\n[344 rows x 7 columns]\n\n\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\npenguins.describe(include=\"object\")\n\n\n\n       species  island   sex\ncount      344     344   333\nunique       3       3     2\ntop     Adelie  Biscoe  Male\nfreq       152     168   168\n\n\n\n\npenguins.value_counts([\"island\", \"species\"])\n\nisland     species  \nBiscoe     Gentoo       124\nDream      Chinstrap     68\n           Adelie        56\nTorgersen  Adelie        52\nBiscoe     Adelie        44\nName: count, dtype: int64\n\n\n\npenguins.value_counts([\"sex\", \"species\"], dropna=False) # NA은 기본적으로 생략\n\nsex     species  \nFemale  Adelie       73\nMale    Adelie       73\n        Gentoo       61\nFemale  Gentoo       58\n        Chinstrap    34\nMale    Chinstrap    34\nNaN     Adelie        6\n        Gentoo        5\nName: count, dtype: int64\n\n\n\n# NA의 개수\npenguins.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\n\n# NA의 비율\npenguins.isna().mean()\n\nspecies              0.000000\nisland               0.000000\nbill_length_mm       0.005814\nbill_depth_mm        0.005814\nflipper_length_mm    0.005814\nbody_mass_g          0.005814\nsex                  0.031977\ndtype: float64\n\n\n\ntips.sort_values(\"tip\", ascending=False)\n\n     total_bill   tip     sex smoker  day    time  size\n170       50.81 10.00    Male    Yes  Sat  Dinner     3\n212       48.33  9.00    Male     No  Sat  Dinner     4\n23        39.42  7.58    Male     No  Sat  Dinner     4\n59        48.27  6.73    Male     No  Sat  Dinner     4\n..          ...   ...     ...    ...  ...     ...   ...\n236       12.60  1.00    Male    Yes  Sat  Dinner     2\n111        7.25  1.00  Female     No  Sat  Dinner     1\n67         3.07  1.00  Female    Yes  Sat  Dinner     1\n92         5.75  1.00  Female    Yes  Fri  Dinner     2\n\n[244 rows x 7 columns]\n\n\n\ntips.sort_values([\"size\", \"tip\"], ascending=[False, True])\n\n     total_bill  tip     sex smoker   day    time  size\n125       29.80 4.20  Female     No  Thur   Lunch     6\n143       27.05 5.00  Female     No  Thur   Lunch     6\n156       48.17 5.00    Male     No   Sun  Dinner     6\n141       34.30 6.70    Male     No  Thur   Lunch     6\n..          ...  ...     ...    ...   ...     ...   ...\n67         3.07 1.00  Female    Yes   Sat  Dinner     1\n111        7.25 1.00  Female     No   Sat  Dinner     1\n82        10.07 1.83  Female     No  Thur   Lunch     1\n222        8.58 1.92    Male    Yes   Fri   Lunch     1\n\n[244 rows x 7 columns]\n\n\n\ntips.nlargest(3, \"tip\")  # 다수의 동등 순위가 있을 때 처리: keep=\"first\", \"last\", \"all\"\n\n     total_bill   tip   sex smoker  day    time  size\n170       50.81 10.00  Male    Yes  Sat  Dinner     3\n212       48.33  9.00  Male     No  Sat  Dinner     4\n23        39.42  7.58  Male     No  Sat  Dinner     4",
    "crumbs": [
      "Introduction",
      "NumPy and pandas",
      "Inspecting data"
    ]
  },
  {
    "objectID": "contents/Exercises/movielens.html",
    "href": "contents/Exercises/movielens.html",
    "title": "Exercises",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Exploratory Analysis II",
      "MovieLens"
    ]
  },
  {
    "objectID": "contents/Exercises/movielens.html#movielens-1m-dataset",
    "href": "contents/Exercises/movielens.html#movielens-1m-dataset",
    "title": "Exercises",
    "section": "MovieLens 1M Dataset",
    "text": "MovieLens 1M Dataset\nSource: MovieLens 1M movie ratings\nMcKinney’s: 13. Data Analysis Examples\n1990년대 후반에서 2000년대 초반의 영화 평가에 대한 3개의 relational data로 이루어져 있고,\nuser_id, movie_id의 keys로 연결되어 있습니다.\n\nusers: 유저에 대한 정보\nratings: 평점에 대한 정보\nmovies: 영화에 대한 정보\n\n\n아래 데이터 링크가 원활하지 않을 시, 위의 MovieLens 1M movie ratings 사이트에서 직접 다운받으세요.\nml-1m.zip 압축파일을 풀어 users.dat, ratings.dat, movies.dat 세 파일을 아래 방식으로 읽어옵니다.\n\nunames = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\nusers = pd.read_table(\n    \"https://raw.githubusercontent.com/wesm/pydata-book/3rd-edition/datasets/movielens/users.dat\",\n    sep=\"::\",\n    header=None,\n    names=unames,\n    engine=\"python\",\n)\nusers\n\n      user_id gender  age  occupation    zip\n0           1      F    1          10  48067\n1           2      M   56          16  70072\n2           3      M   25          15  55117\n...       ...    ...  ...         ...    ...\n6037     6038      F   56           1  14706\n6038     6039      F   45           0  01060\n6039     6040      M   25           6  11106\n\n[6040 rows x 5 columns]\n\n\n\nrnames = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\nratings = pd.read_table(\n    \"https://raw.githubusercontent.com/wesm/pydata-book/3rd-edition/datasets/movielens/ratings.dat\",\n    sep=\"::\",\n    header=None,\n    names=rnames,\n    engine=\"python\",\n)\nratings\n\n         user_id  movie_id  rating  timestamp\n0              1      1193       5  978300760\n1              1       661       3  978302109\n2              1       914       3  978301968\n...          ...       ...     ...        ...\n1000206     6040       562       5  956704746\n1000207     6040      1096       4  956715648\n1000208     6040      1097       4  956715569\n\n[1000209 rows x 4 columns]\n\n\n\nmnames = [\"movie_id\", \"title\", \"genres\"]\nmovies = pd.read_table(\n    \"https://raw.githubusercontent.com/wesm/pydata-book/3rd-edition/datasets/movielens/movies.dat\",\n    sep=\"::\",\n    header=None,\n    names=mnames,\n    engine=\"python\",\n)\nmovies.head(6)\n\n   movie_id                               title                        genres\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n2         3             Grumpier Old Men (1995)                Comedy|Romance\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\n4         5  Father of the Bride Part II (1995)                        Comedy\n5         6                         Heat (1995)         Action|Crime|Thriller\n\n\n\n\nratings과 users 데이터를 merge한 후 다음과 같은 user_rating 데이터셋을 만드세요.\n\n#          user_id gender  age  occupation    zip  movie_id  rating  timestamp\n# 0              1      F    1          10  48067      1193       5  978300760\n# 1              1      F    1          10  48067       661       3  978302109\n# 2              1      F    1          10  48067       914       3  978301968\n# ...          ...    ...  ...         ...    ...       ...     ...        ...\n# 1000206     6040      M   25           6  11106       562       5  956704746\n# 1000207     6040      M   25           6  11106      1096       4  956715648\n# 1000208     6040      M   25           6  11106      1097       4  956715569\n\n다음과 같이 영화(movie_id)별로 남녀(gender)에 따른 rating의 평균과 그 개수(count)을 구해보세요.\n\n#    movie_id gender  mean  count\n# 0         1      F  4.19    591\n# 1         1      M  4.13   1486\n# 2         2      F  3.28    176\n# 3         2      M  3.18    525\n# 4         3      F  3.07    136\n# 5         3      M  2.99    342\n...\n\n다음과 같은 플랏을 그려보고 평가의 수가 적을수록 그 편차가 커지는 현상을 확인해보세요.\n\n혹시, 평가의 수가 많은 영화일수록 평가가 높아지는 현상에 대해 설명할 수 있을까요?\n또한, 남녀의 평가에 차이가 벌어지는 현상을 설명할 수 있을까요?\n\n\n\n\n\n\n\n\n\n\n\n\n3번에서 플랏에 .limit(x=(0, 500), y=(2.5, 4.5))을 추가하여 평가 개수(count)가 0에서 500사이이고, 평균 rating이 2.5에서 4.5 사이인 것으로 확대해서 보고, 평가 개수가 몇 개 정도부터 남녀의 평가의 차이가 대략 일정하게 되는지 살펴보세요.\n\n\n\n\n\n\n\n\n\n\n\n영화별로 남녀의 평가가 크게 갈리는 영화들을 찾기 위해\n\n5.1 2번에서 구한 데이터에서 남녀 모두 rating이 300개 이상 있는 영화로만 간추려보세요.\n\n5.2 이 데이터를 popular_movies라고 명명하고,\n\n이 데이터를 gender에 관해 wide format으로 변환한 후; pivot()을 이용\n\n여자의 평균 rating에서 남자의 평균 rating을 뺀 그 차이를 데이터에 추가한 후; assign()을 이용\n그 차이로 sort한 후,\n\n5.3 여자의 선호가 더 높은 영화 5편과 남자의 선호가 더 높은 영화 5편 (선호 차이의 크기 순서로)을 구해보세요.\n\n이를 위해서 movies 테이블 안의 영화제목(title)을 merge()를 이용해 추가하세요.\n\n\n3번 플랏에서 유추되듯이 평가의 개수가 영화의 완성도 혹은 인기도를 파악할 수 있는 대략적인 지표가 될 수 있습니다. 즉, 평가수가 많을 수록 평점도 높습니다. 따라서 평가 개수를 바탕으로 인기도(popularity)를 수치화 하려고 합니다.\n\n우선, 3번 플랏에서 평가수가 같다면 여성이 더 높은 평점을 주는 것으로 보이는데, 이 현상을 다음과 같이 자세히 들여다 봅니다.\n\n다음과 같은 count를 20개의 구간으로 discretize해주는 함수를 성별로 grouping된 user_rating 데이터에 apply() 하세요.\ndef popular(g):\n    g[\"popularity\"] = pd.qcut(g[\"count\"], q=20, labels=False)\n    return g\n\n이 함수의 의미를 파악하고, 20단계 (0, 1, 2, …, 19)의 popularity가 성별을 고려하여 각 영화에 부여되었음을 이해합니다. (각각은 평가 개수 5%에 해당)\n이제, 다음과 같이 popularity에 따라 평점이 높아지는 현상을 성별을 고려한 후 본 결과를 아래와 같이 플랏을 통해 확인해봅니다.\n남성 유저와 여성 유저의 비율에 큰 차이 (4331:1709)가 있는 것을 고려했을 때, 어떻게 이 현상을 설명할 수 있을까요?\n\n\n\n\n\n\n\n\n\n\n\n\n남녀별로 평점의 편차가 큰, 즉 의견이 분분한 영화들을 구해봅니다.\n\n5번에서 구한 popular_movies에 한해 남녀 각각에 대해 영화별로 평점의 편차를 표준편차로 구해보고,\n\n5번을 못 구한 경우, 다음 파일을 다운로드 받아 popular_movies로 사용합니다.\n\n남녀별로 편차 상위 2개만 표시합니다. (동등한 순위 모두 포함)\n\n다음 method를 이용하는 함수를 정의하고 apply()로 적용해 봅니다.\ndf.nlargest(n, \"variable name\", keep=\"all\")\n\n영화제목을 movies 데이터와 merge하여 표시합니다.\n\n이제 초점을 유저들에게 돌려, 유저들의 특성을 고려해봅니다. 일반적으로 같은 소스(사람)에서 온 데이터는 비슷한 성향을 띄는데 이를 depenency의 문제라고 합니다. 한 가족 구성원으로부터 왔다든가, 같은 학교의 학생들과 같이 구체적으로 명시하기 어렵지만 데이터 상에서 비슷한 군집을 이룹니다. 이 데이터의 경우 동일한 유저들의 특성이 존재할 수 있는데, 예를 들어 후한 점수를 준다든가, 같은 유저라도 어떤 장르의 영화는 매우 낮은 평점을 준다든가 하는 현상이 있을 수 있는데 이를 알아봅니다.\n\n8.1 우선 개인별(성별로 나눠)로 몇 개정도나 평점을 주었는지 분포를 살펴봅니다.\n8.2 개인별로 평점의 평균(mean)과 표준편차(std), 개수(count)를 구합니다; 성별 차는 없는 것으로 간주함\n8.3 이 세 변수의 관계를 보기 위해, 평점 개수(count)를 10개의 구간으로 pd.cut을 이용해 discretize해서 살펴봅니다.\n\n8.1에서 분포를 살펴보았으면, 개수를 먼저 log 스케일로 변환해서 구간으로 쪼개는 것이 유리함을 알 것입니다. (np.log 이용)\n즉, 대다수는 2백개 이하의 평점을 남긴 반면, 소수의 유저들 중에는 수천개의 평점을 남긴 사람도 있습니다.\n10개 구간으로 discretized된 (log) count를 .facet을 이용해 아래와 같이 유저들 각자의 평점 평균과 표준편차의 관계를 살펴봅니다; (아래 플랏은 데이터를 적절히 필터링 한 결과입니다.)\n평점을 얼마나 많이 남겼는지와 관계없이 비슷한 현상이 나타나는데 이 현상을 설명할 수 있을까요?\n평균적으로 낮은 평점을 준 소위 짠 유저들이 더 비판적이고 고민끝에 평점을 준 것이라고 추측할 수 있을까요?\n\n\n\n\n\n\n\n\n\n\n\n\n\n장르별로 남녀의 선호 차이를 두 관점 1) 평점의 개수/비율, 2) 평점의 평균의 관점에서 따로 살펴봅니다.\n\nmovies 데이터에서 장르(genres)는 여러 장르에 포함되는 것을 | 기호로 나누어 표기되어 있고, 분석을 위해서 다음 코드를 이용해 long format으로 각 장르가 열에 한번씩만 나오도록 다음과 같이 바꾸세요.\nmovies_long = movies.copy()\n\nmovies_long[\"genres\"] = movies_long[\"genres\"].str.split(\"|\")\nmovies_long = movies_long.explode(\"genres\")\n\nmovies_long.head(8)\n#    movie_id                    title      genres\n# 0         1         Toy Story (1995)   Animation\n# 0         1         Toy Story (1995)  Children's\n# 0         1         Toy Story (1995)      Comedy\n# 1         2           Jumanji (1995)   Adventure\n# 1         2           Jumanji (1995)  Children's\n# 1         2           Jumanji (1995)     Fantasy\n# 2         3  Grumpier Old Men (1995)      Comedy\n# 2         3  Grumpier Old Men (1995)     Romance\n\n\n이 movie_long과 1번에서 만든 user_rating을 merge한 후 다음 문제를 이어가세요.\n9.1 장르별로 평점의 개수를 간단히 플랏으로 살펴보는데 남녀 별로 따로 비율이 나오도록 해보세요.\n\nso.Hist(\"proportion\", common_norm=False)이 필요할 수 있음\n\n\n\n\n\n\n\n\n\n\n9.2 이번에는 장르별로 평점의 평균를 남녀별로 간단히 플랏으로 확인해보세요.\n\nso.Agg()를 활용하고,\n.limit(x=(3, 4.2))를 써서 확대해서 살펴보세요.\n\n\n\n\n\n\n\n\n\n\n9.3 위의 플랏에서 살펴본 평균 평점의 값을 직접 구해봅니다. 장르별, 남녀로 그룹핑을 하여 평균 평점을 다음과 같이 나오도록 구해보세요.\n\nunstack()을 활용해 보세요.\n\n# gender       genre    F    M\n# 0           Action  ...  ...\n# 1        Adventure  ...  ...\n# ..             ...  ...  ...\n# 16             War  ...  ...\n# 17         Western  ...  ...\n9.4 영화별 남녀의 평균 평점의 차이(Female - Male) 순으로 정렬된 플랏을 대략적으로 다음과 같이 그려봅니다.\n\nCustomizing할 때, .limit(x=(3, 4.2))과 .scale(y=so.Nominal(order=[]))이 필요할 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n9.5 평점의 장르별 분포를 boxplot으로 볼 수도 있지만, 대신 seaborn.object의 Range()를 써서 평균과 표준편차를 다음과 같이 그려보세요.\n\nrange는 .add(so.Range(), so.Est(\"mean\", errorbar=\"sd\"))를 활용하고\n평균은 Agg()을 이용해 표시합니다.\n\n\n\n\n\n\n\n\n\n\n\n장르별로 나이대에 따른 영화시청에 차이가 있는지 살펴봅니다. 나이는 다음과 같이 코딩되어 있습니다.\n\n\n1: “Under 18”\n18: “18-24”\n25: “25-34”\n35: “35-44”\n45: “45-49”\n50: “50-55”\n56: “56+”\n\n9번에서 데이터를 만들지 못한 경우 경우, 다음 .parquet 파일을 받아 이용해 다음 문제를 이어가세요.\npd.read_parquet(\"data/movies_long_all.parquet\")\n10.1 우선, 위에서 western 장르가 남녀의 평점 차이가 가장 크게 나타나 남성이 선호하는 것으로 보이는데 western 장르만 따로 떼어 나이대/남녀 별로 rating에 차이가 있는지 살펴봅니다.\n\n우선 scatterplot으로 살펴보는 것이 유용한지 체크해보세요.\n개선할 방법이 있을까요?\nFitted line만을 so.PolyFit(5)을 이용하여 다음과 같이 그려보세요.\n\nx축 눈금은 .scale(x=so.Continuous().tick(at=[1, 18, 25, 35, 45, 50, 56]))\n\n어떤 점을 파악할 수 있나요?\n\n\n\n\n\n\n\n\n\n\n10.2 western 장르의 남녀 차이는 18:18-24세 사이에 가장 커보이고 그 갭은 다른 연령대에서는 줄어드는 것으로 보입니다.\n\n하지만, 만약 18-24세 대에서 남성이 여성에 비해 과하게 많은 평점을 내렸다면 적절한 결론이 아닐 수 있습니다. 즉, 18-24세 대에서 평점의 개수 비율이 남녀가 동일해야 좀 더 확신을 가질 수 있습니다.\n이를 확인하기 위해 다음과 같은 테이블을 구해보세요.\n예를 들어, 18:18-24세 남성의 14%와 여성의 14%가 평점을 남겼으며, 45:45-49세 남성의 15%와 여성의 9%가 평점을 남겼습니다.\n\n\n\n    age gender  mean  size  total  ratio\n0     1      F  3.60    91   3477   0.03\n1     1      M  3.57   244  17206   0.01\n2    18      F  3.24   503   3477   0.14\n3    18      M  3.52  2360  17206   0.14\n4    25      F  3.45  1034   3477   0.30\n5    25      M  3.63  6019  17206   0.35\n6    35      F  3.63   791   3477   0.23\n7    35      M  3.69  3755  17206   0.22\n8    45      F  3.67   508   3477   0.15\n9    45      M  3.67  1625  17206   0.09\n10   50      F  3.83   330   3477   0.09\n11   50      M  3.73  2090  17206   0.12\n12   56      F  3.77   220   3477   0.06\n13   56      M  3.80  1113  17206   0.06\n\n\n10.3 이 테이블을 다음과 같이 비율을 pointsize에 mapping하여 시각화해보고, 동일한 나이대에서 남녀의 평점 개수 비율에 차이가 크게 나는지 살펴보세요.\n\n\n\n\n\n\n\n\n\n10.4 이제 모든 장르를 한눈에 살펴보기 위해 facet을 이용해 다음과 같이 시각화해보세요.\n\n\n\n\n\n\n\n\n\n\n영화 제목에 있는 출시년도를 추출해 이용하기 위해 다음 코드를 활용하세요.\n\n\nmovies[\"year\"] = movies[\"title\"].str.extract(r'\\((\\d{4})\\)').astype(\"int\")\nmovies\n\n      movie_id                    title                        genres  year\n0            1         Toy Story (1995)   Animation|Children's|Comedy  1995\n1            2           Jumanji (1995)  Adventure|Children's|Fantasy  1995\n2            3  Grumpier Old Men (1995)                Comedy|Romance  1995\n...        ...                      ...                           ...   ...\n3880      3950         Tigerland (2000)                         Drama  2000\n3881      3951  Two Family House (2000)                         Drama  2000\n3882      3952    Contender, The (2000)                Drama|Thriller  2000\n\n[3883 rows x 4 columns]\n\n\n\n11.1 먼저 출시년도별로 얼마나 영화가 있는지 분포를 살펴보세요.\n11.2 출시년도가 없는 영화가 있는가요?\n11.3 오래된 영화일수록 나이든 사람들의 시청 비율이 높을지에 대해 분포를 살펴보세요.\n\n나이를 다음과 같이 (pandas) category type으로 변환하여 분석합니다.\n.assign(\n    age = lambda x: pd.Categorical(x.age.astype(\"string\"), categories=[\"1\", \"18\", \"25\", \"35\", \"45\", \"50\", \"56\"], ordered=True)\n)\n나이를 row에 facet하는 방식으로 분포를 살펴보세요.\n\n\n\n10년 기준으로 년대를 정했을 때, 년대에 따라 장르들의 비율이 어떻게 변화했는지 살펴봅니다.\n\n\n예를 들어, 코메디 장르의 경우 아래 플랏처럼 1920년대에 다른 장르에 비해 상대적으로 높은 비율을 보이다가 1950년대까지는 쭉 낮아지다 다시 증가하는 비율을 보입니다.\n장르별로 facet을 하여 모든 장르들의 비율 변화를 그려보세요.\n우선 년대를 다음 방식으로 만든 후, 예를 들어, 1990-1999년은 1990이 되도록 한 후\nmovies.assign(\n    decade = lambda x: x.year // 10 * 10  # // 나눗셈의 몫\n)\ndecade와 genres에 대해 pd.crosstab()을 적용하여 비율에 대한 테이블을 만든 후 stack()을 적용하여 long foramt으로 바꾼 후 그려보세요.\n\npd.crosstab()의 파라미터를 확인해서 비율을 구하세요.\n\n\n\n\n\n\n\n\n\n\n\n\n매니아적 성향을 파악해 봅니다.\n\n\n13.1 유저들 중에는 소수의 특정 장르만을 시청할 수 있는데, 다시 말해서 유저별 장르의 편향성을 알아볼 수 있는 지표를 만들어 특정 장르만을 독식하는 유저들을 파악해보는 방식에 대해 아이디어를 기술해보고, 코드로 구현할 수 있는 스텝을 대략 제시해보세요.\n13.2 구현된 코드로 실행한 결과를 표시해보세요.\n예를 들어, 유저별로 장르마다 몇 %씩을 평가했는지와 전체 유저의 장르별 평가 비율을 구해 상대적으로 유저들이 얼마나 치우쳐 장르를 소비하는 지를 알아보는 방식도 좋겠습니다.",
    "crumbs": [
      "Exploratory Analysis II",
      "MovieLens"
    ]
  },
  {
    "objectID": "contents/Exercises/billboard.html",
    "href": "contents/Exercises/billboard.html",
    "title": "Exercises",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Exploratory Analysis II",
      "Billboard"
    ]
  },
  {
    "objectID": "contents/Exercises/billboard.html#song-rankings-for-billboard-top-100-in-the-year-2000",
    "href": "contents/Exercises/billboard.html#song-rankings-for-billboard-top-100-in-the-year-2000",
    "title": "Exercises",
    "section": "Song rankings for Billboard top 100 in the year 2000",
    "text": "Song rankings for Billboard top 100 in the year 2000\nSource: The Whitburn Project\n다음 링크의 데이터는 빌보드차트에 관한 데이터입니다; billboard.csv\n\n각 곡이 차트에 진입한 날짜(date_entered)인 첫주(wk1)의 순위부터 78주(wk78)의 순위까지 기록되어 있습니다.\n차트에서 빠진 경우 missing (NA)으로 표시되어 있습니다.\n\n빌보드의 정책과 데이터 추출에 대해서 분명하지 않기 때문에 정확한 분석은 아닐 수 있습니다.\n\n예를 들어, 20주 연속 차트에 있거나, 50위 밖으로 밀려난 경우 차트에서 제거된다고 합니다.\n\n\n\n\n\n\n\n\nNote\n\n\n\n데이터를 불러오는 여러 방식에 대해서는 교재 참고\nChpter 6. Data Loading, Storage, and File Formats in Python for Data Analysis by Wes McKinney\n\n\n\nbillboard = pd.read_csv(\"../data/billboard.csv\")\nbillboard.head(5)\n\n         artist                    track date_entered  wk1   wk2   wk3   wk4  \\\n0         2 Pac  Baby Don't Cry (Keep...   2000-02-26   87 82.00 72.00 77.00   \n1       2Ge+her  The Hardest Part Of ...   2000-09-02   91 87.00 92.00   NaN   \n2  3 Doors Down               Kryptonite   2000-04-08   81 70.00 68.00 67.00   \n3  3 Doors Down                    Loser   2000-10-21   76 76.00 72.00 69.00   \n4      504 Boyz            Wobble Wobble   2000-04-15   57 34.00 25.00 17.00   \n\n    wk5   wk6   wk7  ...  wk67  wk68  wk69  wk70  wk71  wk72  wk73  wk74  \\\n0 87.00 94.00 99.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n1   NaN   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n2 66.00 57.00 54.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n3 67.00 65.00 55.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n4 17.00 31.00 36.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n\n   wk75  wk76  \n0   NaN   NaN  \n1   NaN   NaN  \n2   NaN   NaN  \n3   NaN   NaN  \n4   NaN   NaN  \n\n[5 rows x 79 columns]\n\n\n\n총 몇 명의 가수(artist)가 차트에 있으며, 가수별로 몇 곡(track)이 차트에 들어있는지 알아보세요. (동명이인은 없다고 가정하고)\n곡명은 같지만, 가수가 다른 곡이 있는지 알아보고, 서로 다른 노래가 차트에 몇 개나 있는지 알아보세요. 살펴보았다면, 이후 grouping시 artist와 track을 함께 사용해야함을 이해했을 겁니다.\n이 데이터를 주(week)에 대해서 아래처럼 long format으로 바꿉니다.\n\nwk column에 wk1부터 missing이 없는 wk*까지 숫자로 표현되고,\nrank column에는 해당하는 week의 순위가 나타납니다.\nmelt()를 이용하고, (id_vars=[\"artist\", \"track\", \"date_entered\"])\n.str.replace()와 .astype(\"int64\")를 사용해야 할 수 있습니다.\n.dropna(subset=\"rank\", inplace=True)를 사용하여 rank column의 missing을 처리하세요.\n결과를 billboard_long 변수에 할당하여 이후 문제를 이어가세요.\n구하지 못한 경우, 정제된 다음 파일을 다운받아 사용하세요.\n\n\n\nbillboard_long\n\n                artist                    track date_entered  wk  rank\n0                2 Pac  Baby Don't Cry (Keep...   2000-02-26   1 87.00\n317              2 Pac  Baby Don't Cry (Keep...   2000-02-26   2 82.00\n634              2 Pac  Baby Don't Cry (Keep...   2000-02-26   3 72.00\n...                ...                      ...          ...  ..   ...\n11728  matchbox twenty                     Bent   2000-04-29  37 38.00\n12045  matchbox twenty                     Bent   2000-04-29  38 38.00\n12362  matchbox twenty                     Bent   2000-04-29  39 48.00\n\n[5307 rows x 5 columns]\n\n\n\n50주 이상(포함) 머무른 곡들을 구해보세요.\n\n연속으로 머무를 필요없음; 차트에서 나갔다가 다시 들어오는 곡들이 있음\n머문 기간을 .size()를 이용해 구한 후\n오래 머무른 순서로 정렬 후\nquery()를 이용해 50주 이상으로 필터링\n결과가 artist와 track순으로 정렬되어 있는지 확인해 주세요!\n코드는 다음 예처럼 하나로 연결하여 간결하게 구성해보세요.\n\n(\n   billboard_long.groupby(...)\n   .size()\n   ...\n   .query(...)\n)   \n\n이후 코드도 최대한 이처럼 간결하게 구성해보도록 하세요.\n\n4번에서 구한 곡들 각각에 대해서 주에 따라 순위가 어떻게 변화했는지 시각화를 통해 살펴보기 위해서\n\n\n5.1 먼저, 위 곡들만을 포함하도록 4번의 데이터와 원래 데이터(billboard_long)를 merge()를 이용해 50주 이상 머문 곡으로 필터링하세요.\n\n4번을 구하지 못한 경우, 다음 파일을 받아 이용하세요.\n\n5.2 seaborn.objects를 이용해 대략 다음과 같이 주에 따른 순위의 변화를 그려보세요. \n\n\n(가수별) 곡별로 차트에 머문 기간(weeks)과 가장 높이 올라간 순위를 구해서, 높은 순위를 달성한 곡일수록 차트에 더 오래 머물었는지 알아봅니다.\n\n6.1 곡별로 차트에 머문 기간을 DataFrame으로 구하고; .reset_index(name=\" \") 이용\n6.2 곡별로 최상위 순위를 min()을 이용해 DataFrame으로 구한 후\n6.3 이 두 DataFrame을 merge()를 이용해 합친 후\n6.4 seaborn.objects를 이용해 머문 기간에 따른 최상위 순위에 대한 관계를 아래와 같이 scatterplot으로 살펴보세요.\n\n눈에 띄는 점을 간단히 기술해보세요.\n\n\n\n순위 1위를 달성한 곡들에 한해, 차트에 진입시 순위와 1위에 처음 도달한 주(week)의 정보가 다음과 같이 표시되도록 구해보세요.\n\n함수를 만들고; min()과 argmin()이 필요할 수 있음\napply()로 그 함수를 적용하여 구해보세요.\n\n#                 artist                    track  wk  rank\n# 0              Aaliyah                Try Again   1 59.00\n# 1              Aaliyah                Try Again  14  1.00\n# 2  Aguilera, Christina  Come On Over Baby (A...   1 57.00\n# 3  Aguilera, Christina  Come On Over Baby (A...  11  1.00\n# 4  Aguilera, Christina        What A Girl Wants   1 71.00\n# 5  Aguilera, Christina        What A Girl Wants   8  1.00\n...\n빠르게 1위가 된 곡일 수록 빠르게 차트에서 사라졌을까를 알아보기 위해, 7번의 결과를 이용해 다음과 같이 변형해보세요.\n\n7번을 구하지 못한 경우, 다음 파일을 받아 이용하세요.\n즉, 차트 진입시의 순위 정보와, 1위가 된 week의 정보만을 취해, 그 비율(rate)를 구하면, 얼마나 빠르게 1위가 되었는지 알 수 있습니다.\n\n#                 artist                    track  wk  rank   rate\n# 0              Aaliyah                Try Again  14 59.00   4.21\n# 1  Aguilera, Christina  Come On Over Baby (A...  11 57.00   5.18\n# 2  Aguilera, Christina        What A Girl Wants   8 71.00   8.88\n# 3        Carey, Mariah  Thank God I Found Yo...  11 82.00   7.45\n# 4                Creed      With Arms Wide Open  27 84.00   3.11\n# 5      Destiny's Child  Independent Women Pa...   9 78.00   8.67\n# ...\n\n마지막으로, seaborn.objects를 이용해 다음과 같이 시각화해보세요.",
    "crumbs": [
      "Exploratory Analysis II",
      "Billboard"
    ]
  },
  {
    "objectID": "contents/EDA/discretize.html",
    "href": "contents/EDA/discretize.html",
    "title": "Discretize",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n# import a dataset\ndiamonds = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\ndiamonds2 = diamonds.copy()  # unmodified copy",
    "crumbs": [
      "Exploratory Analysis I",
      "Discretize"
    ]
  },
  {
    "objectID": "contents/EDA/discretize.html#categorical-type-in-pandas",
    "href": "contents/EDA/discretize.html#categorical-type-in-pandas",
    "title": "Discretize",
    "section": "Categorical type in pandas",
    "text": "Categorical type in pandas\n\n일종의 데이터 압축 기술이며, 적은 levels로만 이루어진 데이터의 경우 메모리 절약\n순서가 부여되어, sorting을 하거나 min/max 함수가 알파벳 순서가 아닌 부여된 순서를 적용\n다른 Python library들 중에는 이 순서를 활용하는 것이 있음; 통계적 분석이나 플랏을 그릴 때\n\nMcKinney’s/Categorical data\npandas/Categorical data\n\npd.Categorical(diamonds[\"cut\"])  # defualt: alphabetical order\n\n['Ideal', 'Premium', 'Good', 'Premium', 'Good', ..., 'Ideal', 'Good', 'Very Good', 'Premium', 'Ideal']\nLength: 53940\nCategories (5, object): ['Fair', 'Good', 'Ideal', 'Premium', 'Very Good']\n\n\n\ndiamonds2[\"cut\"].astype(\"category\")  # defualt: alphabetical order\n\n0            Ideal\n1          Premium\n2             Good\n           ...    \n53937    Very Good\n53938      Premium\n53939        Ideal\nName: cut, Length: 53940, dtype: category\nCategories (5, object): ['Fair', 'Good', 'Ideal', 'Premium', 'Very Good']\n\n\n\n카테고리의 순서를 지정\n\ndiamonds[\"cut\"] = pd.Categorical(\n    diamonds[\"cut\"], \n    categories=[\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"],\n    ordered=True\n)\n\n\n# .astype() method를 쓰려면,\ndiamonds[\"cut\"] = (\n    diamonds[\"cut\"]\n    .astype(\"category\")\n    .cat.set_categories([\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"], ordered=True)\n)\n\n# 또는\nfrom pandas.api.types import CategoricalDtype\n\ncat_type = CategoricalDtype(\n    categories=[\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"], ordered=True\n)\ndiamonds[\"cut2\"] = diamonds[\"cut\"].astype(cat_type)\n\n\ndiamonds[\"cut\"]\n\n0            Ideal\n1          Premium\n2             Good\n           ...    \n53937    Very Good\n53938      Premium\n53939        Ideal\nName: cut, Length: 53940, dtype: category\nCategories (5, object): ['Fair' &lt; 'Good' &lt; 'Very Good' &lt; 'Premium' &lt; 'Ideal']\n\n\n\ndiamonds[\"cut\"].cat.categories  # categories 확인\n\nIndex(['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], dtype='object')\n\n\n\ndiamonds[\"cut\"].cat.codes  # codes 확인\n\n0        4\n1        3\n2        1\n        ..\n53937    2\n53938    3\n53939    4\nLength: 53940, dtype: int8\n\n\nCategory 타입의 변수는 데이터에 없는 level을 포함할 수 있음.\n\ndiamonds[\"cut2\"] = pd.Categorical(\n    diamonds[\"cut\"], \n    categories=[\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\", \"Perfect\"],  # \"Perfect\" 추가\n    ordered=True\n)\n\n\ndiamonds.value_counts(\"cut2\", sort=False)\n\ncut2\nFair          1610\nGood          4906\nVery Good    12082\nPremium      13791\nIdeal        21551\nPerfect          0\ndtype: int64\n\n\n\ndiamonds.groupby(\"cut2\")[\"price\"].mean()\n# 경고: 관칠값이 없는 카테고리는 앞으로 보여지지 않을 것임\n# 앞으로는 기본값이 diamonds.groupby(\"cut2\", observed=True)\n\ncut2\nFair        4358.76\nGood        3928.86\nVery Good   3981.76\nPremium     4584.26\nIdeal       3457.54\nPerfect         NaN\nName: price, dtype: float64\n\n\n\n\n카테고리 순서의 적용\n.value_count(), .groupby(), min(), max()\n시각화 library: seaborn, pandas\n\ndiamonds.value_counts(\"cut\", sort=False)\n\ncut\nFair          1610\nGood          4906\nVery Good    12082\nPremium      13791\nIdeal        21551\ndtype: int64\n\n\n\n# 결과의 index는 CategoricalIndex object\ndiamonds.value_counts(\"cut\", sort=False).index\n\nCategoricalIndex(['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], categories=['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], ordered=True, dtype='category', name='cut')\n\n\n\n# group keys\ndiamonds.groupby(\"cut2\")[\"price\"].mean()\n\ncut2\nFair        4358.76\nGood        3928.86\nVery Good   3981.76\nPremium     4584.26\nIdeal       3457.54\nPerfect         NaN\nName: price, dtype: float64\n\n\n\n# min(), max() : 카테고리 순서대로 계산\ndiamonds[\"cut\"].min()\n\n'Fair'\n\n\n\n# sort_values()에서도 카테고리 순서대로 정렬\ndiamonds.sort_values(\"cut\")\n\n       carat    cut color clarity  depth  table  price    x    y    z   cut2\n4654    1.00   Fair     F     SI1  66.70  57.00   3669 6.07 5.99 4.02   Fair\n53338   1.20   Fair     G      I1  64.40  55.00   2655 6.77 6.61 4.31   Fair\n40890   0.50   Fair     E     SI1  65.00  58.00   1176 4.98 4.90 3.21   Fair\n...      ...    ...   ...     ...    ...    ...    ...  ...  ...  ...    ...\n29308   0.25  Ideal     G     VS1  62.70  54.00    438 4.05 4.08 2.55  Ideal\n29339   0.31  Ideal     G     VS2  59.10  57.00    698 4.48 4.45 2.64  Ideal\n53939   0.75  Ideal     D     SI2  62.20  55.00   2757 5.83 5.87 3.64  Ideal\n\n[53940 rows x 11 columns]\n\n\nSeaborn도 Categorical type을 지원함.\n\nleft = so.Plot(diamonds, x=\"cut2\").add(so.Bar(), so.Count())\nright = so.Plot(diamonds2, x=\"cut\").add(so.Bar(), so.Count())  # catergorical type 아님\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(data=diamonds, x=\"cut2\", y=\"price\", fill=False)\nplt.show()  # 생략\n\n\n\n\n\n\n\ndiamonds.boxplot(\"price\", by=\"cut2\")  # pandas boxplot\nplt.show()  # 생략",
    "crumbs": [
      "Exploratory Analysis I",
      "Discretize"
    ]
  },
  {
    "objectID": "contents/EDA/discretize.html#discretizebining",
    "href": "contents/EDA/discretize.html#discretizebining",
    "title": "Discretize",
    "section": "Discretize/Bining",
    "text": "Discretize/Bining\n연속변수를 카테고리화하여 범주형 변수로 변환하여 분석\npd.cut(), pd.qcut()\n\nparameters: bins, precision, labels\n\n\n# pd.cut: 동일한 길이의 10개 구간\n(\n    diamonds\n    .assign(carat_cat = lambda x: pd.cut(x.carat, 10))\n    .value_counts(\"carat_cat\", sort=False)\n)\n\ncarat_cat\n(0.195, 0.681]    25155\n(0.681, 1.162]    18626\n(1.162, 1.643]     7129\n                  ...  \n(3.567, 4.048]        5\n(4.048, 4.529]        2\n(4.529, 5.01]         1\nLength: 10, dtype: int64\n\n\n\n# 나누는 구간을 지정\n(\n    diamonds\n    .assign(carat_cat = lambda x: pd.cut(x.carat, [0, 1, 3, 5]))\n    .value_counts(\"carat_cat\")\n)\n\ncarat_cat\n(0, 1]    36438\n(1, 3]    17470\n(3, 5]       31\ndtype: int64\n\n\n\n# pd.qcut: 동일한 갯수의 관측치를 포함하도록 하는 10개의 구간; 구간의 길이가 모두 다름\n(\n    diamonds.assign(carat_cat = lambda x: pd.qcut(x.carat, 10))\n    .value_counts(\"carat_cat\", sort=False)\n)\n\ncarat_cat\n(0.199, 0.31]    6452\n(0.31, 0.35]     4606\n(0.35, 0.42]     5421\n                 ... \n(1.01, 1.13]     4573\n(1.13, 1.51]     6052\n(1.51, 5.01]     4635\nLength: 10, dtype: int64\n\n\n\n\n\n\n\n\nNote\n\n\n\ncarat_cat = pd.cut(diamonds[\"carat\"], 3)\ncarat_cat.dtype\n# CategoricalDtype(categories=[(0.195, 1.803], (1.803, 3.407], (3.407, 5.01]], ordered=True)\n\ncarat_cat.cat.categories\n# IntervalIndex([(0.195, 1.803], (1.803, 3.407], (3.407, 5.01]], dtype='interval[float64, right]')",
    "crumbs": [
      "Exploratory Analysis I",
      "Discretize"
    ]
  },
  {
    "objectID": "contents/shortcuts.html",
    "href": "contents/shortcuts.html",
    "title": "오늘의 숏컷",
    "section": "",
    "text": "move : alt(option) + arrow up/down\ncopy : alt(option) + shift + arrow up/down\nctrl(cmd) + /\n셀 밖에서 a / b (esc, enter)\n셀 밖에서 c / x / v\ncmd + enter, cmd+ shift + enter (Mac)\nctrl + shift + -\nctrl(cmd) + d\nctrl(cmd) + [, ]\n\n\nctrl -\npandas=2.1.3, seaborn=0.13 update!"
  },
  {
    "objectID": "contents/Introduction/setup.html",
    "href": "contents/Introduction/setup.html",
    "title": "Python 설정",
    "section": "",
    "text": "데이터 사이언스를 위한 Python 개발 환경\n몇 가지 선택지",
    "crumbs": [
      "Introduction",
      "Setup"
    ]
  },
  {
    "objectID": "contents/Introduction/setup.html#클라우드-환경",
    "href": "contents/Introduction/setup.html#클라우드-환경",
    "title": "Python 설정",
    "section": "클라우드 환경",
    "text": "클라우드 환경\nColab\n\n사용법: Colab Welcome\n클라우드 환경 vs. 구글 드라이브 mount\nColab AI assistant\n구글 드라이브의 데이터셋을 import:\n\n\npd.read_csv(\"drive/MyDrive/...\")\n\n\n패키지 업데이트\n\n!pip install --upgrade pandas numpy seaborn matplotlib statsmodels scikit-learn",
    "crumbs": [
      "Introduction",
      "Setup"
    ]
  },
  {
    "objectID": "contents/Introduction/setup.html#로컬-환경",
    "href": "contents/Introduction/setup.html#로컬-환경",
    "title": "Python 설정",
    "section": "로컬 환경",
    "text": "로컬 환경\nPython과 Conda Package Manager\nConda Cheatsheet: 기본적인 conda 명령어 요약\n\nMiniconda 설치\nAnaconda보다는 기본 패키지들이 미리 설치되지 않는 miniconda를 추천: miniconda install page\n\nWindows 경우: 설치시 물어보는 “add Miniconda to your PATH variable” 옵션을 켜고 설치할 것\n\nShell 사용에 대해서는 아래 Command Line Tool 참고\n\nWindows 경우: Anaconda의 응용 프로그램으로 등록된 Anaconda Powershell Prompt를 이용\nMac의 경우: 기본 terminal을 이용\n커서 앞에 (base)가 보이면 conda가 설치된 것\n\n\n\n\n\n\n\nShell(Command Line Interface)에 대한 팁\n\n\n\n\n\nMac의 경우: 기본 terminal을 이용하되 기본 zsh shell 대신 다음 Oh-My-Zsh을 추천\nOh-My-Zsh!: 링크\nWindows의 경우: Windows Terminal 추천\n\n설치 링크는 구글링…\n명령프롬프트(CMD) vs. Powershell\nPowershell에서 conda를 사용하기 위해서는 몇 가지 설정 필요: 블로그 링크\n잘 안될 경우, conda 설치시 함께 설치되는 응용프로그램 콘다 powershell을 이용\n\n\n\n\n# Terminal (Mac) or Miniconda Powershell Prompt (Windows)\n\n(base)&gt; conda info # 콘다 정보 \n(base)&gt; conda update conda # 콘다 업데이트\n\n\nConda Environment\nconda/user guide\n환경 생성: miniconda에서 자체 제공하는 환경 (다른 가상환경 툴인 pyenv나 venv도 있음)\n(base)&gt; conda create --name myenv  # --name 대신 -n으로 축약 가능\n\n# 특정 버전의 파이썬과 함께 설치시\n(base)&gt; conda create --name myenv python=3.13\n환경 확인\n(base)&gt; conda env list\n# conda environments:\n#\n# base         */.../miniconda3\n# myenv         /.../miniconda3/envs/myenv\n환경 제거\n(base)&gt; conda env remove --name myenv\n환경 activate/deactivate\n(base)&gt; conda activate myenv\n(myenv)&gt; conda deactivate\n특정 환경 안의 파이썬 버전 확인\n(myenv)&gt; python --version\n\n\n환경(activated) 내에서 패키지 설치 및 제거\n\n\n\n\n\n\n패키지 repository(channel) 선택\n\n\n\n\n\nconda/managing channels\n다음을 통해 .condarc 환경파일에 configuration 추가\n(base)&gt; conda config --add channels conda-forge\n(base)&gt; conda config --set channel_priority strict  # 채널 순으로 검색, 버전 순이 아니고\n# 개별적으로 채널을 선택해서 install하려면 (특정 환경에 설치하려면 아래 conda environment 참조)\n(base)&gt; conda install --channel conda-forge scipy \n\n# pakcage가 있는 채널들\n(base)&gt; conda search scipy\n\n\n\n# 특정 환경을 activate한 후\n\n# Python을 update하거나 다른 버전을 설치하려면, 가령 3.13으로 업데이트 하려면\n(myenv)&gt; conda install python=3.13  # python update\n\n# 패키지 설치\n(myenv)&gt; conda install &lt;package name1&gt; &lt;package name2&gt; ...\n# 특정한 채널, conda-forge 통한 설치: --channel 대신 -c로 축약 가능\n(myenv)&gt; conda install --channel conda-forge &lt;package name&gt;\n\n# 제거\n(myenv)&gt; conda remove &lt;package name1&gt; &lt;package name2&gt; ...\n\n# 업데이트\n(myenv)&gt; conda update &lt;package name1&gt; &lt;package name2&gt; ...\n(myenv)&gt; conda update --all  # all packages\n\n# 패키지 리스트 확인\n(myenv)&gt; conda list\n환경 밖에서 특정 환경 안에 설치하려면 환경 이름 추가\n(base)&gt; conda install --name myenv &lt;package name1&gt;  # --name 대신 -n으로 축약 가능\npip을 이용한 패키지 설치: conda repository에 없는 패키지들을 설치하는 경우. 충돌의 우려 있음\n(myenv)&gt; pip install &lt;package name1&gt; &lt;package name2&gt; ...\n수업에 필요한 기본 패키지 설치\n# 수업에 필요한 기본 패키지 설치(conda-forge 채널 선택)\n(myenv)&gt; conda install --channel conda-forge jupyter numpy pandas matplotlib seaborn scikit-learn statsmodels",
    "crumbs": [
      "Introduction",
      "Setup"
    ]
  },
  {
    "objectID": "contents/Introduction/setup.html#sec-vscode",
    "href": "contents/Introduction/setup.html#sec-vscode",
    "title": "Python 설정",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\nVS Code 설치\n\n개인마다 선호하는 text editor가 있으나 본 수업에서는 VS Code로 진행: download and install here\n\n\n\nExtensions\n\nPython\nPython Extension Pack 중\n\nIntelliCode\nPython Environment Manager\n\nPylance: 문법 체크, 자동완성, …\nDocs View\n\n안 보일시, 설정에서 language server를 default(Pylance)에서 Jedi로 바꾸면 해결\n\nCopilot…\n\n\n\nPreferences\n\nThemes\nFont, font size (notebook, markup, output)\n\n\n\nShortcuts\nShow Command Palette: ctrl(cmd) + shift + p, 또는 F1\nCell 안과 밖에서 다르게 작동\n\nundo / redo : ctrl(cmd) + z / ctrl(cmd) + shift + z\nmove: alt(option) + arrow up/down\ncopy : alt(option) + shift + arrow up/down\n\n코드 실행 방식 3가지: ctrl/shift/alt(option) + enter\nHelp: Keyboard shortcuts reference의 Basic editing 참고\n\n\n그 외\n\ninteractive mode\nexport\ndocs view: sticky mode\nvariables viewer, data viewer\nformatter: “Black formatter”\nsnippets: 구글링…\n\n\n\nVS Code내에서 terminal 사용\nTerminal: Select Default Profile에서 선택\n\nMac: zsh\nWindows: powershell",
    "crumbs": [
      "Introduction",
      "Setup"
    ]
  },
  {
    "objectID": "contents/Introduction/setup.html#jupyter-notebooklab",
    "href": "contents/Introduction/setup.html#jupyter-notebooklab",
    "title": "Python 설정",
    "section": "Jupyter Notebook/Lab",
    "text": "Jupyter Notebook/Lab\n\n\n\n\n\n\n콘다 환경 등록\n\n\n\n\n\n새로 만든 환경을 등록해줘야 함. 환경을 activate한 상태에서\n(myenv)&gt; ipython kernel install --user --name=myenv\n환경을 삭제해도 등록시킨 kernel 이름은 삭제되지 않으니 직접 삭제.\n등록된 커널 리스트를 확인\n(myenv)&gt; jupyter kernelspec list\n커널 삭제\n(myenv)&gt; jupyter kernelspec remove myenv\n\n\n\nJupyter Notebook 또는 lab 실행\n\nAnaconda 응용 프로그램을 이용해 실행하거나,\n쉘에서 실행하려면,\n\n# jupytet notebook\n(base)&gt; jupyter notebook\n\n# jupyter lab\n(base)&gt; jupyter lab\n등록한 커널을 선택 후 시작\n커널을 종료하려면, 쉘에서 Ctrl-C 두 번",
    "crumbs": [
      "Introduction",
      "Setup"
    ]
  },
  {
    "objectID": "contents/Introduction/intro2.html",
    "href": "contents/Introduction/intro2.html",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), CCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 물리법칙의 발견, 약물의 합성, 생체 내 상호작용의 메커니즘 규명\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 만족도 조사, 취약 계층, 우울\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’",
    "crumbs": [
      "Introduction",
      "Data Analysis Intro"
    ]
  },
  {
    "objectID": "contents/Introduction/intro2.html#미래-데이터의-중요성",
    "href": "contents/Introduction/intro2.html#미래-데이터의-중요성",
    "title": "개관",
    "section": "",
    "text": "다양한 소스들로부터 데이터 생성: 전지구적 개인과 환경에 대한 상세한 정보 발생\n인터넷 & 통신 (SNS, 사진, 위치, 장소, 유동인구, 상품거래, 물류)\n사물인터넷 (IoT), CCTV\n스마트 팩토리, 파밍\n게놈프로젝트, 생체정보, 의료/보건: 인류, 실시간\n\n23andMe, Theranos\n\n과학적 발견: 물리법칙의 발견, 약물의 합성, 생체 내 상호작용의 메커니즘 규명\n자율주행차량: 내부, 외부\n금융정보 및 흐름\n사회 지표 활용: 고용, 직업, 연봉, 만족도 조사, 취약 계층, 우울\n생성형 인공지능: 기계의 정보 생산\nAI companion: 개인 내면에 대한 정보\n\n영화 Her (2013), Sony’s robotics dog ‘Aibo’",
    "crumbs": [
      "Introduction",
      "Data Analysis Intro"
    ]
  },
  {
    "objectID": "contents/Introduction/intro2.html#사회적-파장",
    "href": "contents/Introduction/intro2.html#사회적-파장",
    "title": "개관",
    "section": "사회적 파장",
    "text": "사회적 파장\n\n유토피아 vs. 디스토피아\n\n초연결성, 투명성 vs. 완전한 감시와 통제\n개인화된 서비스 vs. 설득/유혹/조작\n개별성/자율성 vs. 피동적/비주체적\n기계와의 교감 vs. 인간관계의 소외, 현실과의 단절\n정보와 인간에 대한 신뢰 약화와 사회적 연대, 문명 붕괴\n자연과의 조화 vs. 생태계의 파괴\n\n\n\n\n\n\n\n\n\n\n\nBrave New World, 1932\n\n\n\n\n\n\n\nThe Technological Society, 1954\n\n\n\n\n\n\n\nSapiens, Homo Deus, 21 Lessons for the 21st Century by Yuval Noah Harari\n\n\n\n\n\n\n\n\n\n\n\nYuval Noah Harari: An Urgent Warning They Hope You Ignore.\n\n\n\nThe Social Dilemma (2020)\nNetflix documentary",
    "crumbs": [
      "Introduction",
      "Data Analysis Intro"
    ]
  },
  {
    "objectID": "contents/Introduction/intro2.html#data-science",
    "href": "contents/Introduction/intro2.html#data-science",
    "title": "개관",
    "section": "Data Science",
    "text": "Data Science\n\nArtificial intelligence (인공 지능)\nMachine learning (기계 학습)\nDeep learning (심층 학습)\nData mining (데이터 마이닝)\nStatistical Learning (통계적 학습)\n\n\n\n\n소프트웨어 개발\n데이터에 기반한 분석 위해 작동하도록 프로그래밍을 하여 운영되도록 하는 일\n주로 전통적인 컴퓨터 사이언스의 커리큘럼에 의해 트레이닝\n\n유튜브의 영상 추천\n페이스북의 친구 매칭\n스팸메일 필터링\n자율주행\n\n\n\n\n\n\n데이터 분석\n하나의 구체적인 질문에 답하고자 함\n다양한 소스의 정제되는 않은 데이터를 통합하거나 가공하는 기술이 요구\n\nDNA의 분석을 통해 특정 질병의 발병 인자를 탐색\n유동인구와 매출을 분석해 상권을 분석\n어떤 정책의 유효성을 분석에 정책결정에 공헌\n교통 흐름의 지연이 어떻게 발생하는지를 분석, 해결책 제시",
    "crumbs": [
      "Introduction",
      "Data Analysis Intro"
    ]
  },
  {
    "objectID": "contents/Introduction/intro2.html#skills",
    "href": "contents/Introduction/intro2.html#skills",
    "title": "개관",
    "section": "Skills",
    "text": "Skills\n\n\nDomain knowledge\n\n해결하려는 문제에 대한 이해없이 단순한 알고리즘만으로 “one size fits all”은 효과적이지 않음\n추상화된 현실에 대한 모형은 수많은 가정/사전 지식(prior knowledge)을 전제하고 있음.\n각 분야의 전문 지식은 데이터가 발생되는 과정, 데이터의 특성, 데이터의 의미를 이해하는데 필수적\n\nEthics\n\n데이터를 합법적이고 적절하게 사용하려면 규정을 이해하고, 자신의 업무에 미치는 영향과 사회에 미치는 파급력 대한 윤리적 이해가 필요\n\n배출(exhaust) 데이터: 어떤 목적을 가진 데이터 수집 프로세스로부터 얻어진 부산물\n\n소셜 미디어: 사용자가 다른 사람들과 소통할 수 있도록 도움\n\n공유된 이미지, 블로그 게시물, 트윗, 좋아요 등으로부터\n누가/얼마나 많이 보았는지/좋아요/리트윗을 했는지 등을 수집\n\n아마존 웹사이트: 다양한 물건을 편리하게 구매할 수 있도록 도움\n\n사용자가 장바구니에 어떤 품목을 담았는지, 사이트에 얼마나 오래 머물렀는지, 어떤 다른 품목을 보았는지 등을 수집\n\n메타데이터(metadata)\n통화 내역만으로 많은 민감한 정보을 유추할 수 있음\n\n알코올 중독자 모임, 이혼 전문 변호사, 성병 전문 병원 등\n\n\n한편, 서비스와 마케팅을 타겟팅할 수 있는 잠재력\n\n\nWrangling\n\n데이터 소스는 다양한 형식으로 존재\n통합, 정리, 변환, 정규화 등의 작업이 요구\ndata munging, data wrangling, data cleaning, data preparation, data preprocessing 등으로 불림\n\nDatabase & computer science\n\n수집된 데이터가 저장되고, 가공/추출된 데이터의 재저장 등 데이터베이스와의 소통할 수 있는 기술\n다양해지고 방대해진 빅데이터를 저장/배포하기 위한 도구를 활용\nML 모델을 이해하고 개발하여 제품의 출시, 분석, 백엔드 애플리케이션에 통합할 수 있는 기술 등\n\nVisualisation\n\n작업 프로세스의 모든 과정에 관여\n\n데이터를 탐색하거나,\n데이터의 의미를 효과적으로 전달\n\n\nStatistics & Probability\n\n데이터 과학 프로세스 전반에 걸쳐 사용됨\n\n초기 수집과 조사\n다양한 모델과 분석의 결과를 해석\n의사결정에 활용\n\n\nMachine Learning\n\n데이터로부터 패턴을 찾기 위한 다양한 알고리즘을 사용\n응용 측면에서는\n\n수많은 알고리즘에 대해 가정, 특성, 용도, 결과의 의미, 적용가능한 유형의 데이터 등\n해결할 문제와 데이터에 가장 적합한 알고리즘을 파악\n\n\nCommunication\n\n데이터에 담긴 스토리를 효과적으로 전달하는 능력\n분석을 통해 얻은 인사이트, 조직 내 목적에 어떻게 부합하는지, 조직의 기능에 미칠 수 있는 영향 등",
    "crumbs": [
      "Introduction",
      "Data Analysis Intro"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contents/Introduction/rethinking.html",
    "href": "contents/Introduction/rethinking.html",
    "title": "Statistical thinking",
    "section": "",
    "text": "Data is a window, not a mirror to reality!\n\n수많은 가정과 사전 지식을 전제로 함: 올바른 결과 뿐 아니라 효율적인 분석을 위해 필요\n분석 결과는 주어진 가정에 대한 분명한 제시(transparency)와 그 가정을 기반으로 한 적절한 해석이 요구됨\n\n\n\n\n\n\n예측 모델\n예측의 신속성과 정확성\nMachine Learning 강점\nAlgorithmic\n\n이미지/사물 인식\n개인화된 추천 목록: 유튜브, 넷플릭스\n시리, ChatGPT의 답변\n비즈니스 분석\n이상치 탐지\n\n\n\n\n관계/원인 분석\n현상/실재에 대한 이해과 매커니즘 파악\nStatistical Models 강점\nParametric\n\n음식/운동의 효능\n광고의 효과\n복지/치안 정책의 효과\n\n\n\n\n\n\n\n\n\nSource: The Book of Why by Judea Pearl, Dana Mackenzie (2018)\n\n\n\n\n관찰을 기반으로 규칙성 발견하고 예측\n올빼미가 쥐의 움직임을 관찰하고 잠시 후 쥐가 어디에 있을지를 파악\n컴퓨터 바둑 프로그램이 수백만 개의 바둑 게임 데이터베이스를 연구하여 어떤 수와 승률이 높은지 알아내는 것\n하나의 이벤트를 관찰하면 다른 이벤트를 관찰할 가능성이 달라진다면, 하나의 이벤트가 다른 이벤트와 연관되어 있다고 말할 수 있음\n“치약을 구매한 고객이 치실도 구매할 가능성이 얼마나 되는가?”; \\(P(치실 ~| 치약~)\\)\n통계의 핵심: 상관관계, 회귀\n올빼미는 쥐가 왜 항상 A 지점에서 B 지점으로 가는지 이해하지 못해도 훌륭한 사냥꾼이 될 수 있음\n위스키 한 병을 들고 있는 보행자가 경적을 울릴 때 다르게 반응할 가능성이 있다는 것을 기계가 스스로 파악할 수 있는가?\n\nAssociation 단계의 한계: 유연성과 적응성의 부족\n\n\n\n\n\n\n관찰을 넘어, 세상에 대한 개입\n“치약 가격을 두 배로 올리면 치실 판매량은 어떻게 될까?”\n데이터에는 없는 새로운 종류의 지식을 요구\n통계학의 언어로는 이 질문을 표현하는 것조차 불충분함\n수동적으로 수집된 데이터만으로는 이러한 질문에 대답할 수 없음\n\n과거의 데이터를 이용하면?\n과거에 가격이 두 배 비쌌을 때, 치실 판매량으로 추론?\n이전에 가격이 두 배 비쌌을 때, 다른 이유가 있었을 수 있음\n\n전통적으로 실험을 통해 해결\n정확한 인과 관계 모델이 있으면 관찰 데이터만으로도 가능; \\(P(치실 ~| ~do(치약~))\\)\n사실, 일상 생활에서 항상 개입을 수행: 어떻게(How) 하면 두통이 사라질까?\n\n\n\n\n\n두통이 사라졌다면 왜(Why) 그럴까?\n약을 먹지 않았어도 두통이 사라졌을까?: 반사실적(가상의) 세계 (counterfactual world)\nIndividual Treatment Effect: \\(\\tau_i \\equiv Y_i(1) - Y_i(0)\\): the fundamental problem of causal inference\nAverage Treatment Effect: \\(\\tau \\equiv E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)]\\): 여러 관측치(데이터)를 이용해 (노이즈를 제거하면서) 최적의 인과효과를 추정\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(T\\)\n\\(Y\\)\n\\(Y(1)\\)\n\\(Y(0)\\)\n\\(Y(1) - Y(0)\\)\n\n\n\n\n1\n0\n0\n\n0\n?\n\n\n2\n1\n1\n1\n\n?\n\n\n3\n1\n0\n0\n\n?\n\n\n4\n0\n1\n\n1\n?\n\n\n…\n…\n…\n…\n…\n?\n\n\n\n\n\n\n“현재 치약을 구매한 고객이 가격을 두 배로 올려도 여전히 치약을 구매할 확률은 얼마인가?”\n우리는 현실 세계(고객이 현재 가격으로 치약을 구매했다는 것을 알고 있는)와 가상의 세계(가격이 두 배 높은 경우)와 비교\n보이는 세계  볼 수 있는 새로운 세계  볼 수 없는 세계(보이는 것과 모순)\n이를 위해서는 “이론” 또는 “자연의 법칙”이라고 볼 수 있는 근본적인 인과 과정의 모델이 필요\n\n전형적인 인과적 질문들\n\n\n\nHow effective is a given treatment in preventing a disease?\nWas it the new tax break that caused our sales to go up? Or our marketing campaign?\nWhat is the annual health-care costs attributed to obesity?\nCan hiring records prove an employer guilty of sex discrimination?\nI am about to quit my job, will I regret it?\n\n\n\n특정 치료법이 질병 예방에 얼마나 효과적일까요?\n새로운 세금 감면 혜택이 매출 상승의 원인이었을까요? 아니면 마케팅 캠페인 때문이었나요?\n비만으로 인한 연간 의료 비용은 얼마인가요?\n채용 기록으로 고용주의 성차별을 입증할 수 있나요?\n직장을 그만두려고 하는데 후회하게 될까요?\n\n   번역 by DeepL\n\n\n\n\n\n\n닭의 울음이 태양을 솟게 하는가?\n돈과 행복: 패턴 vs. 예외 \n\n특정 A의 임금이 p 에서 q 로 증가할 때, 트렌드대로 움직이겠는가?\n\n특정 B의 임금이 r 에서 s 로 감소할 때, 트렌드대로 움직이겠는가?\n특정 C의 임금을 올려주면, 트렌드대로 움직이겠는가?\n\n\n미혼자에 대한 임금 차별 vs. 편견\n\n미혼자에 대한 임금 차별이 있는가? 차별이 의미하는 바는 무엇인가?\n연령을 고려한 후에도 기혼자의 임금은 미혼자보다 높은가?\n연령을 고려한 후/연령을 조정한 후(adjusted for age)의 차이는 얼마라고 봐야하는가?\n\n\n연령을 고려한 마라톤 기록?\n\n나이와는 무관한/독립적인 마라톤 실력에 대해 말하고자 함\n\n\n\nSource: https://doi.org/10.1186/2052-1847-6-31\n\n가난, 인종, 범죄 간의 관계\nRacial differences in homicide rates are poorly explained by economics\n출산율은 왜 감소하는가?\n\n\n\n\n\n\n심리적 관성/편견 주의\n분석가의 책임의식\n두 가지 접근법(예측과 이해)는 서로 상보 관계!\n\n\n\n\n\n\n\n탐색적 분석 vs. 가설 검증\nexploratory vs. confirmatory\n\n탐색적 분석\n\n통찰 혹은 가설의 기초 제공\n끼워 맞추기? 오류에 빠지기 쉬움: spurious associations\n\n가설 검증\n\n진위의 확률을 높임\n탐색적 분석으로부터 온 가설은 재테스트\n\n\n\n\n\n관찰 vs. 실험 데이터\nobservational vs. experimental\n\n당근과 시력?\n커피의 효과?\n남녀의 임금 차별?\n심리치료의 효과?\n\n\n\n\n표본 vs. 모집단\nsample vs. population\n\nParameter(모수), uncertainty(불확실성)\n내일 태양이 뜰 확률?\n연봉과 삶의 만족도와 관계\n성별과 임금과의 관계\n두통약의 효능: “effect size”",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#예측-모델-vs.-관계원인-분석",
    "href": "contents/Introduction/rethinking.html#예측-모델-vs.-관계원인-분석",
    "title": "Statistical thinking",
    "section": "",
    "text": "예측 모델\n예측의 신속성과 정확성\nMachine Learning 강점\nAlgorithmic\n\n이미지/사물 인식\n개인화된 추천 목록: 유튜브, 넷플릭스\n시리, ChatGPT의 답변\n비즈니스 분석\n이상치 탐지\n\n\n\n\n관계/원인 분석\n현상/실재에 대한 이해과 매커니즘 파악\nStatistical Models 강점\nParametric\n\n음식/운동의 효능\n광고의 효과\n복지/치안 정책의 효과",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#causal-inference",
    "href": "contents/Introduction/rethinking.html#causal-inference",
    "title": "Statistical thinking",
    "section": "",
    "text": "Source: The Book of Why by Judea Pearl, Dana Mackenzie (2018)\n\n\n\n\n관찰을 기반으로 규칙성 발견하고 예측\n올빼미가 쥐의 움직임을 관찰하고 잠시 후 쥐가 어디에 있을지를 파악\n컴퓨터 바둑 프로그램이 수백만 개의 바둑 게임 데이터베이스를 연구하여 어떤 수와 승률이 높은지 알아내는 것\n하나의 이벤트를 관찰하면 다른 이벤트를 관찰할 가능성이 달라진다면, 하나의 이벤트가 다른 이벤트와 연관되어 있다고 말할 수 있음\n“치약을 구매한 고객이 치실도 구매할 가능성이 얼마나 되는가?”; \\(P(치실 ~| 치약~)\\)\n통계의 핵심: 상관관계, 회귀\n올빼미는 쥐가 왜 항상 A 지점에서 B 지점으로 가는지 이해하지 못해도 훌륭한 사냥꾼이 될 수 있음\n위스키 한 병을 들고 있는 보행자가 경적을 울릴 때 다르게 반응할 가능성이 있다는 것을 기계가 스스로 파악할 수 있는가?\n\nAssociation 단계의 한계: 유연성과 적응성의 부족\n\n\n\n\n\n\n관찰을 넘어, 세상에 대한 개입\n“치약 가격을 두 배로 올리면 치실 판매량은 어떻게 될까?”\n데이터에는 없는 새로운 종류의 지식을 요구\n통계학의 언어로는 이 질문을 표현하는 것조차 불충분함\n수동적으로 수집된 데이터만으로는 이러한 질문에 대답할 수 없음\n\n과거의 데이터를 이용하면?\n과거에 가격이 두 배 비쌌을 때, 치실 판매량으로 추론?\n이전에 가격이 두 배 비쌌을 때, 다른 이유가 있었을 수 있음\n\n전통적으로 실험을 통해 해결\n정확한 인과 관계 모델이 있으면 관찰 데이터만으로도 가능; \\(P(치실 ~| ~do(치약~))\\)\n사실, 일상 생활에서 항상 개입을 수행: 어떻게(How) 하면 두통이 사라질까?\n\n\n\n\n\n두통이 사라졌다면 왜(Why) 그럴까?\n약을 먹지 않았어도 두통이 사라졌을까?: 반사실적(가상의) 세계 (counterfactual world)\nIndividual Treatment Effect: \\(\\tau_i \\equiv Y_i(1) - Y_i(0)\\): the fundamental problem of causal inference\nAverage Treatment Effect: \\(\\tau \\equiv E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)]\\): 여러 관측치(데이터)를 이용해 (노이즈를 제거하면서) 최적의 인과효과를 추정\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(T\\)\n\\(Y\\)\n\\(Y(1)\\)\n\\(Y(0)\\)\n\\(Y(1) - Y(0)\\)\n\n\n\n\n1\n0\n0\n\n0\n?\n\n\n2\n1\n1\n1\n\n?\n\n\n3\n1\n0\n0\n\n?\n\n\n4\n0\n1\n\n1\n?\n\n\n…\n…\n…\n…\n…\n?\n\n\n\n\n\n\n“현재 치약을 구매한 고객이 가격을 두 배로 올려도 여전히 치약을 구매할 확률은 얼마인가?”\n우리는 현실 세계(고객이 현재 가격으로 치약을 구매했다는 것을 알고 있는)와 가상의 세계(가격이 두 배 높은 경우)와 비교\n보이는 세계  볼 수 있는 새로운 세계  볼 수 없는 세계(보이는 것과 모순)\n이를 위해서는 “이론” 또는 “자연의 법칙”이라고 볼 수 있는 근본적인 인과 과정의 모델이 필요\n\n전형적인 인과적 질문들\n\n\n\nHow effective is a given treatment in preventing a disease?\nWas it the new tax break that caused our sales to go up? Or our marketing campaign?\nWhat is the annual health-care costs attributed to obesity?\nCan hiring records prove an employer guilty of sex discrimination?\nI am about to quit my job, will I regret it?\n\n\n\n특정 치료법이 질병 예방에 얼마나 효과적일까요?\n새로운 세금 감면 혜택이 매출 상승의 원인이었을까요? 아니면 마케팅 캠페인 때문이었나요?\n비만으로 인한 연간 의료 비용은 얼마인가요?\n채용 기록으로 고용주의 성차별을 입증할 수 있나요?\n직장을 그만두려고 하는데 후회하게 될까요?\n\n   번역 by DeepL",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#구체적인-예들",
    "href": "contents/Introduction/rethinking.html#구체적인-예들",
    "title": "Statistical thinking",
    "section": "",
    "text": "닭의 울음이 태양을 솟게 하는가?\n돈과 행복: 패턴 vs. 예외 \n\n특정 A의 임금이 p 에서 q 로 증가할 때, 트렌드대로 움직이겠는가?\n\n특정 B의 임금이 r 에서 s 로 감소할 때, 트렌드대로 움직이겠는가?\n특정 C의 임금을 올려주면, 트렌드대로 움직이겠는가?\n\n\n미혼자에 대한 임금 차별 vs. 편견\n\n미혼자에 대한 임금 차별이 있는가? 차별이 의미하는 바는 무엇인가?\n연령을 고려한 후에도 기혼자의 임금은 미혼자보다 높은가?\n연령을 고려한 후/연령을 조정한 후(adjusted for age)의 차이는 얼마라고 봐야하는가?\n\n\n연령을 고려한 마라톤 기록?\n\n나이와는 무관한/독립적인 마라톤 실력에 대해 말하고자 함\n\n\n\nSource: https://doi.org/10.1186/2052-1847-6-31\n\n가난, 인종, 범죄 간의 관계\nRacial differences in homicide rates are poorly explained by economics\n출산율은 왜 감소하는가?",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#분석가의-태도",
    "href": "contents/Introduction/rethinking.html#분석가의-태도",
    "title": "Statistical thinking",
    "section": "",
    "text": "심리적 관성/편견 주의\n분석가의 책임의식\n두 가지 접근법(예측과 이해)는 서로 상보 관계!",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#데이터-분석에-관한-전통적인-분류",
    "href": "contents/Introduction/rethinking.html#데이터-분석에-관한-전통적인-분류",
    "title": "Statistical thinking",
    "section": "",
    "text": "탐색적 분석 vs. 가설 검증\nexploratory vs. confirmatory\n\n탐색적 분석\n\n통찰 혹은 가설의 기초 제공\n끼워 맞추기? 오류에 빠지기 쉬움: spurious associations\n\n가설 검증\n\n진위의 확률을 높임\n탐색적 분석으로부터 온 가설은 재테스트\n\n\n\n\n\n관찰 vs. 실험 데이터\nobservational vs. experimental\n\n당근과 시력?\n커피의 효과?\n남녀의 임금 차별?\n심리치료의 효과?\n\n\n\n\n표본 vs. 모집단\nsample vs. population\n\nParameter(모수), uncertainty(불확실성)\n내일 태양이 뜰 확률?\n연봉과 삶의 만족도와 관계\n성별과 임금과의 관계\n두통약의 효능: “effect size”",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#distributions",
    "href": "contents/Introduction/rethinking.html#distributions",
    "title": "Statistical thinking",
    "section": "Distributions",
    "text": "Distributions\n\n남녀 임금의 차이\n \nAssociatiions과 그 strengths 비교\n\n\n\n카테고리 변수에 대해서도 비슷하게 생각할 수 있음.\n이 경우, 두 그룹 간의 차이에 대한 효과의 크기를 말할 수 있고, 예를 들어, 결혼과 삶의 만족도 간의 관계(association)와 그 강도(strength)",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#confounding",
    "href": "contents/Introduction/rethinking.html#confounding",
    "title": "Statistical thinking",
    "section": "Confounding",
    "text": "Confounding\n일반적으로, 표면적으로 드러난 변수간의 관계가 숨겨진 다른 변수들(lurking third variables)에 의해 매개되어 있어 진실한 관계가 아닌 경우, confounding 혹은 confounder가 존재한다고 함.\n\nCommon Cause/Fork\n신발을 신고 잠든 다음날 두통이 생긴다면?\n\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n극단적이지만 이해하지 쉬운 예로는\n\n머리가 길면 우울증도 높다?\n초등생이 발이 크면 독해력도 높다?\n\n\n\n\n\n\n\nAnswers!\n\n\n\n\n\nSpurious relations\n \n\n\n\n앞서 든 예도 마찬가지로\n  \n올바른 관계를 파악하려면, 동일한 나이에 대해 그 관계를 파악한 후 각 나이에서의 효과를 (weighted) 평균해서 살펴봐야함\n통계에서는 이를 나이를 통제 (control for age)한다고 표현하며, 같은 의미로 다음과 같은 표현을 씀\n\n나이를 고려했을 때; account for age\n\n나이를 조정했을 때; adjust for age\n\n나이와 무관/독립인; independent of age\n\nSimpson’s paradox\n\nSource: The book of why by Judea Pearl\n예를 들어, 은퇴한 노인들을 대상으로 규칙적인 걷기가 사망율을 감소시킬 것이라는 가설을 확인하기 위해 1965년 이후 8000명 가량의 남성들을 추적조사한 데이터의 일부를 이용했는데,\n\n12년 후 사망율에서 casual walker(하루 1마일 이하)와 intense walker(하루 2마일 이상)가 각각 43%, 21.5%로 나타났음.\n이 걷기의 효과를 의심케 하는 요소들(confounding)은 무엇인가?\n\n\n\n\n\n\n\nAnswers!\n\n\n\n\n\n\n건강이 나빠 많이 걷지 못했을 수도…\n많이 걷는 사람은 상대적으로 젊을 수도…\n많이 먹는 사람이 덜 걸을 수도…\n술을 많이 먹는 사람이 덜 걸을 수도…\n\n\n\n\n남녀 연봉 차이의 원인을 찾으려면?\n\n직업 특성, 부서, 직급, 연령, 출산, 출세욕\n\n\n\nCOVID-27\nSource: Introduction to Causal Inference (ICI) by Brady Neal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n학생들의 과제는 성적에 영향을 주는가?\nSource: National Education Longitudinal Study of 1988 (NELS:88)\n\n\n\nColiders/Immorality\n미모가 뛰어나면 연기력이 떨어지는가?\n\n코딩 기술이 뛰어나면 협업능력이 떨어지는가?\n어느 회사에서 지원자의 코딩 능력과 협업 능력을 1점부터 5점까지 정량화하여,\n총점 8점 이상을 받은 지원자를 모두 채용한다고 했을 때,\n\n\n\nMechanisms/Mediations/Chains\n\n만약, 장거리 항해에서 상급자(높은 연령)에게만 과일이 제공되었을 때, 나이가 많은 선원들에게서 괴혈병이 덜 생겼다는 현상으로부터 연령과 괴혈병의 (직접적) 관계를 추론해서는 안됨. 하지만 예측은 여전히 유효함.\n\n\n\nInteraction/Moderation\n나이가 듦(age)에 따라 지구력(endurance)의 감소가 강도 높은 운동을 한 기간(년수)(exercise)에 따라 변화\n\n보호 요인 (protective factor)\n위험 요인 (risk factor)\n\n\n\n\n\n\n\n\nInteraction의 패턴\n\n\n\n\nSynergistic or enhancing interaction\n\n\n상호작용 효과가 원래 효과들과 같은 방향으로 작용하는 경우\n삶의 만족도(Y)가 직업 스트레스(X)와 부정적인 관계에 있고, 부부관계의 문제(Z)와도 부정적인 관계에 있는 경우\n이 둘의 상호작용이 부정적이라면, 직업 스트레스와 부부관계의 문제가 동시에 증가하면 각각의 sum이 예측하는 것보다 더 낮은 삶의 만족도가 예측됨.\n\n\nBuffering interaction\n\n\n두 변수가 반대 방향으로 Y에 작용하고 있을 때, 한 변수가 다른 변수의 효과를 감소시키는 경우\n즉, 한 변수의 impact가 다른 변수의 impact를 줄여주는 경우\n건강보건에 대한 연구에서, 한 변수가 질병의 위험요인이고 다른 변수가 질병의 위험을 줄여주는 보호요인인 경우\n위의 예에서처럼, 나이(X)는 지구력 감소의 위험요인이고, 운동기간(Z)은 지구력 보호요인인 경우\n\n\nInterference or antagonistic interactionin\n\n\n두 변수가 같은 방향으로 Y에 작용하고 있을 때, 상호작용은 반대 방향으로 작용하는 경우\n대학생의 학업성취도(Y)에 대하여, 학업동기(X)와 학업능력(Z)이 모두 학업성취도(Y)에 긍정적인 영향을 미치나 이 두 변수는 서로 보완적인 효과를 가지고 있음.\n즉, 성취도에 대한 학업능력의 중요성은 높은 학업동기에 의해 낮아질 수 있음.\n반대로, 학업동기에 대한 중요성은 높은 학업능력에 의해 낮아질 수 있음.",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#selection-bias",
    "href": "contents/Introduction/rethinking.html#selection-bias",
    "title": "Statistical thinking",
    "section": "Selection Bias",
    "text": "Selection Bias\n수집된 데이터의 특성에 따라 인과추론을 방해하거나(confounding); internal validity(내적 타당도)\n일반화할 수 있는 대상의 범위가 제한됨; external validity(외적 타당도)\n\n노인에 관한 데이터: 누가 사망했는가?\n\nSurvival bias: 일종의 collider bias\n예를 들어, 비만이 사망율에 미치는 효과에 대한 과소추정\n\n\n의료 분야에서 발견되는 패러독스\n\n비만은 당뇨 환자에게 이익이 되는가?\n\n과거 기록을 이용?; 수녀들의 자서전 연구\n\n추적조사/종단연구(longitudinal study)\n\n회사 구성원에 대한 조사: 근속년수에 따른 샘플 속성의 변화\n누가 참여(안)했는가? 어떤 방식으로 참여했는가?\n\n관측되지 않은 데이터: 어떤 사람/대상이 왜 누락되었는가?\n어떤 사람들이 설문/실험에 참여했는가? 혹은 어떤 문항에 응답했는가?/하지 않았는가?\n\n어떤 유저들의 데이터인가? 가령, SNS의 기록은 누가 남기는가?\n코호트/특정세대의 특성: 그들만의 특성인가?\n\n\n\nAbraham Wald: “Where are the missing holes?”\n\nSource: War History Online",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/Introduction/rethinking.html#experiments",
    "href": "contents/Introduction/rethinking.html#experiments",
    "title": "Statistical thinking",
    "section": "Experiments",
    "text": "Experiments\n\n개입없이 수동적으로 얻은 관찰 데이터의 분석에서는 항상 confounding이 존재할 기능성이 있음\n결정적인 인과관계를 파악하기 위해, 전통적으로 “통계학”의 시각에서 인과문제를 해결하기 위해 RCT (randomized controlled trial)라고 부르는 소위 gold standard한 실험 연구를 통해서 해결하고자 했음\n개념적으로는 “물리적 통제”라고 볼 수 있음; vs. “통계적 통제”\n두 그룹으로 집단을 randomly assign(무선/무작위 배정/할당): 모든 면에서 동질한 성향을 가짐. 예를 들어, 두 집단의 연령이 평균적으로 동일해짐.\n분야마다 효과를 제대로 검증하기 위한 많은 실험 설계들이 발전되었음; 연구방법론\n\n \n앞서 든 예에서, 걷기가 사망율에 미치는 효과를 검증하려면, 가령 600명을 300명씩 두 그룹으로 무작위로 나눈 후 한쪽은 1마일 이하를 걷도록 하고 나머지는 2마일 이상을 걷게 한 후 12년 후 사망율을 확인해야 함.\n하지만, 실험 연구는 자체로 많은 한계를 지님\n\n많은 경우 실험이 불가능하거나 완전한 통제가 어려움\n실험에서 처치한 구체적인 상황에서만 유효하고; 어느 지형을 어느 속도로 누구와 어떻게 걸었는지에 대한 실험 통제하에서\n따라서 그 효과 또한 일반화되어 표현하기 어려움\n반대로, 덜 통제된 실험의 경우 어떤 요인의 효과인지 불분명\n완전한 통제를 할수록 더 인위적인 상황이 연출됨; 자연스러운/현실적인 상황에서 적용된다는 보장이 없음\n실험 참여자는 어떻게 왜 참여한 것인가?",
    "crumbs": [
      "Introduction",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "contents/notice.html#중간고사",
    "href": "contents/notice.html#중간고사",
    "title": "Notice",
    "section": "중간고사",
    "text": "중간고사",
    "crumbs": [
      "{{< fa check size=xs >}} Notice"
    ]
  },
  {
    "objectID": "contents/notice.html#기말고사",
    "href": "contents/notice.html#기말고사",
    "title": "Notice",
    "section": "기말고사",
    "text": "기말고사",
    "crumbs": [
      "{{< fa check size=xs >}} Notice"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월, 수 1:00 ~ 2:50PM\n면담 시간: 수업 후\nWebsite: dgdavs.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "{{< fa angles-right size=xs >}} Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-정보",
    "href": "index.html#강의-정보",
    "title": "Welcome",
    "section": "",
    "text": "강사: 조성균\nemail: sk.cho@snu.ac.kr\n수업시간: 월, 수 1:00 ~ 2:50PM\n면담 시간: 수업 후\nWebsite: dgdavs.modellings.art\n과제: Notice\n질문: Communicate/Ask",
    "crumbs": [
      "{{< fa angles-right size=xs >}} Welcome"
    ]
  },
  {
    "objectID": "index.html#강의-개요",
    "href": "index.html#강의-개요",
    "title": "Welcome",
    "section": "강의 개요",
    "text": "강의 개요\n본 강의에서는 인터넷과 기술의 발전으로 풍부한 데이터들이 양산됨에 따라 그 안에 숨겨진 패턴을 찾고 분석하여 실증적 사실과 원리를 파악하는데 요구되는 기술들을 계발하는데 도움을 주고자 합니다. 이를 위해서는 1) 데이터 분석 툴을 자유자재로 다룰 수 있는 기술, 2) 주어진 데이터에 적절한 툴을 선택할 수 있는 판단력, 3) 파악한 패턴으로부터 현상의 본질을 추론할 수 있는 인과관계 추론의 원리들이 함께 필요합니다.\n\n데이터를 조작, 가공하는 기술을 익히고,\n이를 통해 얻은 정제된 데이터를 시각화를 통해패턴을 여러 각도에서 살펴보고,\n데이터 모델링과 통계적 분석을 접목하여 현상에 대한 올바른이해를 돕습니다.\n\n수업은 크게 5부분으로 나뉨\n\n탐색적 분석 (exploratory data analysis)과 그에 필요한 데이터 전처리 (data wrangling)\n데이터 시각화 (data visualization)\n기술적 분석 (descriptive analysis)\n모델링 (modelling)\n통계 (statistics)\n\n\n참고도서\n\n번역서: Pandas를 이용한 데이터 분석 실습 2/e\nHands-On Data Analysis with Pandas (2e) by Stefanie Molin: code in GitHub\nPython for Data Analysis (3e) by Wes McKinney: code in GitHub\n3판 번역서: 파이썬 라이브러리를 활용한 데이터 분석\nR for Data Science by Wickham & Grolemund; 2nd edition",
    "crumbs": [
      "{{< fa angles-right size=xs >}} Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-활동",
    "href": "index.html#수업-활동",
    "title": "Welcome",
    "section": "수업 활동",
    "text": "수업 활동\n출석 (10%), 일반과제 (30%), 중간고사 (30%), 기말고사 (30%)",
    "crumbs": [
      "{{< fa angles-right size=xs >}} Welcome"
    ]
  },
  {
    "objectID": "index.html#수업-계획",
    "href": "index.html#수업-계획",
    "title": "Welcome",
    "section": "수업 계획",
    "text": "수업 계획\n1주. 강의 개요 및 데이터 분석의 의미 소개\n2주. 넘파이와 판다스 라이브러리의 기본\n3주. 탐색적 분석을 위한 시각화 라이브러리\n4주. 시각화의 활용1\n5주. 시각화의 활용2\n6주. 판다스를 이용한 데이터의 변형 및 가공 1\n7주. 판다스를 이용한 데이터의 변형 및 가공 2\n8주. 기술적(descriptive) 분석 1\n9주. 기술적(descriptive) 분석 2\n10주. 탐색적 분석 & 및 중간고사\n11주. 데이터 모델링 기초 & 통계의 기초 개념\n12주. 데이터 모델링 1\n13주. 데이터 모델링 2\n14주. Binary 명목 변수에 대한 분석: 로지스틱 회귀분석 및 GLM 소개\n15주. 기말고사",
    "crumbs": [
      "{{< fa angles-right size=xs >}} Welcome"
    ]
  },
  {
    "objectID": "contents/EDA/eda.html",
    "href": "contents/EDA/eda.html",
    "title": "Exploratory Analysis I",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nsource: R for Data Science\nVisualising, transforming, modelling을 통해 질문들을 개선하거나 새로운 질문들을 생성하면서 데이터에 대한 이해를 늘리면서 질문들에 답을 구하는 반복순환 과정\n크게 다음 2가지 타입의 질문을 기본으로 시작\n# import a dataset\ndiamonds_data = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\")\ndiamonds = diamonds_data.data\nprint(diamonds_data.__doc__)\ndiamonds\n\n       carat        cut color clarity  depth  table  price    x    y    z\n0       0.23      Ideal     E     SI2  61.50  55.00    326 3.95 3.98 2.43\n1       0.21    Premium     E     SI1  59.80  61.00    326 3.89 3.84 2.31\n2       0.23       Good     E     VS1  56.90  65.00    327 4.05 4.07 2.31\n...      ...        ...   ...     ...    ...    ...    ...  ...  ...  ...\n53937   0.70  Very Good     D     SI1  62.80  60.00   2757 5.66 5.68 3.56\n53938   0.86    Premium     H     SI2  61.00  58.00   2757 6.15 6.12 3.74\n53939   0.75      Ideal     D     SI2  62.20  55.00   2757 5.83 5.87 3.64\n\n[53940 rows x 10 columns]\n# cut, color, clarity 모두 Categorical type으로 변형\ndiamonds[\"cut\"] = pd.Categorical(\n    diamonds[\"cut\"], \n    categories=[\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"],\n    ordered=True\n)\ndiamonds[\"color\"] = pd.Categorical(\n    diamonds[\"color\"], \n    categories=[\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"],\n    ordered=True\n)\ndiamonds[\"clarity\"] = pd.Categorical(\n    diamonds[\"clarity\"], \n    categories=[\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"],\n    ordered=True\n)",
    "crumbs": [
      "Exploratory Analysis I"
    ]
  },
  {
    "objectID": "contents/EDA/eda.html#missing-values",
    "href": "contents/EDA/eda.html#missing-values",
    "title": "Exploratory Analysis I",
    "section": "Missing values",
    "text": "Missing values\n특이값을 missing (NA)으로 처리할 때는 신중하게…\n특이한 값들이 있는 행을 다 제거하는 방식은 금물!\n예를 들어, diamonds.query('y &gt;= 3 & y &lt;= 20')\n만약 NA로 바꾸기로 했다면, .where(), .mask()를 활용\n\ndiamonds2 = diamonds.assign(\n    y = lambda x: np.where((x.y &lt; 3) | (x.y &gt; 20), np.nan, x.y)\n)\n\n\n\n\n\n\n\nTip\n\n\n\n.mask(), .where()\n# 조건에 맞는 값들을 NA로 바꿈\ndiamonds[\"y\"] = diamonds[\"y\"].mask((diamonds[\"y\"] &lt; 3) | (diamonds[\"y\"] &gt; 20))\n\n\n\n# NA의 제거에 대해 경고 없음!\n(\n    so.Plot(diamonds2, x=\"x\", y=\"y\")\n    .add(so.Dots(alpha=1))\n)\n\n\n\n\n\n\n\n\n결측치들을 포함한 경우와 제거한 경우를 비교해 보고자 할 때,\n\n결측치인지 아닌지를 나타내는 명목변수를 이용",
    "crumbs": [
      "Exploratory Analysis I"
    ]
  },
  {
    "objectID": "contents/EDA/eda.html#a-categorical-and-continuous-variable",
    "href": "contents/EDA/eda.html#a-categorical-and-continuous-variable",
    "title": "Exploratory Analysis I",
    "section": "A categorical and continuous variable",
    "text": "A categorical and continuous variable\n\n카테고리별로 분포(frequency)를 나누어 비교\nfrequency polygon 이나 boxplot을 이용\nOne dimensional scatter plot으로 겹치지 않게 그리려면, jitter를 활용\n\n\n(\n    so.Plot(diamonds, x=\"price\", color=\"cut\")\n    .add(so.Line(), so.Hist(binwidth=500))\n)\n\n\n\n\n\n\n\n\n\n# 각 cut 내에서의 분포가 cut마다 어떻게 다른가 확인\n(\n    so.Plot(diamonds, x=\"price\", color=\"cut\")\n    .add(so.Line(), so.Hist(binwidth=500, stat=\"proportion\", common_norm=False))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(diamonds, x=\"price\", color=\"cut\")\n    .add(so.Bars(), so.Hist(binwidth=500, stat=\"proportion\", common_norm=False))\n    .facet(\"cut\")\n)\n\n\n\n\n\n\n\n\nQ: 왜 fair 컷의 평균 가격이 가장 높은가???\nsns.boxplot(diamonds, x=\"cut\", y=\"price\", fill=False);\n\n\n\n\n\n\n\n\nUse custom plots\n\n# Custom plots\nfrom sbcustom import rangeplot\nrangeplot(diamonds, x=\"cut\", y=\"price\").layout(size=(6, 4))\n\n\n\n\n\n\n\n\n# Violin plot\nsns.violinplot(diamonds, x=\"cut\", y=\"price\", fill=False);\n\n\n\n\n\n\n\n다이아몬드 컷의 질이 낮을수록 평균 가격이 높은, 직관적으로 반대되는 패턴을 보임.\n한편, 카테고리의 순서가 존재하지 않는 경우: 의미있는 순서로 재정렬하여 패턴 파악을 용이하게 할 수 있음\n예를 들어, 자동차의 class를 고속도로 연비의 중앙값 순으로 정렬하고자 하면,\nmpg = sm.datasets.get_rdataset(\"mpg\", \"ggplot2\").data\nhwy_order = mpg.groupby(\"class\")[\"hwy\"].median().sort_values().index\n\nsns.boxplot(mpg, y=\"class\", x=\"hwy\", fill=False, order=hwy_order);\n\n\n\n\n\n\n\n\n(\n    rangeplot(mpg, y=\"class\", x=\"hwy\", marker=\"v\")\n    .scale(y=so.Nominal(order=hwy_order))\n    .layout(size=(6, 4))\n)\n\n\n\n\n\n\n\n\n\n연습문제\n\n** What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?\n* Compare and contrast a violin plot with a facetted histogram, or a coloured frequency polygon. What are the pros and cons of each method?",
    "crumbs": [
      "Exploratory Analysis I"
    ]
  },
  {
    "objectID": "contents/EDA/eda.html#two-categorical-variables",
    "href": "contents/EDA/eda.html#two-categorical-variables",
    "title": "Exploratory Analysis I",
    "section": "Two categorical variables",
    "text": "Two categorical variables\n\n두 범주형 변수 사이의 covariation을 파악하려면, 두 변수 값의 모든 조합에 대한 count를 표시\n\n\ntable_cut = diamonds.groupby([\"cut\", \"clarity\"]).size().reset_index(name=\"n\")\ntable_cut\n\n      cut clarity     n\n0    Fair      I1   210\n1    Fair     SI2   466\n2    Fair     SI1   408\n..    ...     ...   ...\n37  Ideal    VVS2  2606\n38  Ideal    VVS1  2047\n39  Ideal      IF  1212\n\n[40 rows x 3 columns]\n\n\n\n각 조합에 해당하는 관측값의 양에 비례하여 원의 크기를 표시하면,\ncut와 clarity 사이에 약한 상관관계를 맺는 것으로 보임\n\n\np1 = (\n    so.Plot(table_cut, x=\"cut\", y=\"clarity\", pointsize=\"n\", color=\"n\")\n    .add(so.Dot())\n    .scale(pointsize=(5, 30))\n)\np2 = (\n    so.Plot(diamonds, x='cut', y='clarity')\n    .add(so.Line(), so.PolyFit(3))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(diamonds, x='cut', color='clarity')\n    .add(so.Bars(), so.Hist(stat='proportion', common_norm=[\"x\"]), so.Stack())\n    .scale(color=so.Nominal(order=diamonds.clarity.cat.categories))\n)\n\n/Users/skcho/miniconda3/envs/envconda/lib/python3.11/site-packages/seaborn/_stats/counting.py:228: UserWarning: Undefined variable(s) passed for Hist.common_norm: 'x'.\n  self._check_grouping_vars(\"common_norm\", grouping_vars)\n\n\n\n\n\n\n\n\n\n순서가 없는 범주형 변수인 경우, 행과 열을 유사한 정도에 따라 순서를 매기는 알고리즘을 통해 재정렬하여 패턴을 볼 수도 있음\n\ngapminder = sm.datasets.get_rdataset(\"gapminder\", \"gapminder\").data\ngapminder\n\n          country continent  year  lifeExp       pop  gdpPercap\n0     Afghanistan      Asia  1952    28.80   8425333     779.45\n1     Afghanistan      Asia  1957    30.33   9240934     820.85\n2     Afghanistan      Asia  1962    32.00  10267083     853.10\n...           ...       ...   ...      ...       ...        ...\n1701     Zimbabwe    Africa  1997    46.81  11404948     792.45\n1702     Zimbabwe    Africa  2002    39.99  11926563     672.04\n1703     Zimbabwe    Africa  2007    43.49  12311143     469.71\n\n[1704 rows x 6 columns]\n\n\n\nplotdata = (\n    gapminder\n    .query('continent == \"Asia\"')\n    .pivot(index=\"country\", columns=\"year\", values=\"lifeExp\")  # wide format 변환\n)\nplotdata\n\n\n\n\n\n\n\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n2002\n2007\n\n\ncountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n28.80\n30.33\n32.00\n34.02\n36.09\n38.44\n39.85\n40.82\n41.67\n41.76\n42.13\n43.83\n\n\nBahrain\n50.94\n53.83\n56.92\n59.92\n63.30\n65.59\n69.05\n70.75\n72.60\n73.92\n74.80\n75.64\n\n\nBangladesh\n37.48\n39.35\n41.22\n43.45\n45.25\n46.92\n50.01\n52.82\n56.02\n59.41\n62.01\n64.06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nVietnam\n40.41\n42.89\n45.36\n47.84\n50.25\n55.76\n58.82\n62.82\n67.66\n70.67\n73.02\n74.25\n\n\nWest Bank and Gaza\n43.16\n45.67\n48.13\n51.63\n56.53\n60.77\n64.41\n67.05\n69.72\n71.10\n72.37\n73.42\n\n\nYemen, Rep.\n32.55\n33.97\n35.18\n36.98\n39.85\n44.17\n49.11\n52.92\n55.60\n58.02\n60.31\n62.70\n\n\n\n\n33 rows × 12 columns\n\n\n\n\n# hierarchical clustering in scipy\nsns.clustermap(plotdata, col_cluster=False, method=\"ward\");\n\n\n\n\n\n\n\n\n\n\nHierarchical Clustering\nimport scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(\n    sch.linkage(plotdata, method=\"ward\"),\n    labels=plotdata.index,\n    orientation=\"left\",\n    no_plot=True,  # dendrogram plot 생략\n)\n\n(\n    gapminder\n    .query('continent == \"Asia\"')\n    .pipe(so.Plot, x=\"year\", y=\"country\", color=\"lifeExp\")  # pipe operator\n    .add(so.Dot(pointsize=12, marker=\"s\"))  # square marker\n    .scale(y=so.Nominal(order=dendrogram[\"ivl\"]), color=\"rocket\")\n    .layout(size=(4, 7))\n)\n\n\n\n\n\n\n\n\n\n\n연습문제\n\nHow could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut?\n\ncount 대신 비율을 계산 후 그려보세요.\n예를 들어, color D에 cut 각각의 비율들을 구해 시각화\n pd.crosstab(diamonds[\"cut\"], diamonds[\"color\"], normalize='columns')  # 'columns': 비율을 구하는 방향\n\nExplore how average flight departure delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?\n\ngroupby([\"month\", \"dest\"])로 그룹핑을 해서 시작해 볼 것\n결측치를 어떻게 처리하면 좋을지 생각해 볼 것\nsns.clustermap() 또는 위의 dendrogram을 직접 이용한 플랏도 그려볼 것",
    "crumbs": [
      "Exploratory Analysis I"
    ]
  },
  {
    "objectID": "contents/EDA/eda.html#two-continuous-variables",
    "href": "contents/EDA/eda.html#two-continuous-variables",
    "title": "Exploratory Analysis I",
    "section": "Two continuous variables",
    "text": "Two continuous variables\n\nScatterplot, 2d-histogram\nDiscretize: pd.cut(), pd.qcut()\n\n\n(\n    so.Plot(diamonds.query('carat &lt; 3'), x=\"carat\", y=\"price\")\n    .add(so.Dots(alpha=1/100, color=\".6\"))\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n2d-histogram: x, y축 모두 binning\nfrom matplotlib.colors import LogNorm\n\nsns.jointplot(diamonds, x=\"carat\", y=\"price\", kind=\"hex\", gridsize=30, height=5, norm=LogNorm());  # gridsize: bin 개수\n\n\n\n\n\n\n\nDiscretize: 연속 변수를 카테고리화\n\ndiamonds_cat = diamonds.query(\"carat &lt; 3\").assign(\n    carat_cat=lambda x: pd.cut(x.carat, 20, labels=False),\n    carat_qcat=lambda x: pd.qcut(x.carat, 20, labels=False),\n)\n\nsns.boxplot(diamonds_cat, x=\"carat_cat\", y=\"price\", fill=False);\n\n\n\n\n\n\n\n\nrangeplot(diamonds_cat, x=\"carat_cat\", y=\"price\")\n\n\n\n\n\n\n\n\nsns.boxplot(diamonds_cat, x=\"carat_qcat\", y=\"price\", fill=False);\n\n\n\n\n\n\n\n\n연습문제\n\nInstead of summarising the conditional distribution with a boxplot, you could use a frequency polygon.\n\n즉, 두 연속변수 중 하나를 카테고리화하는데, boxplot대신 frequency polygon으로 그려볼 것\n\nVisualise the distribution of carat, partitioned by price.\nHow does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you?\nCombine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price.\nTwo dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately.\nWhy is a scatterplot a better display than a binned plot for this case?\n\n\n(\n    so.Plot(diamonds, x='x', y='y')\n    .add(so.Dots())\n    .limit(x=(4, 11), y=(4, 11))\n)",
    "crumbs": [
      "Exploratory Analysis I"
    ]
  },
  {
    "objectID": "contents/EDA/eda.html#buliding-models",
    "href": "contents/EDA/eda.html#buliding-models",
    "title": "Exploratory Analysis I",
    "section": "Buliding models",
    "text": "Buliding models\nDiamonds 데이터셋에서,\n컷(cut)과 가격(price)의 관계는 예상과 달랐는데, 이는 컷(cut)과 무게(carat), 무게(carat)와 가격(price) 사이에 깊은 관계가 있었기 때문임.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델링을 통해 가격(price)과 크기(carat) 사이의 강한 관계를 제거할 수 있도록 모델을 세울 수 있음\n\n캐럿 당 가격으로 처리?\n\n이 관계를 제거한 후 남은 패턴을 탐색할 수 있음\nBuild a model!\n\nLinear model, 선형(직선) 모형을 여기서는 특히, \\(y = b_1x + b_0\\)인 1차 함수꼴의 모형으로 세움\n\\(price =  b_1 * carat + b_0 + e\\) (residual/error, 잔차/예측오차)의 모형을 세운 후 데이터에 가장 적합한(fit) 파라미터 \\(b_1, b_0\\) 값을 구하는 과정: fitted model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidual samples\n\n\n\n\n\nimport statsmodels.formula.api as smf\n\ndiamonds2 = diamonds.query(\"carat &lt; 3\")\ndiamonds_fit = smf.ols(\"price ~ carat\", data=diamonds2).fit()\n\ndiamonds2 = diamonds2.assign(\n    pred=diamonds_fit.fittedvalues,\n    resid=diamonds_fit.resid,\n)\n\nnp.random.seed(126)\ndiamonds2[[\"cut\", \"carat\", \"price\", \"pred\", \"resid\"]].sample(7)\n\n\n\n\n\n             cut  carat  price     pred    resid\n32609      Ideal   0.31    802   117.54   684.46\n16255      Ideal   1.35   6502  8263.21 -1761.21\n17451      Ideal   1.01   6999  5600.20  1398.80\n16089       Good   1.01   6429  5600.20   828.80\n42762    Premium   0.56   1345  2075.63  -730.63\n26768       Good   2.02  16593 13510.90  3082.10\n47809  Very Good   0.71   1902  3250.49 -1348.49\n\n\nprint(diamonds_fit.params)  # print coefficients\nprint(f\"R2 = {diamonds_fit.rsquared:.3f}\")  # print R-squared\n\n\n\nIntercept   -2310.50\ncarat        7832.37\ndtype: float64\nR2 = 0.853\n\n\n\n\nresiduals (잔차)은 모형이 예측하지 못하는 정도를 나타내며, 그것들의 총체를 줄이는 것이 좋은 모형임.\n\n총체를 구하는 방법에는 여러가지가 있음.\n\n크기(carat)로 가격(price)을 예측/설명(account for)하는 정도를 제거한 후, 즉, 크기(carat)로 예측/설명되지 않는 가격(price)의 정도에 대해서 다른 변수들과의 관계를 탐구\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n(\n    so.Plot(diamonds2, x='carat', y='resid')\n    .add(so.Dots(color=\".6\", alpha=1/100))\n    .add(so.Line(color=\"red\"), so.Agg(lambda x: 0))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n캐럿(carat)으로 설명되지 않는 가격의 값들로만 추가 분석을 실시하면,\n즉, cut으로 “다이아몬드의 무게로는 설명되지 않는 가격”의 변량을 얼마나 추가로 설명할 수 있는가?\n\n\n\n\n\n\nCode\n\n\n\n\n\nfrom sbcustom import rangeplot\n(\n    rangeplot(diamonds2, x=\"cut\", y=\"resid\")\n    .add(so.Line(color=\"red\"), so.Agg(lambda x: 0))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n다이아몬드의 무게로 가격을 설명한 후 남은(residualized) 가격의 변량을 cut이 어떻게 예측하는가?\n다이아몬드의 무게로 설명되지(not acounted for) 않는 가격의 변량을 cut이 어떻게 예측하는가?\n다이아몬드의 무게가 가격에 미친 영향을 제거한 후, 혹은 그 영향을 넘어서서 (above and beyond) cut이 어떻게 가격을 예측하는가?\n다이아몬드의 무게(carat)가 동일하다면, 즉 무게가 고정된다면 (hold it constant), 무게의 영향이 제거된 다이아몬드의 퀄리티(cut, …)에 따른 가격에 대한 관계가 나타날 것임.\n\n다음과 같이 다이아몬드의 무게를 비슷한 구간으로 나누어 살펴보면,\n\n\n\n\n\n\nCode\n\n\n\ndiamonds2[\"carat_cat\"] = pd.qcut(diamonds2[\"carat\"], 12)\n(\n    rangeplot(diamonds2, x='cut', y='price')\n    .facet(\"carat_cat\", wrap=3)\n    .share(y=False)\n    .layout(size=(8, 8))\n)",
    "crumbs": [
      "Exploratory Analysis I"
    ]
  },
  {
    "objectID": "contents/Exercises/billboard_sol.html",
    "href": "contents/Exercises/billboard_sol.html",
    "title": "Billboard Solutions",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Solutions",
      "Billboard"
    ]
  },
  {
    "objectID": "contents/Exercises/billboard_sol.html#song-rankings-for-billboard-top-100-in-the-year-2000",
    "href": "contents/Exercises/billboard_sol.html#song-rankings-for-billboard-top-100-in-the-year-2000",
    "title": "Billboard Solutions",
    "section": "Song rankings for Billboard top 100 in the year 2000",
    "text": "Song rankings for Billboard top 100 in the year 2000\n“billboard” in a package “tidyr”\nSource: The Whitburn Project\n다음 링크의 데이터는 빌보드차트에 관한 데이터입니다; 링크\n\n각 곡이 차트에 진입한 날짜(date_entered)인 첫주(wk1)의 순위부터 78주(wk78)의 순위까지 기록되어 있습니다.\n차트에서 빠진 경우 missing (NA)으로 표시되어 있습니다.\n\n빌보드의 정책과 데이터 추출에 대해서 분명하지 않기 때문에 정확한 분석은 아닐 수 있습니다.\n\n예를 들어, 20주 연속 차트에 있거나, 50위 밖으로 밀려난 경우 차트에서 제거된다고 합니다.\n\n\n\nbillboard = pd.read_csv(\"data/billboard.csv\")\nbillboard.head(5)\n\n         artist                    track date_entered  wk1   wk2   wk3   wk4  \\\n0         2 Pac  Baby Don't Cry (Keep...   2000-02-26   87 82.00 72.00 77.00   \n1       2Ge+her  The Hardest Part Of ...   2000-09-02   91 87.00 92.00   NaN   \n2  3 Doors Down               Kryptonite   2000-04-08   81 70.00 68.00 67.00   \n3  3 Doors Down                    Loser   2000-10-21   76 76.00 72.00 69.00   \n4      504 Boyz            Wobble Wobble   2000-04-15   57 34.00 25.00 17.00   \n\n    wk5   wk6   wk7  ...  wk67  wk68  wk69  wk70  wk71  wk72  wk73  wk74  \\\n0 87.00 94.00 99.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n1   NaN   NaN   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n2 66.00 57.00 54.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n3 67.00 65.00 55.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n4 17.00 31.00 36.00  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n\n   wk75  wk76  \n0   NaN   NaN  \n1   NaN   NaN  \n2   NaN   NaN  \n3   NaN   NaN  \n4   NaN   NaN  \n\n[5 rows x 79 columns]\n\n\n\n총 몇 명의 가수(artist)가 차트에 있으며, 가수별로 몇 곡(track)이 차트에 들어있는지 알아보세요. (동명이인은 없다고 가정하고)\n\n\nbillboard.value_counts(\"artist\")\n\nartist\nJay-Z                5\nHouston, Whitney     4\nDixie Chicks, The    4\n                    ..\nHollister, Dave      1\nHot Boys             1\nmatchbox twenty      1\nName: count, Length: 228, dtype: int64\n\n\n\n곡명은 같지만, 가수가 다른 곡이 있는지 알아보고, 서로 다른 노래가 차트에 몇 개나 있는지 알아보세요.\n\n\nbillboard.value_counts(\"track\")\n\ntrack\nWhere I Wanna Be      2\nOriginal Prankster    1\nSeparated             1\n                     ..\nHe Loves U Not        1\nHe Can't Love U       1\nwww.memory            1\nName: count, Length: 316, dtype: int64\n\n\n\n이 데이터를 주(week)에 대해서 아래처럼 long format으로 바꿉니다.\n\n\nbillboard_long = (\n    billboard\n    .melt(id_vars=billboard.columns[:3], var_name=\"wk\", value_name=\"rank\")\n    .sort_values([\"artist\", \"track\"])\n)\nbillboard_long[\"wk\"] = billboard_long[\"wk\"].str.replace(\"wk\", \"\").astype(\"int64\")\nbillboard_long.dropna(subset=\"rank\", inplace=True)\n\n\nbillboard_long\n\n                artist                    track date_entered  wk  rank\n0                2 Pac  Baby Don't Cry (Keep...   2000-02-26   1 87.00\n317              2 Pac  Baby Don't Cry (Keep...   2000-02-26   2 82.00\n634              2 Pac  Baby Don't Cry (Keep...   2000-02-26   3 72.00\n...                ...                      ...          ...  ..   ...\n11728  matchbox twenty                     Bent   2000-04-29  37 38.00\n12045  matchbox twenty                     Bent   2000-04-29  38 38.00\n12362  matchbox twenty                     Bent   2000-04-29  39 48.00\n\n[5307 rows x 5 columns]\n\n\n\n50주 이상(포함) 머무른 곡들\n\n\nwks_50 = (\n    billboard_long.groupby([\"artist\", \"track\"], as_index=False)\n    .size()\n    .sort_values(\"size\", ascending=False)\n    .query(\"size &gt;= 50\")\n)\nwks_50\n\n           artist       track  size\n62          Creed      Higher    57\n179      Lonestar      Amazed    55\n121   Hill, Faith     Breathe    53\n2    3 Doors Down  Kryptonite    53\n\n\n5.1 먼저, 위 곡들만을 포함하도록 4번의 데이터와 원래 데이터(billboard_long)를 merge()를 이용해 추린 후, 50주 이상 머문 곡으로 필터링하세요.\n\nwks_50_all =  billboard_long.merge(wks_50)\nwks_50_all\n\n           artist       track date_entered  wk  rank  size\n0    3 Doors Down  Kryptonite   2000-04-08   1 81.00    53\n1    3 Doors Down  Kryptonite   2000-04-08   2 70.00    53\n2    3 Doors Down  Kryptonite   2000-04-08   3 68.00    53\n..            ...         ...          ...  ..   ...   ...\n215      Lonestar      Amazed   1999-06-05  62 42.00    55\n216      Lonestar      Amazed   1999-06-05  63 45.00    55\n217      Lonestar      Amazed   1999-06-05  64 50.00    55\n\n[218 rows x 6 columns]\n\n\n5.2 seaborn.objects를 이용해 대략 다음과 같이 주에 따른 순위의 변화를 그려보세요.\n\n(\n    so.Plot(wks_50_all, x=\"wk\", y=\"rank\", color=\"track\")\n    .add(so.Line())\n    .scale(color=\"Set2\")\n)\n\n\n\n\n\n\n\n\n6.1 곡별로 차트에 머문 기간을 포함한 데이터\n\nlengths = (\n    billboard_long\n    .groupby([\"artist\", \"track\"])\n    .size()\n    .reset_index(name=\"length\")\n)\nlengths\n\n              artist                    track  length\n0              2 Pac  Baby Don't Cry (Keep...       7\n1            2Ge+her  The Hardest Part Of ...       3\n2       3 Doors Down               Kryptonite      53\n..               ...                      ...     ...\n314  Ying Yang Twins  Whistle While You Tw...      14\n315    Zombie Nation            Kernkraft 400       2\n316  matchbox twenty                     Bent      39\n\n[317 rows x 3 columns]\n\n\n6.2 곡별로 최상위 순위를 min()을 이용해 구한 데이터를\n\nhighs = (\n    billboard_long\n    .groupby([\"artist\", \"track\"])[\"rank\"]\n    .min()\n    .reset_index(name=\"high\")\n)\nhighs\n\n              artist                    track  high\n0              2 Pac  Baby Don't Cry (Keep... 72.00\n1            2Ge+her  The Hardest Part Of ... 87.00\n2       3 Doors Down               Kryptonite  3.00\n..               ...                      ...   ...\n314  Ying Yang Twins  Whistle While You Tw... 74.00\n315    Zombie Nation            Kernkraft 400 99.00\n316  matchbox twenty                     Bent  1.00\n\n[317 rows x 3 columns]\n\n\n6.3 merge()를 이용해 합친 후\n\nlengths_highs = pd.merge(lengths, highs)\nlengths_highs\n\n              artist                    track  length  high\n0              2 Pac  Baby Don't Cry (Keep...       7 72.00\n1            2Ge+her  The Hardest Part Of ...       3 87.00\n2       3 Doors Down               Kryptonite      53  3.00\n..               ...                      ...     ...   ...\n314  Ying Yang Twins  Whistle While You Tw...      14 74.00\n315    Zombie Nation            Kernkraft 400       2 99.00\n316  matchbox twenty                     Bent      39  1.00\n\n[317 rows x 4 columns]\n\n\n6.4 seaborn.objects를 이용해 머문 기간에 따른 최상위 순위에 대한 관계를 아래와 같이 scatterplot으로 살펴보세요.\n\n(\n    so.Plot(lengths_highs, x=\"length\", y=\"high\")\n    .add(so.Dot(color=\"deepskyblue\", edgecolor=\"white\"))\n    .add(so.Line(), so.PolyFit(5))\n    .label(x=\"Length (wks)\", y=\"Highest Ranking\")\n)\n\n\n\n\n\n\n\n\n\n순위 1위를 달성한 곡들에 한해, 다음과 같이 차트에 진입시 순위와 1위에 처음 도달한 주(week)의 정보가 다음과 같이 표시되도록 구해보세요.\n\n함수를 만들고; min()과 argmin()이 필요할 수 있음\napply()로 그 함수를 적용하여 구해보세요.\n\n\n\ndef isranked(df, n=1):\n    if df[\"rank\"].min() == n:\n        idx_rank = df[\"rank\"].argmin()\n        idx_wk = df[\"wk\"].argmin()\n        return df.iloc[[idx_wk, idx_rank], :] \n\nwk_rank = (\n    billboard_long.groupby([\"artist\", \"track\"])[[\"rank\", \"wk\"]]\n    .apply(isranked)\n    # .reset_index(level=[0, 1])\n    # .reset_index().drop(columns=\"level_2\")\n)\nwk_rank\n\n                                                  rank  wk\nartist              track                                 \nAaliyah             Try Again               8    59.00   1\n                                            4129  1.00  14\nAguilera, Christina Come On Over Baby (A... 11   57.00   1\n...                                                ...  ..\nVertical Horizon    Everything You Want     8225  1.00  26\nmatchbox twenty     Bent                    316  60.00   1\n                                            4120  1.00  13\n\n[34 rows x 2 columns]\n\n\n\n# groupby filtering을 이용한 방법\n(\n    billboard_long.groupby([\"artist\", \"track\"])\n    .filter(lambda x: x[\"rank\"].min() == 1)  # groupby filtering\n    .groupby([\"artist\", \"track\"])[[\"rank\", \"wk\"]]\n    .apply(lambda x: x.iloc[[x[\"wk\"].argmin(), x[\"rank\"].argmin()], -2:])\n)\n\n                                                  rank  wk\nartist              track                                 \nAaliyah             Try Again               8    59.00   1\n                                            4129  1.00  14\nAguilera, Christina Come On Over Baby (A... 11   57.00   1\n...                                                ...  ..\nVertical Horizon    Everything You Want     8225  1.00  26\nmatchbox twenty     Bent                    316  60.00   1\n                                            4120  1.00  13\n\n[34 rows x 2 columns]\n\n\n\n빠르게 1위가 된 곡일 수록 빠르게 차트에서 사라졌을까를 알아보기 위해, 7번의 결과를 이용해 다음과 같이 변형해보세요.\n\n즉, 차트 진입시의 순위 정보와, 1위가 된 week의 정보만을 취해, 그 비율(rate)를 구하면, 얼마나 빠르게 1위가 되었는지 알 수 있습니다.\n\n\n\nwk_rank.groupby([\"artist\", \"track\"]).max()\n\n                                             rank  wk\nartist              track                            \nAaliyah             Try Again               59.00  14\nAguilera, Christina Come On Over Baby (A... 57.00  11\n                    What A Girl Wants       71.00   8\n...                                           ...  ..\nSisqo               Incomplete              77.00   8\nVertical Horizon    Everything You Want     70.00  26\nmatchbox twenty     Bent                    60.00  13\n\n[17 rows x 2 columns]\n\n\n\n# 한 주에 몇 위 변동했는지 계산\nrates = (\n    wk_rank.groupby([\"artist\", \"track\"])\n    .max()\n    .assign(rate=lambda x: x[\"rank\"] / x[\"wk\"])\n    .reset_index()\n)\nrates\n\n                 artist                    track  rank  wk  rate\n0               Aaliyah                Try Again 59.00  14  4.21\n1   Aguilera, Christina  Come On Over Baby (A... 57.00  11  5.18\n2   Aguilera, Christina        What A Girl Wants 71.00   8  8.88\n..                  ...                      ...   ...  ..   ...\n14                Sisqo               Incomplete 77.00   8  9.62\n15     Vertical Horizon      Everything You Want 70.00  26  2.69\n16      matchbox twenty                     Bent 60.00  13  4.62\n\n[17 rows x 5 columns]\n\n\n\n# 차트에 머무른 기간(주)\nlengths = billboard_long.groupby([\"artist\", \"track\"]).size().reset_index(name=\"length\")\nlengths\n\n              artist                    track  length\n0              2 Pac  Baby Don't Cry (Keep...       7\n1            2Ge+her  The Hardest Part Of ...       3\n2       3 Doors Down               Kryptonite      53\n..               ...                      ...     ...\n314  Ying Yang Twins  Whistle While You Tw...      14\n315    Zombie Nation            Kernkraft 400       2\n316  matchbox twenty                     Bent      39\n\n[317 rows x 3 columns]\n\n\n\nlengths_rates = pd.merge(rates, lengths)\nlengths_rates\n\n                 artist                    track  rank  wk  rate  length\n0               Aaliyah                Try Again 59.00  14  4.21      32\n1   Aguilera, Christina  Come On Over Baby (A... 57.00  11  5.18      21\n2   Aguilera, Christina        What A Girl Wants 71.00   8  8.88      24\n..                  ...                      ...   ...  ..   ...     ...\n14                Sisqo               Incomplete 77.00   8  9.62      26\n15     Vertical Horizon      Everything You Want 70.00  26  2.69      41\n16      matchbox twenty                     Bent 60.00  13  4.62      39\n\n[17 rows x 6 columns]\n\n\n\n(\n    so.Plot(lengths_rates, x=\"rate\", y=\"length\")\n    .add(so.Dot(color=\"deepskyblue\"))\n    .add(so.Line(), so.PolyFit(2))\n    .label(x=\"Rate (rank per week )\", y=\"Length (wks)\")\n)",
    "crumbs": [
      "Solutions",
      "Billboard"
    ]
  },
  {
    "objectID": "contents/Exercises/movielens_sol.html",
    "href": "contents/Exercises/movielens_sol.html",
    "title": "Movielens Solutions",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 8  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Solutions",
      "MovieLens"
    ]
  },
  {
    "objectID": "contents/Exercises/movielens_sol.html#movielens-1m-dataset",
    "href": "contents/Exercises/movielens_sol.html#movielens-1m-dataset",
    "title": "Movielens Solutions",
    "section": "MovieLens 1M Dataset",
    "text": "MovieLens 1M Dataset\nSource: MovieLens 1M movie ratings\nMcKinney’s: 13. Data Analysis Examples\n1990년대 후반에서 2000년대 초반의 영화 평가에 대한 3개의 relational data로 이루어져 있고,\nuser_id, movie_id의 keys로 연결되어 있습니다.\n\nusers: 유저에 대한 정보\nratings: 평점에 대한 정보\nmovies: 영화에 대한 정보\n\n\n\n\nratings과 users 데이터를 merge한 후 user_rating 데이터셋을 만드세요.\n\n\nuser_rating = pd.merge(users, ratings)\nuser_rating\n\n         user_id gender  age  occupation    zip  movie_id  rating  timestamp\n0              1      F    1          10  48067      1193       5  978300760\n1              1      F    1          10  48067       661       3  978302109\n2              1      F    1          10  48067       914       3  978301968\n3              1      F    1          10  48067      3408       4  978300275\n...          ...    ...  ...         ...    ...       ...     ...        ...\n1000205     6040      M   25           6  11106      1094       5  956704887\n1000206     6040      M   25           6  11106       562       5  956704746\n1000207     6040      M   25           6  11106      1096       4  956715648\n1000208     6040      M   25           6  11106      1097       4  956715569\n\n[1000209 rows x 8 columns]\n\n\n\n다음과 같이 영화(movie_id)별로 남녀(gender)에 따른 rating의 평균과 그 개수(count)을 구해보세요.\n\n\nmean_ratings = (\n    user_rating\n    .groupby([\"movie_id\", \"gender\"])[\"rating\"]\n    .agg([\"mean\", \"count\"])\n    .reset_index()\n)\nmean_ratings.head(6)\n\n   movie_id gender  mean  count\n0         1      F  4.19    591\n1         1      M  4.13   1486\n2         2      F  3.28    176\n3         2      M  3.18    525\n4         3      F  3.07    136\n5         3      M  2.99    342\n\n\n\n다음과 같이 평가의 수가 적을수록 그 편차가 커지는 현상을 확인해보세요. 평가의 수가 많은 영화일수록 평가가 높아지는 현상에 대해 설명할 수 있을까요? 또한, 남녀의 평가에 차이가 벌어지는 현상을 설명할 수 있을까요?\n\n\n(\n    so.Plot(mean_ratings, x=\"count\", y=\"mean\", color=\"gender\")\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n    .scale(color=\"Dark2\")\n)\n\n\n\n\n\n\n\n\n유명한/잘 만든 영화를 더 많이 봐서? 좋게 본 영화는 더 평점을 내리는 경향이 있어서?\n\n# 남녀 총 평점 수를 고려해서 비율로 보면\nmean_ratings[\"total\"] = mean_ratings.groupby(\"gender\")[\"count\"].transform(\"sum\")\nmean_ratings = mean_ratings.assign(\n    prop = lambda x: x[\"count\"] / x[\"total\"]\n)\n\n\nmean_ratings\n\n      movie_id gender  mean  count   total  prop\n0            1      F  4.19    591  246440  0.00\n1            1      M  4.13   1486  753769  0.00\n2            2      F  3.28    176  246440  0.00\n3            2      M  3.18    525  753769  0.00\n...        ...    ...   ...    ...     ...   ...\n7148      3951      F  3.71     17  246440  0.00\n7149      3951      M  4.04     23  753769  0.00\n7150      3952      F  3.76    105  246440  0.00\n7151      3952      M  3.79    283  753769  0.00\n\n[7152 rows x 6 columns]\n\n\n\n(\n    so.Plot(mean_ratings, x=\"prop\", y=\"mean\", color=\"gender\")\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n    # .scale(color=\"Dark2\", x=\"log\") # log scale\n    .limit(y=(2, 4.5))\n)\n\n\n\n\n\n\n\n\n\n3번에서 seaborn.objects에 .limit(x=(0, 500), y=(2.5, 4.5))을 추가하여 평가 개수(count)가 0에서 500사이이고, 평균 rating이 2.5에서 4.5 사이인 것으로 확대해서 보고, 평가 개수가 몇 개 정도부터 남녀의 평가의 차이가 대략 일정하게 되는지 살펴보세요.\n\n\n(\n    so.Plot(mean_ratings, x=\"count\", y=\"mean\", color=\"gender\")\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n    .scale(color=\"Dark2\")\n    .limit(x=(0, 500), y=(2.5, 4.5))\n)\n\n\n\n\n\n\n\n\n\n영화별로 남녀의 평가가 크게 갈리는 영화들을 찾기 위해\n\n5.1 2번에서 구한 데이터에서 남녀 모두 rating이 300개 이상 있는 영화로만 간추려보세요.\n\n5.2 이 데이터를 popular_movies라고 명명하고,\n\n이 데이터를 gender에 관해 wide format으로 변환한 후; pivot()을 이용\n\n여자의 평균 rating에서 남자의 평균 rating을 뺀 그 차이를 데이터에 추가한 후; assign()을 이용\n그 차이로 sort한 후,\n\n5.3 여자의 선호가 더 높은 영화 5편과 남자의 선호가 더 높은 영화 5편 (선호 차이의 크기 순서로)을 구해보세요.\n\n이를 위해서 movies 테이블 안의 영화제목(title)을 merge()를 이용해 추가하세요.\n\n\n\n5.1 2번에서 구한 데이터에서 남녀 모두 rating이 300개 이상 있는 영화로만 간추려보세요.\n\nmean_ratings\n\n      movie_id gender  mean  count   total  prop\n0            1      F  4.19    591  246440  0.00\n1            1      M  4.13   1486  753769  0.00\n2            2      F  3.28    176  246440  0.00\n3            2      M  3.18    525  753769  0.00\n...        ...    ...   ...    ...     ...   ...\n7148      3951      F  3.71     17  246440  0.00\n7149      3951      M  4.04     23  753769  0.00\n7150      3952      F  3.76    105  246440  0.00\n7151      3952      M  3.79    283  753769  0.00\n\n[7152 rows x 6 columns]\n\n\n\n# groupby .filter()를 이용\npopular_movies = (\n    mean_ratings\n    .groupby(\"movie_id\", group_keys=False)\n    .filter(lambda x: (x[\"count\"] &gt;= 300).all())  # groupby filtering, 남녀 모두 300개 이상이어야 하므로 .all()\n)\npopular_movies\n\n      movie_id gender  mean  count   total  prop\n0            1      F  4.19    591  246440  0.00\n1            1      M  4.13   1486  753769  0.00\n20          11      F  3.92    379  246440  0.00\n21          11      M  3.72    654  753769  0.00\n...        ...    ...   ...    ...     ...   ...\n6451      3578      F  4.09    385  246440  0.00\n6452      3578      M  4.11   1539  753769  0.00\n6773      3751      F  3.89    367  246440  0.00\n6774      3751      M  3.88    962  753769  0.00\n\n[262 rows x 6 columns]\n\n\n\n# 또는\npopular_movies = (\n    mean_ratings\n    .query('count &gt;= 300')\n    .groupby(\"movie_id\")\n    .filter(lambda x: x.shape[0] == 2)  # df.shape: (row 개수, col 개수)\n)\npopular_movies.head(6)\n\n    movie_id gender  mean  count   total  prop\n0          1      F  4.19    591  246440  0.00\n1          1      M  4.13   1486  753769  0.00\n20        11      F  3.92    379  246440  0.00\n21        11      M  3.72    654  753769  0.00\n32        17      F  4.23    420  246440  0.00\n33        17      M  3.82    415  753769  0.00\n\n\n5.2 이 데이터를 popular_movies라고 명명하고,\n이 데이터를 gender에 관해 wide format으로 변환한 후; pivot()을 이용\n여자의 평균 rating에서 남자의 평균 rating의 차이를 구해 데이터에 추가한 후; assign()을 이용\n\ndiffs_fm = (\n    popular_movies\n    .pivot(index=\"movie_id\", columns=\"gender\", values=\"mean\")\n    .assign(diffs = lambda x: x.F - x.M)\n    .sort_values(\"diffs\")\n)\ndiffs_fm\n\ngender      F    M  diffs\nmovie_id                 \n2791     3.66 4.06  -0.41\n1221     4.04 4.44  -0.40\n589      3.79 4.12  -0.33\n1214     3.89 4.22  -0.33\n...       ...  ...    ...\n17       4.23 3.82   0.41\n920      4.27 3.83   0.44\n1028     4.20 3.73   0.47\n2657     3.67 3.16   0.51\n\n[131 rows x 3 columns]\n\n\n5.3 여자의 선호가 더 큰 영화 5편과 남자의 선호가 더 큰 영화 5편 (선호 차이의 크기 순서로)을 구해보세요. 이를 위해서 movies 테이블에서 영화제목을 merge()를 이용해 구하세요.\n\ndiffs_fm = diffs_fm.merge(\n    movies[[\"movie_id\", \"title\"]], left_index=True, right_on=\"movie_id\"\n)  # left_index: index를 키로 사용할 때; diffs_fm의 movie_id가 index!\n\n\ndiffs_fm.head(5)\n\n        F    M  diffs  movie_id                              title\n2722 3.66 4.06  -0.41      2791                   Airplane! (1980)\n1203 4.04 4.44  -0.40      1221     Godfather: Part II, The (1974)\n585  3.79 4.12  -0.33       589  Terminator 2: Judgment Day (1991)\n1196 3.89 4.22  -0.33      1214                       Alien (1979)\n1220 3.90 4.21  -0.31      1240             Terminator, The (1984)\n\n\n\ndiffs_fm.tail(5)\n\n        F    M  diffs  movie_id                                  title\n1171 4.17 3.77   0.41      1188               Strictly Ballroom (1992)\n16   4.23 3.82   0.41        17           Sense and Sensibility (1995)\n908  4.27 3.83   0.44       920              Gone with the Wind (1939)\n1015 4.20 3.73   0.47      1028                    Mary Poppins (1964)\n2588 3.67 3.16   0.51      2657  Rocky Horror Picture Show, The (1975)\n\n\n\n3번에서 플랏에서 유추되듯이 평가의 개수가 영화의 완성도 혹은 인기도를 파악할 수 있는 대략적인 지표가 될 수 있습니다. 즉, 평가수가 많을 수록 평점도 높습니다. 따라서 평가 개수를 바탕으로 인기도(popularity)를 수치화 하려고 합니다.\n\n6.1 우선, 3번 플랏에서 평가수가 같다면 여성이 더 높은 평점을 주는 것으로 보이는데, 이 현상을 다음과 같이 자세히 들여다 봅니다.\n\n다음 함수를 성별로 grouping된 user_rating 데이터에 apply() 하세요.\ndef popular(g):\n    g[\"popularity\"] = pd.qcut(g[\"count\"], q=20, labels=False)\n    return g\n\n이 함수의 의미를 파악하고, 20단계 (0, 1, 2, …, 19)의 popularity가 성별을 고려하여 각 영화에 부여되었음을 이해합니다.\n이제, 다음과 같이 popularity에 따라 평점이 높아지는 현상을 성별을 고려한 후 본 결과를 플랏을 통해 확인해봅니다.\n\n\n\ndef popular(g):\n    g[\"popularity\"] = pd.qcut(g[\"count\"], q=20, labels=False)\n    return g\n\npopularity = mean_ratings.groupby(\"gender\", group_keys=False).apply(popular)\npopularity\n\n/var/folders/tv/fwb_421x50z8bj5v37vw680r0000gn/T/ipykernel_3161/3304174707.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  popularity = mean_ratings.groupby(\"gender\", group_keys=False).apply(popular)\n\n\n      movie_id gender  mean  count   total  prop  popularity\n0            1      F  4.19    591  246440  0.00          19\n1            1      M  4.13   1486  753769  0.00          19\n2            2      F  3.28    176  246440  0.00          17\n3            2      M  3.18    525  753769  0.00          17\n...        ...    ...   ...    ...     ...   ...         ...\n7148      3951      F  3.71     17  246440  0.00           6\n7149      3951      M  4.04     23  753769  0.00           4\n7150      3952      F  3.76    105  246440  0.00          15\n7151      3952      M  3.79    283  753769  0.00          15\n\n[7152 rows x 7 columns]\n\n\n\n(\n    so.Plot(popularity, x=\"popularity\", y=\"mean\", color=\"gender\")\n    .add(so.Dots(alpha=0.01), so.Jitter(width=.5))\n    .add(so.Line(), so.PolyFit(5))\n    .scale(x=so.Continuous().tick(at=np.arange(20)))  # tick: x축에 표시할 값\n    .layout(size=(5, 6))\n)\n\n\n\n\n\n\n\n\n\n남녀별로 평점의 편차가 큰, 즉 의견이 분분한 영화들을 구해봅니다.\n\n5번에서 구한 popular_movies에 한해 남녀별 평점의 편차를 표준편차로 구해보고,\n남녀별로 편차 상위 2개만 표시합니다. (동등한 순위 모두)\n영화제목을 movies 데이터와 merge하여 표시합니다.\n\n\n\npopular_movies\n\n      movie_id gender  mean  count   total  prop\n0            1      F  4.19    591  246440  0.00\n1            1      M  4.13   1486  753769  0.00\n20          11      F  3.92    379  246440  0.00\n21          11      M  3.72    654  753769  0.00\n...        ...    ...   ...    ...     ...   ...\n6451      3578      F  4.09    385  246440  0.00\n6452      3578      M  4.11   1539  753769  0.00\n6773      3751      F  3.89    367  246440  0.00\n6774      3751      M  3.88    962  753769  0.00\n\n[262 rows x 6 columns]\n\n\n\nratings_std = (\n    user_rating\n    .groupby([\"gender\", \"movie_id\"])[\"rating\"]\n    .agg([\"std\", \"count\"])\n)\nratings_std\n\n                 std  count\ngender movie_id            \nF      1        0.87    591\n       2        1.02    176\n       3        1.08    136\n       4        0.93     85\n...              ...    ...\nM      3949     0.92    224\n       3950     1.12     44\n       3951     1.07     23\n       3952     0.90    283\n\n[7152 rows x 2 columns]\n\n\n\ndef top(g, n=3):\n    return g.nlargest(n, \"std\", keep=\"all\")\n\nratings_top = (\n    ratings_std.query(\"count &gt; 100\")  # 100개 이상의 평점을 받은 영화만 제한\n    .groupby([\"gender\"], group_keys=False)\n    .apply(top, n=2)\n)\n\n\nratings_top\n\n                 std  count\ngender movie_id            \nF      2700     1.37    258\n       288      1.36    114\nM      1924     1.47    208\n       3864     1.35    115\n\n\n\nratings_top.reset_index().merge(movies).drop(columns=\"movie_id\")\n\n  gender  std  count                                           title  \\\n0      F 1.37    258     South Park: Bigger, Longer and Uncut (1999)   \n1      F 1.36    114                     Natural Born Killers (1994)   \n2      M 1.47    208                  Plan 9 from Outer Space (1958)   \n3      M 1.35    115  Godzilla 2000 (Gojira ni-sen mireniamu) (1999)   \n\n                    genres  \n0         Animation|Comedy  \n1          Action|Thriller  \n2            Horror|Sci-Fi  \n3  Action|Adventure|Sci-Fi  \n\n\n\n이제 초점을 유저들에게 돌려, 유저들의 특성을 고려해봅니다. 일반적으로 같은 소스(사람)에서 온 데이터는 비슷한 성향을 띄는데 이를 depenency의 문제라고 합니다. 한 가족 구성원으로부터 왔다든가 같은 학교의 학생들과 같이 구체적으로 명시하기 어렵지만 데이터 상에서 비슷한 군집을 이룹니다. 이 데이터의 경우 동일은 유저들의 특성이 존재할 수 있는데 예를 들어 후한 점수를 준다든가 같은 유저라도 어떤 장르의 영화는 매우 낮은 평점을 준다든가 하는 현상이 있을 수 있는데 이를 알아봅니다.\n\n8.1 우선 개인별(성별로 나눠)로 몇 개정도나 평점을 주었는지 분포를 살펴봅니다.\n8.2 개인별로 평점의 평균과 표준편차를 구합니다.\n\n\n8.1 우선 개인별(성별로 나눠)로 몇 개정도나 평점을 주었는지 분포를 살펴봅니다.\n\nrate_n = user_rating.groupby([\"user_id\", \"gender\"]).size().reset_index(name=\"n\")\nrate_n\n\n      user_id gender    n\n0           1      F   53\n1           2      M  129\n2           3      M   51\n3           4      M   21\n...       ...    ...  ...\n6036     6037      F  202\n6037     6038      F   20\n6038     6039      F  123\n6039     6040      M  341\n\n[6040 rows x 3 columns]\n\n\n\np = (\n    so.Plot(rate_n, x=\"n\", color=\"gender\")\n    .add(so.Line(), so.Hist())\n)\np\n\n\n\n\n\n\n\n\n\np.scale(x=\"log\")\n\n\n\n\n\n\n\n\n8.2 유저 개인별로 평점(rating)의 평균과 표준편차, 개수를 구합니다.\n\nuser_stats = (\n    user_rating\n    .groupby(\"user_id\")[\"rating\"]\n    .agg([\"mean\", \"std\", \"count\"])\n)\nuser_stats\n\n         mean  std  count\nuser_id                  \n1        4.19 0.68     53\n2        3.71 1.00    129\n3        3.90 0.98     51\n4        4.19 1.08     21\n...       ...  ...    ...\n6037     3.72 0.88    202\n6038     3.80 1.11     20\n6039     3.88 0.73    123\n6040     3.58 1.18    341\n\n[6040 rows x 3 columns]\n\n\n8.3 이 세 변수의 관계를 보기 위해, 평점 개수를 10개의 구간으로 pd.cut을 이용해 discretize해서 살펴봅니다.\n\n8.1에서 분포를 살펴보았으면, 개수를 먼저 log 스케일로 변환해서 구간으로 쪼개는 것이 유리함을 알 것입니다.\n즉, 다수는 2백개 이하의 평점을 남긴 반면, 소수의 유저들 중에는 수천개의 평점을 남긴 사람도 있습니다.\n10개 구간으로 discretized된 (log) count를 .facet을 이용해 다음과 같이 유저들 각자의 평점 평균과 표준편차의 관계를 살펴봅니다.\n이 현상을 설명할 수 있을까요?\n평균적으로 낮은 평점을 준 소위 짠 유저들이 더 비판적이고 고민끝에 평점을 준 것이라고 추측할 수 있을까요?\n\n\nuser_stats[\"lcount\"] = np.log(user_stats[\"count\"])\nuser_stats[\"lcount_cat\"] = pd.cut(user_stats[\"lcount\"], 10, labels=False)\n\n\n(\n    so.Plot(user_stats.query('mean &gt; 2 & lcount_cat &lt; 9'), x=\"mean\", y=\"std\")\n    .add(so.Dots(alpha=.3))\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"lcount_cat\", wrap=3)\n    .layout(size=(15, 12))\n)\n\n\n\n\n\n\n\n\n\n# count의 범위를 표시하기 위해\nrange_counts = (\n    user_stats.query('mean &gt; 2')\n    .groupby(\"lcount_cat\")[\"count\"].agg([\"min\", \"max\"])\n    .assign(range = lambda x: list(zip(x[\"min\"], x[\"max\"])))\n)\nrange_counts[\"range\"] = range_counts[\"range\"].astype(str)\nuser_stats[\"lcount_cat2\"] = user_stats[\"lcount_cat\"].map(range_counts[\"range\"])\n\n\n(\n    so.Plot(user_stats.query('mean &gt; 2 & lcount_cat &lt; 9'), x=\"mean\", y=\"std\")\n    .add(so.Dots(alpha=.3))\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"lcount_cat2\", wrap=3)\n    .layout(size=(15, 12))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(user_stats.query('mean &gt; 2 & lcount_cat &lt; 9'), x=\"mean\", y=\"std\")\n    .add(so.Line(), so.PolyFit(1), color=\"lcount_cat\")\n)\n\n\n\n\n\n\n\n\n\n장르별로 남녀의 선호 차이를 살펴봅니다.\n\nmovies 데이터는 genres가 | 기호로 나누어 표기되어 있어서 분석을 위해서는 가공할 필요가 있는데, 다음 코드를 이용해 long format으로 바꾸세요.\n\n\n\nmovies_long = movies.copy()\n\nmovies_long[\"genres\"] = movies_long[\"genres\"].str.split(\"|\")\nmovies_long = movies_long.explode(\"genres\")\n\n\nmovies_long_all = movies_long.merge(user_rating)\nmovies_long_all \n\n         movie_id                  title     genres  user_id gender  age  \\\n0               1       Toy Story (1995)  Animation        1      F    1   \n1               1       Toy Story (1995)  Animation        6      F   50   \n2               1       Toy Story (1995)  Animation        8      M   25   \n3               1       Toy Story (1995)  Animation        9      M   25   \n...           ...                    ...        ...      ...    ...  ...   \n2101811      3952  Contender, The (2000)   Thriller     5831      M   25   \n2101812      3952  Contender, The (2000)   Thriller     5837      M   25   \n2101813      3952  Contender, The (2000)   Thriller     5927      M   35   \n2101814      3952  Contender, The (2000)   Thriller     5998      M   18   \n\n         occupation    zip  rating   timestamp  \n0                10  48067       5   978824268  \n1                 9  55117       4   978237008  \n2                12  11413       4   978233496  \n3                17  61614       5   978225952  \n...             ...    ...     ...         ...  \n2101811           1  92120       3   986223125  \n2101812           7  60607       4  1011902656  \n2101813          14  10003       1   979852537  \n2101814           4  61820       4  1001781044  \n\n[2101815 rows x 10 columns]\n\n\n9.1 장르별로 평점의 개수를 간단히 플랏으로 살펴보는데 남녀 별로 따로 비율이 나오도록 해보고, 남녀 차이가 특히 많이 나는 장르를 살펴보세요.\n\n(\n    so.Plot(movies_long_all, y=\"genres\", color=\"gender\")  # y에 genres가 나오도록!\n    .add(so.Bar(), so.Hist(\"proportion\", common_norm=False), so.Dodge(gap=.2))  # gap: bar 사이의 간격\n)\n\n\n\n\n\n\n\n\n9.2 이번에는 장르별로 평점의 평균를 남녀별로 간단히 플랏으로 확인해보세요.\n\n(\n    so.Plot(movies_long_all, y=\"genres\", x=\"rating\", color=\"gender\")\n    .add(so.Bar(), so.Agg(), so.Dodge())\n    .limit(x=(3, 4.2))\n)\n\n\n\n\n\n\n\n\n9.3 위의 플랏에서 살펴본 평균 평점의 값을 직접 구해봅니다. 장르별, 남녀로 그룹핑을 하여 평균 평점을 다음과 같이 나오도록 구해보세요.\n\ngenre_gender_wide = (\n    movies_long_all.groupby([\"genres\", \"gender\"])[\"rating\"]\n    .mean()\n    .unstack()\n    .reset_index()\n)\ngenre_gender_wide\n\ngender      genres    F    M\n0           Action 3.49 3.49\n1        Adventure 3.51 3.47\n2        Animation 3.74 3.66\n3       Children's 3.57 3.36\n..             ...  ...  ...\n14          Sci-Fi 3.45 3.47\n15        Thriller 3.57 3.57\n16             War 3.89 3.89\n17         Western 3.55 3.66\n\n[18 rows x 3 columns]\n\n\n\n# pivot_table을 이용하면,\nmovies_long_all.pivot_table(index=\"genres\", columns=\"gender\", values=\"rating\")\n\ngender        F    M\ngenres              \nAction     3.49 3.49\nAdventure  3.51 3.47\nAnimation  3.74 3.66\nChildren's 3.57 3.36\n...         ...  ...\nSci-Fi     3.45 3.47\nThriller   3.57 3.57\nWar        3.89 3.89\nWestern    3.55 3.66\n\n[18 rows x 2 columns]\n\n\n9.4 영화별 남녀의 평균 평점의 차이(Female - Male) 순으로 정렬된 플랏을 대략적으로 다음과 같이 그려봅니다.\n\ngenre_gender_diff = genre_gender_wide.assign(\n    diff = lambda x: x.F - x.M\n).sort_values(\"diff\")\ngenre_gender_diff\n\ngender      genres    F    M  diff\n17         Western 3.55 3.66 -0.10\n9        Film-Noir 4.02 4.09 -0.07\n5            Crime 3.69 3.71 -0.02\n14          Sci-Fi 3.45 3.47 -0.02\n..             ...  ...  ...   ...\n8          Fantasy 3.51 3.43  0.09\n13         Romance 3.67 3.57  0.10\n11         Musical 3.81 3.60  0.21\n3       Children's 3.57 3.36  0.21\n\n[18 rows x 4 columns]\n\n\n\ngenre_gender_mean = (\n    movies_long_all.groupby([\"genres\", \"gender\"])[\"rating\"]\n    .mean()\n    .reset_index()\n)\ngenre_gender_mean\n\n       genres gender  rating\n0      Action      F    3.49\n1      Action      M    3.49\n2   Adventure      F    3.51\n3   Adventure      M    3.47\n..        ...    ...     ...\n32        War      F    3.89\n33        War      M    3.89\n34    Western      F    3.55\n35    Western      M    3.66\n\n[36 rows x 3 columns]\n\n\n\n(\n    so.Plot(genre_gender_mean, y=\"genres\", x=\"rating\", color=\"gender\")\n    .add(so.Bar(), so.Dodge())\n    .scale(\n        y=so.Nominal(order=genre_gender_diff[\"genres\"].values),\n    )\n    .limit(x=(3, 4.2))\n)\n\n\n\n\n\n\n\n\n\n# 평균과 표준편차를 error bar를 이용해 볼 수도 있음.\n(\n    so.Plot(movies_long_all, x=\"rating\", y=\"genres\", color=\"gender\")\n    .add(so.Range(), so.Est(\"mean\", errorbar=\"sd\"), so.Dodge())\n    .add(so.Dot(pointsize=3), so.Agg(\"mean\"), so.Dodge())\n    .scale(\n        y=so.Nominal(order=genre_gender_diff[\"genres\"].values)\n    )\n)\n\n\n\n\n\n\n\n\n\n장르별로 나이대에 따른 영화시청에 차이가 있는지 살펴봅니다.\n\n10.1 우선, 위에서 western 장르가 남녀의 평점 차이가 가장 크게 나타나 남성이 선호하는 것으로 보이는데 western 장르만 따로 떼어 나이대 별로 rating에 차이가 있는지 살펴봅니다.\n\nwestern = movies_long_all.query('genres == \"Western\"')\n\n\n(\n    so.Plot(western, x=\"age\", y=\"rating\", color=\"gender\")\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\nwestern_age_gender = western.groupby([\"age\", \"gender\"])[\"rating\"].agg([\"mean\", \"size\"]).reset_index()\nwestern_age_gender\n\n    age gender  mean  size\n0     1      F  3.60    91\n1     1      M  3.57   244\n2    18      F  3.24   503\n3    18      M  3.52  2360\n..  ...    ...   ...   ...\n10   50      F  3.83   330\n11   50      M  3.73  2090\n12   56      F  3.77   220\n13   56      M  3.80  1113\n\n[14 rows x 4 columns]\n\n\n\nwestern_age_gender[\"total\"] = western_age_gender.groupby(\"gender\")[\"size\"].transform(\"sum\")\n\n\nwestern_age_gender = western_age_gender.assign(\n    ratio = lambda x: x[\"size\"] / x[\"total\"]\n)\nwestern_age_gender\n\n    age gender  mean  size  total  ratio\n0     1      F  3.60    91   3477   0.03\n1     1      M  3.57   244  17206   0.01\n2    18      F  3.24   503   3477   0.14\n3    18      M  3.52  2360  17206   0.14\n..  ...    ...   ...   ...    ...    ...\n10   50      F  3.83   330   3477   0.09\n11   50      M  3.73  2090  17206   0.12\n12   56      F  3.77   220   3477   0.06\n13   56      M  3.80  1113  17206   0.06\n\n[14 rows x 6 columns]\n\n\n\n(\n    so.Plot(western_age_gender, x=\"age\", y=\"mean\", pointsize=\"ratio\", color=\"gender\")\n    .add(so.Dot())\n    .scale(pointsize=(5, 25))\n    .scale(x=so.Continuous().tick(at=[1, 18, 25, 35, 45, 50, 56]))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(movies_long_all, x=\"age\", y=\"rating\", color=\"gender\")\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"genres\", wrap=5)\n    .share(y=False)\n    .layout(size=(10, 8))\n    .scale(x=so.Continuous().tick(at=[1, 18, 25, 35, 45, 50, 56]))\n)\n\n\n\n\n\n\n\n\n\n# 평균 선으로 표시\n(\n    so.Plot(movies_long_all, x=\"age\", y=\"rating\", color=\"gender\")\n    .add(so.Line(marker=\".\"), so.Agg())\n    .facet(\"genres\", wrap=5)\n    .share(y=False)\n    .layout(size=(10, 8))\n    .scale(x=so.Continuous().tick(at=[1, 18, 25, 35, 45, 50, 56]))\n)\n\n\n\n\n\n\n\n\n\n영화 제목에 있는 출시년도를 추출해 이용하기 위해 다음 코드를 활용하세요.\n\n\nmovies[\"year\"] = movies[\"title\"].str.extract(r'\\((\\d{4})\\)').astype(\"int\")\nmovies\n\n      movie_id                       title                        genres  year\n0            1            Toy Story (1995)   Animation|Children's|Comedy  1995\n1            2              Jumanji (1995)  Adventure|Children's|Fantasy  1995\n2            3     Grumpier Old Men (1995)                Comedy|Romance  1995\n3            4    Waiting to Exhale (1995)                  Comedy|Drama  1995\n...        ...                         ...                           ...   ...\n3879      3949  Requiem for a Dream (2000)                         Drama  2000\n3880      3950            Tigerland (2000)                         Drama  2000\n3881      3951     Two Family House (2000)                         Drama  2000\n3882      3952       Contender, The (2000)                Drama|Thriller  2000\n\n[3883 rows x 4 columns]\n\n\n11.1 먼저 출시년도별로 얼마나 영화가 있는지 분포를 살펴보세요.\n\n(\n    so.Plot(movies, x=\"year\")\n    .add(so.Bars(), so.Hist())\n)\n\n\n\n\n\n\n\n\n11.2 출시년도가 없는 영화가 있는가요?\n\nmovies.year.value_counts(dropna=False)\n\nyear\n1996    345\n1995    342\n1998    337\n1997    315\n       ... \n1919      3\n1922      2\n1920      2\n1921      1\nName: count, Length: 81, dtype: int64\n\n\n11.3 오래된 영화일 수록 나이든 사람들의 시청 비율이 높을지에 대해 분포를 살펴보세요.\n\nyear_movies = pd.merge(user_rating[[\"user_id\", \"gender\", \"age\", \"movie_id\", \"rating\"]], movies)\nyear_movies.head(3)\n\n   user_id gender  age  movie_id  rating  \\\n0        1      F    1      1193       5   \n1        1      F    1       661       3   \n2        1      F    1       914       3   \n\n                                    title                        genres  year  \n0  One Flew Over the Cuckoo's Nest (1975)                         Drama  1975  \n1        James and the Giant Peach (1996)  Animation|Children's|Musical  1996  \n2                     My Fair Lady (1964)               Musical|Romance  1964  \n\n\n\nyear_movies = year_movies.assign(\n    age = lambda x: pd.Categorical(x.age.astype(\"string\"), categories=[\"1\", \"18\", \"25\", \"35\", \"45\", \"50\", \"56\"], ordered=True)\n)\n\n\n(\n    so.Plot(year_movies, x=\"year\", color=\"age\")\n    .add(so.Bars(), so.Hist(\"proportion\", common_norm=False, discrete=True))\n    .share(y=False)\n    .facet(\"age\", wrap=1)\n    .layout(size=(8, 12))\n)\n\n\n\n\n\n\n\n\n\n# 성별에 따라서도 나누어 보면,\n(\n    so.Plot(year_movies, x=\"year\", color=\"age\")\n    .add(so.Bars(), so.Hist(\"proportion\", common_norm=[\"color\", \"col\"], discrete=True))\n    .share(y=False)\n    .facet(row=\"age\", col=\"gender\")\n    .layout(size=(10, 12))\n)\n\n\n\n\n\n\n\n\n\n10년 기준으로 년대를 정했을 때, 코미디 장르의 영화 비율의 년대별 변화를 살펴봅니다.\n\n\nmovies_long[\"year\"] = movies_long[\"title\"].str.extract(r'\\((\\d{4})\\)').astype(\"int\")\n\nmovies_long_10 = movies_long.assign(\n    decade = lambda x: x.year // 10 * 10\n)\nmovies_long_10\n\n      movie_id                    title      genres  year  decade\n0            1         Toy Story (1995)   Animation  1995    1990\n0            1         Toy Story (1995)  Children's  1995    1990\n0            1         Toy Story (1995)      Comedy  1995    1990\n1            2           Jumanji (1995)   Adventure  1995    1990\n...        ...                      ...         ...   ...     ...\n3880      3950         Tigerland (2000)       Drama  2000    2000\n3881      3951  Two Family House (2000)       Drama  2000    2000\n3882      3952    Contender, The (2000)       Drama  2000    2000\n3882      3952    Contender, The (2000)    Thriller  2000    2000\n\n[6408 rows x 5 columns]\n\n\n\n# pd.crosstab의 비율을 이용하면,\ndecade_perc = pd.crosstab(\n    movies_long_10[\"decade\"], movies_long_10[\"genres\"], normalize=\"index\"\n).stack()\n\ndecade_perc\n\ndecade  genres    \n1910    Action       0.20\n        Adventure    0.20\n        Animation    0.00\n        Children's   0.00\n                     ... \n2000    Sci-Fi       0.04\n        Thriller     0.10\n        War          0.01\n        Western      0.00\nLength: 180, dtype: float64\n\n\n\n(\n    so.Plot(decade_perc.reset_index(name=\"perc\"), x=\"decade\", y=\"perc\", color=\"genres\")\n    .add(so.Line())\n    .facet(\"genres\", wrap=5)\n    .share(y=False)\n    .layout(size=(12, 8))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(decade_perc.reset_index(name=\"perc\").query('genres == \"Comedy\"'), x=\"decade\", y=\"perc\")\n    .add(so.Line())\n)\n\n\n\n\n\n\n\n\nWide format의 이용\n\ndf_wide = pd.crosstab(\n    movies_long_10[\"decade\"], movies_long_10[\"genres\"], normalize=\"index\"\n)\ndf_wide.head(3)\n\ngenres  Action  Adventure  Animation  Children's  Comedy  Crime  Documentary  \\\ndecade                                                                         \n1910      0.20       0.20       0.00        0.00    0.20   0.00         0.00   \n1920      0.03       0.03       0.00        0.00    0.33   0.03         0.00   \n1930      0.02       0.05       0.02        0.03    0.19   0.03         0.01   \n\ngenres  Drama  Fantasy  Film-Noir  Horror  Musical  Mystery  Romance  Sci-Fi  \\\ndecade                                                                         \n1910     0.40     0.00       0.00    0.00     0.00     0.00     0.00    0.00   \n1920     0.36     0.00       0.00    0.03     0.03     0.00     0.05    0.03   \n1930     0.21     0.00       0.01    0.06     0.09     0.04     0.12    0.01   \n\ngenres  Thriller  War  Western  \ndecade                          \n1910        0.00 0.00     0.00  \n1920        0.05 0.05     0.00  \n1930        0.07 0.04     0.01  \n\n\n\n# Plot using pandas or matplotlib\ndf_wide.plot(kind=\"line\",figsize=(10, 6), subplots=True, layout=(4, 5));\n\n\n\n\n\n\n\n\n\n# Plot using pandas or matplotlib\ndf_wide.plot(kind=\"line\",figsize=(10, 6), layout=(4, 5));",
    "crumbs": [
      "Solutions",
      "MovieLens"
    ]
  },
  {
    "objectID": "contents/Introduction/pandas.html",
    "href": "contents/Introduction/pandas.html",
    "title": "NumPy and pandas",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nNumpy & pandas\nPython 언어는 수치 계산을 위해 디자인되지 않았기 때문에, 데이터 분석에 대한 효율적이고 빠른 계산이 요구되면서 C/C++이라는 언어로 구현된 NumPy (Numerical Python)가 탄생하였고, Python 생태계 안에 통합되었음. 기본적으로 Python 언어 안에 새로운 언어라고 볼 수 있음. 데이터 사이언스에서의 대부분의 계산은 NumPy의 ndarray (n-dimensioal array)와 수학적 operator들을 통해 계산됨.\n데이터 사이언스가 발전함에 따라 단일한 floating-point number들을 성분으로하는 array들의 계산에서 벗어나 칼럼별로 다른 데이터 타입(string, integer, object..)을 포함하는 tabular 형태의 데이터를 효율적으로 처리해야 할 필요성이 나타났고, 이를 다룰 수 있는 새로운 언어를 NumPy 위에 개발한 것이 pandas임. 이는 기본적으로 Wes Mckinney에 의해 독자적으로 개발이 시작되었으며, 디자인적으로 불만족스러운 점이 지적되고는 있으나 데이터 사이언스의 기본적인 언어가 되었음.\nNumPy와 pandas에 대한 자세한 내용은 Python for Data Analysis by Wes MacKinney 참고\n특히, NumPy는 Ch.4 & appendices",
    "crumbs": [
      "Introduction",
      "NumPy and pandas"
    ]
  },
  {
    "objectID": "contents/Introduction/pandas.html#numpy",
    "href": "contents/Introduction/pandas.html#numpy",
    "title": "NumPy and pandas",
    "section": "NumPy",
    "text": "NumPy\n\n수학적 symbolic 연산에 대한 구현이라고 볼 수 있으며,\n행렬(matrix) 또는 벡터(vector)를 ndarray (n-dimensional array)이라는 이름으로 구현함.\n\n사실상 정수(int)나 실수(float)의 한가지 타입으로 이루어짐.\n\n고차원의 arrays 가능\n\nSource: Medium.com\n\n가령, 다음과 같은 행렬 연산이 있다면,\n\\(\\begin{bmatrix}1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} \\begin{bmatrix}2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix}0 \\\\ 2 \\\\ 4 \\end{bmatrix}\\)\n\n\nA = np.array([[1, 2],\n              [3, 4],\n              [5, 6]]) # 3x2 matrix\nX = np.array([[2],\n              [-1]]) # 2x1 matrix\n\nA @ X  # A * X : matrix multiplication\n\narray([[0],\n       [2],\n       [4]])\n\n\n\nA.dot(X)\n\narray([[0],\n       [2],\n       [4]])\n\n\nVector vs. Matrix\n\nprint(np.array([0, 2, 4])) # 1-dim matrix: vector\nprint(np.array([0, 2, 4]).reshape(3, 1)) # 3x1 matrix\n\n[0 2 4]\n[[0]\n [2]\n [4]]\n\n\n\narr = np.array([0, 2, 4])\narr.reshape(3, -1).T\n\narray([[0, 2, 4]])\n\n\n\nX2 = np.array([2, -1])\nA @ X2  # same as A.dot(X2)\n\narray([0, 2, 4])\n\n\n\nprint(A.shape)\nprint(A.ndim)\nprint(A.dtype)\n\n(3, 2)\n2\nint64\n\n\n\nA + A # element-wise addition\n\narray([[ 2,  4],\n       [ 6,  8],\n       [10, 12]])\n\n\n\n2 * A - 1 # braodcasting\n\narray([[ 1,  3],\n       [ 5,  7],\n       [ 9, 11]])\n\n\n\nnp.exp(A) # element-wise\n\narray([[  2.72,   7.39],\n       [ 20.09,  54.6 ],\n       [148.41, 403.43]])\n\n\n\nPython vs. NumPy\n\na = 2**31 - 1\nprint(a)\nprint(a + 1)\n\n2147483647\n2147483648\n\n\n\na = np.array([2**31 - 1], dtype='int32')\nprint(a)\nprint(a + 1)\n\n[2147483647]\n[-2147483648]\n\n\n\nSource: Ch.4 in Python for Data Analysis (3e) by Wes McKinney",
    "crumbs": [
      "Introduction",
      "NumPy and pandas"
    ]
  },
  {
    "objectID": "contents/Introduction/pandas.html#pandas",
    "href": "contents/Introduction/pandas.html#pandas",
    "title": "NumPy and pandas",
    "section": "pandas",
    "text": "pandas\nSeries & DataFrame\n\nSeries\n1개의 칼럼으로 이루어진 데이터 포멧: 1d numpy array에 labels을 부여한 것으로 볼 수 있음.\nDataFrame의 각 칼럼들을 Series로 이해할 수 있음.\n\nSource: Practical Data Science\n\n\nDataFrame\n각 칼럼들이 한 가지 데이터 타입으로 이루어진 tabular형태 (2차원)의 데이터 포맷\n\n각 칼럼은 기본적으로 한 가지 데이터 타입인 것이 이상적이나, 다른 타입이 섞여 있을 수 있음\nNumPy의 2차원 array의 각 칼럼에 labels을 부여한 것으로 볼 수도 있으나, 여러 다른 기능들이 추가됨\nNumPy의 경우 고차원의 array를 다룰 수 있음: ndarray\n\n고차원의 DataFrame과 비슷한 것은 xarray가 존재\n\nLabels와 index를 제외한 데이터 값은 거의 NumPy ndarray로 볼 수 있음\n(pandas.array 존재)\n\n\nSource: Practical Data Science\n\n\nndarray &lt;&gt; DataFrame\n\ndf = pd.DataFrame(A, columns=[\"A1\", \"A2\"])\ndf\n\n   A1  A2\n0   1   2\n1   3   4\n2   5   6\n\n\n\n# 데이터 값들은 NumPy array\ndf.values # 또는 df.to_numpy()\n\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\n\nColumns\nSeries로 추출\n\ns = df[\"A1\"] # A1 칼럼 선택\ns\n# DataFrame의 column 이름이 Series의 name으로 전환\n\n0    1\n1    3\n2    5\nName: A1, dtype: int64\n\n\n\ntype(s)\n\npandas.core.series.Series\n\n\n\ns.values # Series의 값은 1d array\n\narray([1, 3, 5])\n\n\n\n\n\n\n\n\nA DataFrame with a single column\n\n\n\n\n\ndf[[\"A1\"]] # double brackets\n\n\n\n\n\n\n\n\nIndex objects\nframe = pd.DataFrame(np.arange(6).reshape((2, 3)),\n                     index=pd.Index([\"Ohio\", \"Colorado\"], name=\"state\"),\n                     columns=pd.Index([\"one\", \"two\", \"three\"], name=\"number\"))\nframe\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\n\n\nframe.index\n\nIndex(['Ohio', 'Colorado'], dtype='object', name='state')\n\n\n\nframe.columns # columns도 index object\n\nIndex(['one', 'two', 'three'], dtype='object', name='number')\n\n\n\n\n\n\n\n\nNote\n\n\n\n“number”: columns의 이름\n“state”: index의 이름\nframe.columns.name #&gt; ‘number’\nframe.index.name #&gt; ‘state’\n\n\n\nMulti-Index object\nIndex는 여러 levels을 지닐 수 있음\n\nframe.stack() # stack()은 long form으로 변환\n# 2 levels의 index를 가진 Series\n\nstate     number\nOhio      one       0\n          two       1\n          three     2\nColorado  one       3\n          two       4\n          three     5\ndtype: int64\n\n\n\n# MultiIndex를 직접 구성\npd.DataFrame(np.arange(12).reshape((4, 3)),\n        index=pd.MultiIndex.from_arrays([[\"a\", \"a\", \"b\", \"b\"], [1, 2, 1, 2]], names=[\"idx1\", \"idx2\"]),\n        columns=pd.MultiIndex.from_arrays([[\"Ohio\", \"Ohio\", \"Colorado\"], [\"Green\", \"Red\", \"Green\"]], names=[\"state\", \"color\"]))\n\nstate      Ohio     Colorado\ncolor     Green Red    Green\nidx1 idx2                   \na    1        0   1        2\n     2        3   4        5\nb    1        6   7        8\n     2        9  10       11\n\n\n\n\nTime Series\nIndex는 times series에 특화\n파일 다운로드: Bike Sharing in Washington D.C. Dataset\n\nbike = pd.read_csv('../data/day.csv', index_col='dteday', parse_dates=True)\nbike.head(3)\n\n            instant  season  yr  mnth  holiday  weekday  workingday  \\\ndteday                                                                \n2011-01-01        1       1   0     1        0        6           0   \n2011-01-02        2       1   0     1        0        0           0   \n2011-01-03        3       1   0     1        0        1           1   \n\n            weathersit  temp  atemp  hum  windspeed  casual  registered   cnt  \ndteday                                                                         \n2011-01-01           2  0.34   0.36 0.81       0.16     331         654   985  \n2011-01-02           2  0.36   0.35 0.70       0.25     131         670   801  \n2011-01-03           1  0.20   0.19 0.44       0.25     120        1229  1349  \n\n\n\nbike.plot(kind='line', y=['casual', 'registered'], figsize=(7, 4), title='Bike Sharing')\nplt.show()\n\n\n\n\n\n\n\n\nindex없이 분석 가능?\nIndex를 column으로 전환시켜 분석할 수 있음: .reset_index()\n\nbike.reset_index()\n\n        dteday  instant  season  yr  mnth  holiday  weekday  workingday  \\\n0   2011-01-01        1       1   0     1        0        6           0   \n1   2011-01-02        2       1   0     1        0        0           0   \n2   2011-01-03        3       1   0     1        0        1           1   \n..         ...      ...     ...  ..   ...      ...      ...         ...   \n728 2012-12-29      729       1   1    12        0        6           0   \n729 2012-12-30      730       1   1    12        0        0           0   \n730 2012-12-31      731       1   1    12        0        1           1   \n\n     weathersit  temp  atemp  hum  windspeed  casual  registered   cnt  \n0             2  0.34   0.36 0.81       0.16     331         654   985  \n1             2  0.36   0.35 0.70       0.25     131         670   801  \n2             1  0.20   0.19 0.44       0.25     120        1229  1349  \n..          ...   ...    ...  ...        ...     ...         ...   ...  \n728           2  0.25   0.24 0.75       0.12     159        1182  1341  \n729           1  0.26   0.23 0.48       0.35     364        1432  1796  \n730           2  0.22   0.22 0.58       0.15     439        2290  2729  \n\n[731 rows x 16 columns]\n\n\n반대로 column을 index로 전환: .set_index(\"column\")\n\nbike.reset_index().set_index(\"dteday\")\n\n            instant  season  yr  mnth  holiday  weekday  workingday  \\\ndteday                                                                \n2011-01-01        1       1   0     1        0        6           0   \n2011-01-02        2       1   0     1        0        0           0   \n2011-01-03        3       1   0     1        0        1           1   \n...             ...     ...  ..   ...      ...      ...         ...   \n2012-12-29      729       1   1    12        0        6           0   \n2012-12-30      730       1   1    12        0        0           0   \n2012-12-31      731       1   1    12        0        1           1   \n\n            weathersit  temp  atemp  hum  windspeed  casual  registered   cnt  \ndteday                                                                         \n2011-01-01           2  0.34   0.36 0.81       0.16     331         654   985  \n2011-01-02           2  0.36   0.35 0.70       0.25     131         670   801  \n2011-01-03           1  0.20   0.19 0.44       0.25     120        1229  1349  \n...                ...   ...    ...  ...        ...     ...         ...   ...  \n2012-12-29           2  0.25   0.24 0.75       0.12     159        1182  1341  \n2012-12-30           1  0.26   0.23 0.48       0.35     364        1432  1796  \n2012-12-31           2  0.22   0.22 0.58       0.15     439        2290  2729  \n\n[731 rows x 15 columns]\n\n\n\n\n\n\nDataFrame의 연산\nNumPy의 ndarray들이 연산되는 방식과 동일하게 series나 DataFrame들의 연산 가능함\n\ndf + 2 * df\n\n   A1  A2\n0   3   6\n1   9  12\n2  15  18\n\n\n\nnp.log(df)\n\n    A1   A2\n0 0.00 0.69\n1 1.10 1.39\n2 1.61 1.79\n\n\n사실 연산은 index를 align해서 시행됨\n\n\n\n\n\n\nnumber    one  two  three\nstate                    \nOhio        0    1      2\nColorado    3    4      5\n\n\nnumber  one  two  three\nstate                  \nOhio      0    2      4\nFloria    6    8     10\n\n\n\nframe1 + frame2\n\n\n\nnumber    one  two  three\nstate                    \nColorado  NaN  NaN    NaN\nFloria    NaN  NaN    NaN\nOhio     0.00 3.00   6.00\n\n\n\n\n(참고) Mixed Data Type\n\ns = pd.Series([1, 2, \"3\"])\n\n\ns.dtype\n\ndtype('O')\n\n\n\ns + s\n\n0     2\n1     4\n2    33\ndtype: object\n\n\n\ns_int = s.astype(\"int\")\ns_int + s_int\n\n0    2\n1    4\n2    6\ndtype: int64\n\n\n\ns2 = pd.Series([1, 2, 3.1])\ns2.dtype\n\ndtype('float64')\n\n\n\ns2.astype(\"int\")\n\n0    1\n1    2\n2    3\ndtype: int64",
    "crumbs": [
      "Introduction",
      "NumPy and pandas"
    ]
  },
  {
    "objectID": "contents/Introduction/pandas.html#missing",
    "href": "contents/Introduction/pandas.html#missing",
    "title": "NumPy and pandas",
    "section": "Missing",
    "text": "Missing\nNaN, NA, None\n\npandas에서는 missing을 명명하는데 R의 컨벤션을 따라 NA (not available)라 부름.\n\n대부분의 경우에서 NumPy object NaN(np.nan)을 NA을 나타내는데 사용됨.\n\nnp.nan은 실제로 floating-point의 특정 값으로 float64 데이터 타입임. Integer 또는 string type에서 약간 이상하게 작동될 수 있음.\n\nPython object인 None은 pandas에서 NA로 인식함.\n\n현재 NA라는 새로운 pandas object 실험 중임\n\nNA의 handling에 대해서는 교재 참고\n.dropna(), .fillna(), .isna(), .notna()\n\nMckinney’s: 7.1 Handling Missing Data,\nWorking with missing data\n\n\ns = pd.Series([1, 2, np.nan])\ns\n\n0   1.00\n1   2.00\n2    NaN\ndtype: float64\n\n\n\n# type을 변환: float -&gt; int\ns.astype(\"Int64\")\n\n0       1\n1       2\n2    &lt;NA&gt;\ndtype: Int64\n\n\n\ns = pd.Series([\"a\", \"b\", np.nan])\ns\n\n0      a\n1      b\n2    NaN\ndtype: object\n\n\n\n# type을 변환: object -&gt; string\ns.astype(\"string\")\n\n0       a\n1       b\n2    &lt;NA&gt;\ndtype: string\n\n\n\ns = pd.Series([1, 2, np.nan, None, pd.NA])\ns\n\n0       1\n1       2\n2     NaN\n3    None\n4    &lt;NA&gt;\ndtype: object\n\n\nMissing인지를 확인: .isna(), .notna()\n\ns.isna() # or s.isnull()\n\n0    False\n1    False\n2     True\n3     True\n4     True\ndtype: bool\n\n\n\ns.notna() # or s.notnull()\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\npandas에서는 ExtensionDtype이라는 새로운 데이터 타입이 도입되었음.\n\ns2 = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\ns2.dtype  # date type 확인\n\nInt8Dtype()\n\n\n\nimport pyarrow as pa\ns2 = pd.Series([2, pd.NA], dtype=pd.ArrowDtype(pa.uint16()))\ns2.dtype\n\nuint16[pyarrow]\n\n\npandas dtypes 참고\n\n\n\n\n\n\nNote\n\n\n\nPython object인 None의 경우\nNone == None\n#&gt; True\nNumPy object인 np.nan의 경우\nnp.nan == np.nan\n#&gt; False",
    "crumbs": [
      "Introduction",
      "NumPy and pandas"
    ]
  },
  {
    "objectID": "contents/Introduction/pandas.html#attributes",
    "href": "contents/Introduction/pandas.html#attributes",
    "title": "NumPy and pandas",
    "section": "Attributes",
    "text": "Attributes\n자주 사용되는 attributes;\nSeries objects: name, dtype, shape, index, values\nIndex objects: name, dtype, shape, values, is_unique\nDataFrame objects: dtype, shape, index, columns, values",
    "crumbs": [
      "Introduction",
      "NumPy and pandas"
    ]
  },
  {
    "objectID": "contents/Introduction/pandas.html#creating-dataframes",
    "href": "contents/Introduction/pandas.html#creating-dataframes",
    "title": "NumPy and pandas",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\nDataFrame을 만드는 방식에 대해서는\nMckinney’s: 5.1 Introduction to pandas Data Structures",
    "crumbs": [
      "Introduction",
      "NumPy and pandas"
    ]
  },
  {
    "objectID": "contents/Modelling/model-basic.html",
    "href": "contents/Modelling/model-basic.html",
    "title": "Model Basics",
    "section": "",
    "text": "Source: R for Data Science by Wickham & Grolemund\n\n\n모델의 목표는 데이터 세트에 대한 간단한 저차원 요약을 제공하는 것입니다. 이상적으로, 모델은 진정한 ‘신호’(즉, 관심 있는 현상에 의해 생성된 패턴)를 포착하고 ‘노이즈’(즉, 관심 없는 임의의 변동)는 무시합니다. (번역 by DeepL)\n\n\nThe goal of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in).\n\n이상적으로, 모형(model)이 현상으로부터 노이즈가 제거된 진정한 신호를 잡아내 주기를 기대.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n예를 들어, 캐럿과 가격의 진정한 관계를 모델로 표현\nLinear model: \\(Y = aX + b + \\epsilon\\),   \\(\\epsilon\\): errors\n\n\\(price = a \\cdot carat + b + \\epsilon\\)\n\n로그 변환 후 선형관계로 가정: \\(log(price) = a \\cdot log(carat) + b + \\epsilon\\)\n\n\\(\\epsilon\\): Gaussian 분포로 가정, 즉 \\(\\epsilon \\sim N(0, \\sigma^2)\\)\n\n\\(E(Y|X = X_i) = a \\cdot X_i + b\\)   (\\(E\\): expectation, 기대값)\n\n즉, \\(X = X_i\\)에 대한 conditional mean이 선형함수로 결정됨을 가정\n\n\nErrors은 노이즈?\n\nReducible error: 모형이 잡아내지 못한 신호; 영향을 미치지만 측정하지 않은 변수가 존재\nIrreducible error:\n\n측정 오차 (measurement error): ex. 성별, 젠더, 키, 온도, 강수량, 지능, 불쾌지수, …\nRandom processes\n\n물리적 세계의 불확실성: stochastic vs. deterministic world\n예를 들어, 동전을 4번 던질 때\n\n보통 Gaussian 분포를 이루거나 가정: ex. 측정 오차들, 동전 앞면의 개수들, 키, 몸무게, IQ, …\n\n그 외에 자연스럽게 나타난다고 가정할 수 있는 여러 분포들이 있음; Binomial, Poisson, Exponential, Weibull, Gamma, Beta, …\n\n\n불확실성(uncertainty)\n\n예측은 정확할 수 없으며, 이 불확실성이 error로 표현되며,\n확률의 개념으로 모형의 일부로 포함되어 예측하는데 중요한 요소로 활용됨.\n\nGaussian/Normal distribution\n\n랜덤한 값들의 합/평균들이 나타내는 분포; 중심극한정리(Central Limit Theorem)\n\n측정 오차의 분포(error distribution)\n다양한 힘들의 상호작용으로 인한 분포\n\n분산이 유한한 분포 중에 정보 엔트로피가 최대(maximum entropy)인 분포\n\\(X \\sim N(\\mu, \\sigma^2)\\); density function \\(\\displaystyle f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\\)\n\\(X \\sim N(0, 1)\\); \\(\\displaystyle f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}x^2}\\), (standard normal distribution)\n\n\n\n\n\n\n\n1000 people, each 16 steps tossing a coin: \n [[-1  1 -1 ... -1  1 -1]\n [ 1 -1 -1 ... -1  1  1]\n [-1  1 -1 ... -1  1 -1]\n ...\n [-1  1 -1 ... -1  1 -1]\n [-1  1 -1 ... -1  1 -1]\n [-1 -1  1 ... -1 -1 -1]]\n\n\n\n\n\n\n\n\n\n\n\n\nCoin tossing widget code\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(0)\ndef plot_Gaussin(n=0, ax=None):\n    N = 200\n    num_people, num_rounds = N, n\n\n    if n == 0:\n        toss = np.zeros(N).reshape(N, -1)\n    else:\n        toss = np.random.choice([-1, 1], size=(num_people, num_rounds))\n    toss_sum = toss.sum(axis=1)\n\n    ax = ax or plt.gca()\n    ax.set_title(f\"{N} people, each tossing a fair coin {n} times\")\n\n    df = pd.DataFrame({\"TossSum\": toss_sum})\n    df2 = df.value_counts().reset_index()\n    x = df2[\"TossSum\"].values.astype(int)\n    y = df2[\"count\"].values\n\n    if n &lt; 10:\n        ax.bar(x, y, color=\"#1f77b4\", alpha=.6, width=.8)\n        ax.set_xlim(-10, 10)\n        ax.set_xticks(x)\n    else:\n        ax.bar(x, y, color=\"#1f77b4\", alpha=.6)\n        ax.set_xticks(x)\n\n    for i, v in enumerate(y):\n        ax.text(x[i], v + 0.5, str(v), ha=\"center\", color=\"r\")\n\nfrom ipywidgets import interact, fixed\ninteract(plot_Gaussin,  n=(0, 100), ax=fixed(None))",
    "crumbs": [
      "Modelling",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/Modelling/model-basic.html#a-simple-model",
    "href": "contents/Modelling/model-basic.html#a-simple-model",
    "title": "Model Basics",
    "section": "A simple model",
    "text": "A simple model\nData: sim1.csv\n\nsim1 = pd.read_csv(\"data/sim1.csv\")\n\n\n\n\n\n\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n\n\n\n\n패턴: 강한 선형 관계\n선형 모델 family/class인 \\(y = \\beta_0 + \\beta_1 x\\)을 세운 후\n무수히 많은 \\(\\beta_0, \\beta_1\\)의 값들 중 위 데이터에 가장 가까운 값을 찾음\n그 예로, 임의로 250개의 선형 모델을 그려보면,\n\n\n\n\n\n\n\n\n\n\n\n이 선형모델 중 데이터에 가장 가까운 모델을 찾고자 하는데, 이를 위해서는 데이터와 모델과의 거리를 정의해야 함.\n  \\(d =|~data - model~|\\)\n예) 모델과 데이터의 수직 거리(residuals)의 총체\n\nModel 1.1: \\(y = 1.5x+7\\)의 경우, 이 모델이 예측하는 값들\n\n\narray([ 8.5,  8.5,  8.5, 10. , 10. , 10. , 11.5, 11.5, 11.5, 13. , 13. ,\n       13. , 14.5, 14.5, 14.5, 16. , 16. , 16. , 17.5, 17.5, 17.5, 19. ,\n       19. , 19. , 20.5, 20.5, 20.5, 22. , 22. , 22. ])\n\n\n이 때, 관측치(\\(Y_i\\))와 예측치(\\(\\hat{Y}_i\\))의 차이, \\(Y_i - \\hat{Y}_i\\)를 잔차(residuals) 또는 예측 오차(errors)라고 함\n\n\n     x     y  pred  resid | e\n0    1  4.20  8.50      -4.30\n1    1  7.51  8.50      -0.99\n2    1  2.13  8.50      -6.37\n..  ..   ...   ...        ...\n27  10 24.97 22.00       2.97\n28  10 23.35 22.00       1.35\n29  10 21.98 22.00      -0.02\n\n[30 rows x 4 columns]\n\n\n\n\n\n\n\n\nRMSE = \\(\\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{e^2}}\\) = 2.67\n\n\nMAE = \\(\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}|~e~|\\) = 1.43\n\n\n\n\n\n\n\n\n\nModel evaluation\n\n\n\nError functions\n\nRoot-mean-squared error: \\(RMSE = \\displaystyle\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(y_i -\\hat y_i)^2}}\\)\n\nMean absolute error: \\(MAE = \\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}{|~y_i -\\hat y_i~|}\\) : 극단값들에 덜 민감함\n\n\n\n즉, 데이터셋 sim1과 model 1.1 과의 거리를 RMSE로 정의하면, \\(d=|~sim1 -model1~| = 2.67\\)\n위의 250개의 모델에 대해 각각 거리를 구하면\n\n\n       b0    b1  dist\n0   21.79 -2.92 17.42\n1   -2.83 -0.57 22.83\n2   -6.39  2.16 10.26\n..    ...   ...   ...\n247  0.51  4.19 10.38\n248 27.94 -0.84 11.59\n249 27.93  2.45 25.99\n\n[250 rows x 3 columns]\n\n\n이 중 제일 좋은 모델(dist가 최소) 10개의 모델을 그리면,\n\n\n\n\n\n\n\n\n\n\n250개의 모델 중 10개의 모델을 다음과 같은 \\((\\beta_0, \\beta_1)\\) 평면으로 살펴보면, 즉, model space에서 살펴보면\n\n오렌지 색은 위에서 구한 10 best models\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Introduction to Statistical Learning by James et al.\n\n점차 촘촘한 간격으로 grid search를 하면서 거리를 최소로 하는 모델을 찾아가는 것이고, 실제로는 Newton-Raphson search를 통해 최소값을 구하는 알고리즘을 통해 구할 수 있음.\n즉, 거리를 최소로 하는 \\(\\beta_0\\), \\(\\beta_1\\)를 찾으면,\n\nfrom scipy.optimize import minimize\nminimize(measure_distance, [0, 0], args=(sim1)).x\n\narray([4.22, 2.05])\n\n\n\n\n\n\n\n\n\n\n\n이렇게 squared error가 최소가 되도록 추정하는 것을 ordinary least squares(OLS) estimattion라고 함.\n실제로는 위에서 처럼 grid search를 하지 않고, closed-form solution을 통해 바로 구할 수 있음.\n\nfrom statsmodels.formula.api import ols\n\nmod = ols('y ~ x', data=sim1).fit()\nmod.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.885\n\n\nModel:\nOLS\nAdj. R-squared:\n0.880\n\n\nMethod:\nLeast Squares\nF-statistic:\n214.7\n\n\nDate:\nWed, 14 May 2025\nProb (F-statistic):\n1.17e-14\n\n\nTime:\n11:42:02\nLog-Likelihood:\n-65.226\n\n\nNo. Observations:\n30\nAIC:\n134.5\n\n\nDf Residuals:\n28\nBIC:\n137.3\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.2208\n0.869\n4.858\n0.000\n2.441\n6.001\n\n\nx\n2.0515\n0.140\n14.651\n0.000\n1.765\n2.338\n\n\n\n\n\n\n\n\nOmnibus:\n0.125\nDurbin-Watson:\n2.254\n\n\nProb(Omnibus):\n0.939\nJarque-Bera (JB):\n0.333\n\n\nSkew:\n0.081\nProb(JB):\n0.847\n\n\nKurtosis:\n2.510\nCond. No.\n13.7\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n다음과 같은 전형적인 Gaussian이 아닌 분포에 대해서는 OLS estimation은 정확하지 않을 수 있음.\nNon-constant variance(왼쪽), Poisson distribution(오른쪽)",
    "crumbs": [
      "Modelling",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/Modelling/model-basic.html#predictive-accuracy",
    "href": "contents/Modelling/model-basic.html#predictive-accuracy",
    "title": "Model Basics",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\n전통적인 모형에서는\n\n샘플에 대해서 계산된 값은 실제보다 overestimate 되는 경향이 있으므로,\n이를 보정하는 방식으로 계산: adjusted, shrunken\n\n현대적인 방식으로는\n\n샘플을 training/test set으로 나누어서, test set에 대한 예측값을 계산하고, 이를 통해 예측의 정확성을 평가함\n비슷하게, resampling 방식의 bootstrapping을 통해 해결\n\n주로 사용되는 지표들\n\n\\(RMSE = \\displaystyle\\sqrt{{\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}{{e^2}}}}\\)\n\\(MAE = \\displaystyle\\frac{{1}}{{n}} \\sum_{{i=1}}^{{n}}|~e~|\\)\n\\(R^2 = 1 - \\displaystyle\\frac{\\frac{1}{n} \\sum_{i=1}^{n}{(e-0)^2}}{\\frac{1}{n} \\sum_{i=1}^{n}{(Y-\\overline{Y})^2}} = 1 - \\frac{V(e)}{V(Y)} = \\frac{V(\\widehat Y)}{V(Y)}\\),   \\(V(Y) = V(\\widehat Y) + V(e)\\)\n\n전통적으로 X가 Y를 얼마나 잘 “설명”해주는지에 대한 지표로서 \\(R^2\\)를 사용함\n\n\ncalculate RMSE, MAE, R2\nfrom statsmodels.tools.eval_measures import rmse, meanabs\nypred = mod.predict(sim1)\ny = sim1[\"y\"]\n\nprint(f\"RMSE = {rmse(y, ypred):.2f} \\nMAE = {meanabs(y, ypred):.2f} \\nR-squared = {mod.rsquared:.2f}\")\n\n\nRMSE = 2.13 \nMAE = 1.71 \nR-squared = 0.88",
    "crumbs": [
      "Modelling",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/Modelling/model-basic.html#predictions-the-pattern-that-the-model-has-captured",
    "href": "contents/Modelling/model-basic.html#predictions-the-pattern-that-the-model-has-captured",
    "title": "Model Basics",
    "section": "Predictions: the pattern that the model has captured",
    "text": "Predictions: the pattern that the model has captured\n우선, 예측 변수들의 데이터 값을 커버하는 grid를 구성\nData: sim1.csv\n\nsim1 = pd.read_csv(\"/data/sim1.csv\")\nsim1\n\n     x     y\n0    1  4.20\n1    1  7.51\n2    1  2.13\n..  ..   ...\n27  10 24.97\n28  10 23.35\n29  10 21.98\n\n[30 rows x 2 columns]\n\n\n\n# create a grid for the range of x sim1: new data\ngrid = pd.DataFrame(dict(x=np.linspace(sim1.x.min(), sim1.x.max(), 10)))\n\n모델에 grid를 입력하여 prediction값을 추가\n\n# a model for sim1\nfrom statsmodels.formula.api import ols\nsim1_mod = ols(\"y ~ x\", data=sim1).fit()\n\ngrid[\"pred\"] = sim1_mod.predict(grid) # column 이름이 매치되어야 함\ngrid\n\n       x  pred\n0   1.00  6.27\n1   2.00  8.32\n2   3.00 10.38\n..   ...   ...\n7   8.00 20.63\n8   9.00 22.68\n9  10.00 24.74\n\n[10 rows x 2 columns]\n\n\nprediction을 시각화\n\n\nShow the code\n(\n    so.Plot(sim1, x='x', y='y')\n    .add(so.Dot(color=\".8\"))\n    .add(so.Line(marker=\".\", pointsize=10), x=grid.x, y=grid.pred)  # prediction!\n    .layout(size=(4.5, 3.5))\n    .scale(x=so.Continuous().tick(at=grid.x))\n)",
    "crumbs": [
      "Modelling",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/Modelling/model-basic.html#residuals-what-the-model-has-missed.",
    "href": "contents/Modelling/model-basic.html#residuals-what-the-model-has-missed.",
    "title": "Model Basics",
    "section": "Residuals: what the model has missed.",
    "text": "Residuals: what the model has missed.\n\\(e = Y - \\hat{Y}\\) : 관측값 - 예측값\n\nsim1[\"fitted\"] = sim1_mod.fittedvalues  # 또는 sim1_mod.predict(sim1): predicted values (Y_hat)\nsim1[\"resid\"] = sim1_mod.resid  # Y - Y_hat\n\n\nsim1\n\n     x     y  fitted  resid\n0    1  4.20    6.27  -2.07\n1    1  7.51    6.27   1.24\n2    1  2.13    6.27  -4.15\n..  ..   ...     ...    ...\n27  10 24.97   24.74   0.23\n28  10 23.35   24.74  -1.39\n29  10 21.98   24.74  -2.76\n\n[30 rows x 4 columns]\n\n\n우선, residuals의 분포를 시각화해서 살펴보면,\n\nsns.set_theme()\nsim1[\"resid\"].hist(bins=20);\n\n\n\n\n\n\n\n\n예측 변수와 residuals의 관계를 시각화해서 보면,\n\n(\n    so.Plot(sim1, x='x', y='resid')\n    .add(so.Dot())\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(5, 4))\n)\n\n\n\n\n\n\n\n\n위의 residuals은 특별한 패턴을 보이지 않아야 모델이 데이터의 패턴을 잘 잡아낸 것으로 판단할 수 있음.\n아래는 원래 데이터와 일차 선형 모형에 대한 예측값의 관계를 시각화한 것\n\n\n\n\n\n\n\n\n\nResiduals에 패턴이 보이는 경우\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n앞서 houses 데이터셋에서 price를 livingArea2와 bedrooms로 예측한 모델의 residuals를 시각화하면,\n\ncode\nhouses[\"livingArea2\"] = houses[\"livingArea\"] / 35.5\nmod_houses = ols(\"price ~ livingArea2 + bedrooms\", data=houses).fit()\n\nhouses[\"resid\"] = mod_houses.resid\n\np1 = (\n    so.Plot(houses, x='livingArea2', y='resid')\n    .add(so.Dots(color=\".5\"))\n    .add(so.Line(), so.PolyFit(5))\n)\n\np2 = (\n    so.Plot(houses, x='bedrooms', y='resid')\n    .add(so.Dots(color=\".5\"))\n    .add(so.Line(), so.PolyFit(5))\n)\ndisplay(p1, p2)",
    "crumbs": [
      "Modelling",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/Modelling/model-basic.html#two-continuous",
    "href": "contents/Modelling/model-basic.html#two-continuous",
    "title": "Model Basics",
    "section": "Two continuous",
    "text": "Two continuous\n두 연속변수가 서로 상호작용하는 경우: not additive, but multiplicative\n\n각각의 효과가 더해지는 것을 넘어서서 서로의 효과를 증폭시키거나 감소시키는 경우\n강수량과 풍속이 함께 항공편의 지연을 가중시키는 경우\n운동량과 식사량이 함께 체중 감량에 영향을 미치는 경우\n\n\n\nShow the code\nnp.random.seed(123)\nx1 = np.random.uniform(0, 10, 200)\nx2 = 2*x1 - 1 + np.random.normal(0, 12, 200)\ny = x1 + x2 + x1*x2 + np.random.normal(0, 50, 200)\ndf = pd.DataFrame(dict(precip=x1, wind=x2, delay=y))\ndf\n\n\n     precip   wind  delay\n0      6.96   4.04  95.70\n1      2.86   5.60  31.23\n2      2.27   8.37 -30.97\n..      ...    ...    ...\n197    7.45  16.38 186.67\n198    4.73 -18.56 -96.90\n199    1.22  -5.63 -22.95\n\n[200 rows x 3 columns]\n\n\n\n# additive model\nmod1 = ols('delay ~ precip + wind', data=df).fit()\n\n# interaction model\nmod2 = ols('delay ~ precip + wind + precip:wind', data=df).fit()\n\nmod2: y ~ x1 + x2 + x1:x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\) 로 변환되고,\n변형하면, \\(\\hat{y} = a_0 + a_1x_1 + (a_2 + a_3x_1)x_2\\)",
    "crumbs": [
      "Modelling",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/Modelling/model-basic.html#continuous-and-categorical",
    "href": "contents/Modelling/model-basic.html#continuous-and-categorical",
    "title": "Model Basics",
    "section": "Continuous and Categorical",
    "text": "Continuous and Categorical\n연속변수와 범주형 변수가 서로 상호작용하는 경우\n\n운동량이 건강에 미치는 효과: 혼자 vs. 단체\n\nData: sim3.csv\n\nsim3 = pd.read_csv(\"data/sim3.csv\")\nsim3\n\n     x1 x2  rep     y  sd\n0     1  a    1 -0.57   2\n1     1  a    2  1.18   2\n2     1  a    3  2.24   2\n..   .. ..  ...   ...  ..\n117  10  d    1  6.56   2\n118  10  d    2  5.06   2\n119  10  d    3  5.14   2\n\n[120 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3, x='x1', y='y', color='x2')\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), so.PolyFit(5), color=None)\n)\n\n\n\n\n\n\n\n\n\n두 가지 모델로 fit할 수 있음\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\n\nformula y ~ x1 * x2는 \\(\\hat{y} = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)로 변환됨\n\n하지만, 여기서는 x2가 범주형 변수라 dummy-coding후 적용됨.\nDesign matrix를 확인해 보면,\n\ny, X = dmatrices(\"y ~ x1 + x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1\n0       0.00     0.00     0.00  1.00\n1       0.00     0.00     0.00  1.00\n2       0.00     0.00     0.00  1.00\n..       ...      ...      ...   ...\n117     0.00     0.00     1.00 10.00\n118     0.00     0.00     1.00 10.00\n119     0.00     0.00     1.00 10.00\n\n[120 rows x 4 columns]\n\n\n\ny, X = dmatrices(\"y ~ x1 * x2\", data=sim3, return_type=\"dataframe\")\nX.iloc[:, 1:]\n\n     x2[T.b]  x2[T.c]  x2[T.d]    x1  x1:x2[T.b]  x1:x2[T.c]  x1:x2[T.d]\n0       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n1       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n2       0.00     0.00     0.00  1.00        0.00        0.00        0.00\n..       ...      ...      ...   ...         ...         ...         ...\n117     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n118     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n119     0.00     0.00     1.00 10.00        0.00        0.00       10.00\n\n[120 rows x 7 columns]\n\n\n\ngrid = sim3.value_counts([\"x1\", \"x2\"]).reset_index().drop(columns=\"count\")\ngrid[\"mod1\"] =  mod1.predict(grid)\ngrid[\"mod2\"] =  mod2.predict(grid)\ngrid_long = grid.melt(id_vars=[\"x1\", \"x2\"], var_name=\"model\", value_name=\"pred\")\ngrid_full = grid_long.merge(sim3[[\"x1\", \"x2\", \"y\"]])\n\n\ngrid_full\n\n     x1 x2 model  pred     y\n0     1  a  mod1  1.67 -0.57\n1     1  a  mod1  1.67  1.18\n2     1  a  mod1  1.67  2.24\n..   .. ..   ...   ...   ...\n237  10  d  mod2  3.98  6.56\n238  10  d  mod2  3.98  5.06\n239  10  d  mod2  3.98  5.14\n\n[240 rows x 5 columns]\n\n\n\n\nCode\n(\n    so.Plot(grid_full, x=\"x1\", y=\"y\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(), y=\"pred\")\n    .facet(\"model\")\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\n\ninteraction이 없는 모형 mod1의 경우, 네 범주에 대해 기울기가 동일하고 절편의 차이만 존재\ninteraction이 있는 모형 mod2의 경우, 네 범주에 대해 기울기가 다르고 절편도 다름\n\n\n\\(y = a_0 + a_1x_1 + a_2x_2 + a_3x_1x_2\\)에서 \\(x_1x_2\\)항이 기울기를 변할 수 있도록 해줌 \\(y = a_0 + a_2x_2 + (a_1 + a_3x_2)x_1\\)으로 변형하면, \\(x_1\\)의 기울기는 \\(a_1 + a_3 x_2\\)\n\n\n\n\n\n\n\nFitted models\n\n\n\n\n\nmod1 = ols('y ~ x1 + x2', data=sim3).fit()\nmod1.params\n# Intercept    1.87\n# x2[T.b]      2.89\n# x2[T.c]      4.81\n# x2[T.d]      2.36\n# x1          -0.20\n\nmod2 = ols('y ~ x1 * x2', data=sim3).fit() # 같은 의미 'y ~ x1 + x2 + x1:x2'\nmod2.params\n# Intercept     1.30\n# x2[T.b]       7.07\n# x2[T.c]       4.43\n# x2[T.d]       0.83\n# x1           -0.09\n# x1:x2[T.b]   -0.76\n# x1:x2[T.c]    0.07\n# x1:x2[T.d]    0.28\n\n\n\n두 모형을 비교하여 중 더 나은 모형을 선택하기 위해, residuals을 차이를 살펴보면,\n\nsim3[\"mod1\"] = mod1.resid\nsim3[\"mod2\"] = mod2.resid\n\nsim3_long = sim3.melt(\n    id_vars=[\"x1\", \"x2\"],\n    value_vars=[\"mod1\", \"mod2\"],\n    var_name=\"model\",\n    value_name=\"resid\",\n)\nsim3_long\n\n     x1 x2 model  resid\n0     1  a  mod1  -2.25\n1     1  a  mod1  -0.49\n2     1  a  mod1   0.56\n3     1  b  mod1   2.87\n..   .. ..   ...    ...\n236  10  c  mod2  -0.64\n237  10  d  mod2   2.59\n238  10  d  mod2   1.08\n239  10  d  mod2   1.16\n\n[240 rows x 4 columns]\n\n\n\n\nCode\n(\n    so.Plot(sim3_long, x=\"x1\", y=\"resid\", color=\"x2\")\n    .add(so.Dot(pointsize=4))\n    .add(so.Line(linestyle=\":\", color=\".5\"), so.Agg(lambda x: 0))\n    .facet(\"x2\", \"model\")\n    .layout(size=(9, 6))\n    .scale(color=\"Set2\")\n)\n\n\n\n\n\n\n\n\n\n\n둘 중 어떤 모델이 더 나은지에 대한 정확한 통계적 비교가 가능하나 (잔차의 제곱의 평균인 RMSE나 잔차의 절대값의 평균인 MAE 등)\n여기서는 직관적으로 어느 모델이 데이터의 패턴을 더 잘 잡아냈는지를 평가하는 것으로 충분\n잔차를 직접 들여다봄으로써, 어느 부분에서 어떻게 예측이 잘 되었는지, 잘 안 되었는지를 면밀히 검사할 수 있음\ninteraction 항이 있는 모형이 더 나은 모형\n\nSaratogaHouses 데이터에서 가령, livingArea와 centralAir의 interaction을 살펴보면,\n\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\n(\n    so.Plot(houses, x='livingArea', y='price')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(color=\"orangered\"), so.PolyFit(1))\n    .facet(\"centralAir\")\n    .label(title=\"Central Air: {}\".format)\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\nmod1 = ols('price ~ livingArea + centralAir', data=houses).fit()\nmod2 = ols('price ~ livingArea * centralAir', data=houses).fit()\n\ndisplay(mod1.params, mod2.params)\n\nIntercept           14144.05\ncentralAir[T.Yes]   28450.58\nlivingArea            106.76\ndtype: float64\n\n\nIntercept                       44977.64\ncentralAir[T.Yes]              -53225.75\nlivingArea                         87.72\nlivingArea:centralAir[T.Yes]       44.61\ndtype: float64\n\n\nR-squared 비교\n\ndisplay(mod1.rsquared, mod2.rsquared)\n\n0.5253223149339137\n\n\n0.5430362101820772",
    "crumbs": [
      "Modelling",
      "Model Basics"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1_ex.html",
    "href": "contents/Modelling/modelling1_ex.html",
    "title": "Modelling Building 1 Exercises",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Modelling",
      "Flights to SFO"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling1_ex.html#flights-to-sfo",
    "href": "contents/Modelling/modelling1_ex.html#flights-to-sfo",
    "title": "Modelling Building 1 Exercises",
    "section": "Flights to SFO",
    "text": "Flights to SFO\n\n# import the data\nflights = sm.datasets.get_rdataset('flights', 'nycflights13').data\n\n# convert the date column to a datetime object\nflights[\"time_hour\"] = pd.to_datetime(flights[\"time_hour\"])\n\n# add a column for the day of the week\nflights[\"dow\"] = (\n    flights[\"time_hour\"]\n    .dt.day_name()\n    .str[:3]\n    .astype(\"category\")\n    .cat.set_categories([\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"])\n)\n\n# add a column for the season\nflights[\"season\"] = np.where(flights[\"month\"].isin([6, 7]), \"summer\", \"other month\")\n\n\n# filter out the flights to SFO\nsfo = flights.query('dest == \"SFO\" & arr_delay &lt; 500').copy()\n\n\nfrom statsmodels.formula.api import ols\nmod = ols(\"arr_delay ~ hour + origin + carrier + season + dow\", data=sfo).fit()\n\n이미 세운 모형의 잔차를 분석\n\nsfo[\"resid\"] = mod.resid\n\n\n(\n    so.Plot(sfo, x='hour', y='resid')\n    #.add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(sfo, x='hour', y='resid')\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"season\")\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(sfo, x='hour', y='resid')\n    .add(so.Line(), so.PolyFit(5))\n    .facet(row=\"season\", col=\"dow\")\n    .layout(size=(9, 6))\n)\n\n\n\n\n\n\n\n\n처음부터 변수들 간의 관계를 파악하면서 모형을 세운다면,\n\n(\n    so.Plot(sfo, x='hour', y='arr_delay')\n    #.add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(sfo, x='hour', y='arr_delay')\n    #.add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"month\")\n    .layout(size=(12, 4))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(sfo, x='hour', y='arr_delay')\n    #.add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"dow\")\n    .layout(size=(9, 4))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(sfo.query('dow == \"Sat\"'), x='hour', y='arr_delay')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(9, 4))\n    .limit(y=(-25, 25))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(sfo.query('dow == \"Tue\"'), x='hour', y='arr_delay')\n    .add(so.Dots(alpha=.1))\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(9, 4))\n    .limit(y=(-25, 25))\n)\n\n\n\n\n\n\n\n\n\nfrom sbcustom import rangeplot\nrangeplot(sfo, x='origin', y='arr_delay')\n\n\n\n\n\n\n\n\n\nrangeplot(sfo, x='carrier', y='arr_delay')\n\n\n\n\n\n\n\n\n\nprint(mod.summary(slim=True))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              arr_delay   R-squared:                       0.096\nModel:                            OLS   Adj. R-squared:                  0.095\nNo. Observations:               13169   F-statistic:                     107.0\nCovariance Type:            nonrobust   Prob (F-statistic):          8.13e-275\n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept          -20.4363      2.116     -9.660      0.000     -24.583     -16.289\norigin[T.JFK]        4.1590      0.966      4.306      0.000       2.266       6.052\ncarrier[T.B6]       -9.5832      1.807     -5.302      0.000     -13.126      -6.040\ncarrier[T.DL]      -17.9685      1.553    -11.571      0.000     -21.012     -14.925\ncarrier[T.UA]       -4.0238      1.424     -2.826      0.005      -6.815      -1.232\ncarrier[T.VX]       -4.9082      1.537     -3.194      0.001      -7.921      -1.896\nseason[T.summer]    24.6238      0.991     24.838      0.000      22.681      26.567\ndow[T.Mon]          -2.6982      1.424     -1.895      0.058      -5.489       0.093\ndow[T.Tue]          -6.0939      1.423     -4.282      0.000      -8.883      -3.304\ndow[T.Wed]          -5.3144      1.425     -3.728      0.000      -8.108      -2.520\ndow[T.Thu]          -1.2890      1.425     -0.905      0.366      -4.082       1.504\ndow[T.Fri]          -4.9515      1.422     -3.483      0.000      -7.738      -2.165\ndow[T.Sat]         -10.3953      1.518     -6.847      0.000     -13.371      -7.419\nhour                 2.0666      0.086     23.944      0.000       1.897       2.236\n====================================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nmod = ols(\"arr_delay ~ hour + origin + carrier + season + dow\", data=sfo).fit()\n\n# origin removed\nmod1 = ols(\"arr_delay ~ hour + carrier + season + dow\", data=sfo).fit()\n\n# carrier removed\nmod2 = ols(\"arr_delay ~ hour + season + dow\", data=sfo).fit()\n\nprint(f\"R-squared: {mod.rsquared:.3f}, {mod1.rsquared:.3f}, {mod2.rsquared:.3f}\")\n\n\n\nR-squared: 0.096, 0.094, 0.084\n\n\n\n# hour와 season이 상호작용하는 것으로 가정\nmod3 = ols(\"arr_delay ~ hour * season + origin + carrier + dow\", data=sfo).fit()\n\n# hour와 모든 달과 상호작용하는 것으로 가정\nmod4 = ols(\"arr_delay ~ hour * C(month) + origin + carrier + dow\", data=sfo).fit()  # C(month) : month를 범주형 변수로 취급\n\n# hour, 모든 달, 요일이 서로 상호작용하는 것으로 가정\nmod5 = ols(\"arr_delay ~ hour * C(month) * dow + origin + carrier\", data=sfo).fit()  # C(month) : month를 범주형 변수로 취급\n\nprint(f\"R-squared: {mod.rsquared:.3f}, {mod3.rsquared:.3f}, {mod4.rsquared:.3f}, {mod5.rsquared:.3f}\")\n\n\n\nR-squared: 0.096, 0.118, 0.136, 0.177\n\n\n\nSample에서 더 많은 변량이 설명된다는 것이 반드시 좋은 모형을 의미하지는 않음!\n통계에서는 adjusted(shrunken) R-squared로 보정\n\nprint(mod5.summary().tables[0])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              arr_delay   R-squared:                       0.177\nModel:                            OLS   Adj. R-squared:                  0.166\nMethod:                 Least Squares   F-statistic:                     16.25\nDate:                Tue, 03 Dec 2024   Prob (F-statistic):               0.00\nTime:                        03:09:23   Log-Likelihood:                -67798.\nNo. Observations:               13169   AIC:                         1.359e+05\nDf Residuals:                   12996   BIC:                         1.372e+05\nDf Model:                         172                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n\n\nCross-validation\n5-fold cross-validation을 통해 모형의 성능을 평가\n\n\ncode for cv_score()\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error\n\n\ndef cv_score(mod):\n    print(f\"the number of parameters: {mod.df_model}\\n\")\n\n    kfold = KFold(5, shuffle=True, random_state=0)\n    ave_r2, ave_aic, ave_bic = 0, 0, 0\n\n    for i, (train_idx, test_idx) in enumerate(kfold.split(sfo)):\n        sfo_train, sfo_test = sfo.iloc[train_idx], sfo.iloc[test_idx]\n        \n        mod_ = ols(mod.model.formula, data=sfo_train).fit()\n\n        # R-squared\n        r2_train = r2_score(sfo_train[\"arr_delay\"], mod_.predict(sfo_train))\n        r2_test = r2_score(sfo_test[\"arr_delay\"], mod_.predict(sfo_test))\n        ave_r2 += r2_test\n\n        # AIC, BIC\n        aic, bic = mod_.aic, mod_.bic\n        ave_aic += aic\n        ave_bic += bic\n\n        print(f\"Fold {i+1}: Train R2: {r2_train:.2f}, Test R2: {r2_test:.2f}\")\n\n    print(f\"\\nAverage Test R2: {ave_r2/5:.2f}\")\n    print(f\"Average AIC: {ave_aic/5:.2f}, Average BIC: {ave_bic/5:.2f}\")\n\n\n\ncv_score(mod5)\n\nthe number of parameters: 172.0\n\nFold 1: Train R2: 0.19, Test R2: 0.12\nFold 2: Train R2: 0.18, Test R2: 0.13\nFold 3: Train R2: 0.18, Test R2: 0.17\nFold 4: Train R2: 0.17, Test R2: 0.18\nFold 5: Train R2: 0.18, Test R2: 0.16\n\nAverage Test R2: 0.15\nAverage AIC: 108788.40, Average BIC: 110044.81\n\n\n\ncv_score(mod4)\n\nthe number of parameters: 34.0\n\nFold 1: Train R2: 0.14, Test R2: 0.11\nFold 2: Train R2: 0.14, Test R2: 0.12\nFold 3: Train R2: 0.13, Test R2: 0.14\nFold 4: Train R2: 0.13, Test R2: 0.14\nFold 5: Train R2: 0.14, Test R2: 0.14\n\nAverage Test R2: 0.13\nAverage AIC: 109047.00, Average BIC: 109301.19\n\n\n\ncv_score(mod3)\n\nthe number of parameters: 14.0\n\nFold 1: Train R2: 0.12, Test R2: 0.09\nFold 2: Train R2: 0.12, Test R2: 0.11\nFold 3: Train R2: 0.12, Test R2: 0.13\nFold 4: Train R2: 0.12, Test R2: 0.13\nFold 5: Train R2: 0.12, Test R2: 0.13\n\nAverage Test R2: 0.12\nAverage AIC: 109229.17, Average BIC: 109338.10\n\n\n\n# Spline regression\nmod5_spline = ols(\"arr_delay ~ cr(hour, df=5) * C(month) * dow + origin + carrier\", data=sfo).fit()\n\n\ncv_score(mod5_spline)\n\nthe number of parameters: 424.0\n\nFold 1: Train R2: 0.21, Test R2: 0.10\nFold 2: Train R2: 0.21, Test R2: 0.12\nFold 3: Train R2: 0.20, Test R2: 0.14\nFold 4: Train R2: 0.20, Test R2: 0.16\nFold 5: Train R2: 0.20, Test R2: 0.15\n\nAverage Test R2: 0.14\nAverage AIC: 108957.71, Average BIC: 112044.26\n\n\n\n# Ridge regression with polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import make_pipeline\n\nkfold = KFold(5, shuffle=True, random_state=0)\nsfo[\"month\"] = sfo[\"month\"].astype(\"category\")  # 범주형 변수로 변환\n\nX = sfo[[\"hour\", \"origin\", \"carrier\", \"month\", \"dow\"]]\nX = pd.get_dummies(X, drop_first=True)\ny = sfo[\"arr_delay\"]\n\npoly = PolynomialFeatures(2)  # 모든 2차 항을 포함\nridge = RidgeCV()\n\npipe = make_pipeline(poly, ridge)\ncross_val_score(pipe, X, y, cv=kfold, scoring=\"r2\")\n\narray([0.13, 0.15, 0.17, 0.19, 0.16])\n\n\n\n# the number of parameters\npoly.fit_transform(X).shape\n\n(13169, 300)\n\n\n\n# XGBoost\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor(n_estimators=5000, learning_rate=0.01, max_depth=3, random_state=0)\ncross_val_score(xgb, X, y, cv=kfold, scoring=\"r2\")\n\narray([0.13, 0.15, 0.18, 0.19, 0.18])",
    "crumbs": [
      "Modelling",
      "Flights to SFO"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2_ex.html",
    "href": "contents/Modelling/modelling2_ex.html",
    "title": "Model Buidling 2 Exercises",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Modelling",
      "Bike Sharing"
    ]
  },
  {
    "objectID": "contents/Modelling/modelling2_ex.html#bike-sharing-demand",
    "href": "contents/Modelling/modelling2_ex.html#bike-sharing-demand",
    "title": "Model Buidling 2 Exercises",
    "section": "Bike Sharing Demand",
    "text": "Bike Sharing Demand\nBike Sharing in Washington D.C. Dataset\n\nbikeshare = pd.read_csv(\"../data/hour.csv\")\nbikeshare_daily = pd.read_csv(\"../data/day.csv\")\n\n\ndef clean_data(df):\n    df.rename({\"dteday\": \"date\", \"cnt\": \"count\"}, axis=1, inplace=True)\n\n    df = df.assign(\n        date=lambda x: pd.to_datetime(x[\"date\"]),  # datetime type으로 변환\n        year=lambda x: x[\"date\"].dt.year.astype(str),  # year 추출\n        day=lambda x: x[\"date\"].dt.day_of_year,  # day of the year 추출\n        month=lambda x: x[\"date\"].dt.month_name().str[:3],  # month 추출\n        wday=lambda x: x[\"date\"].dt.day_name().str[:3],  # 요일 추출\n    )\n\n    df[\"season\"] = (\n        df[\"season\"]\n        .map({1: \"winter\", 2: \"spring\", 3: \"summer\", 4: \"fall\"})  # season을 문자열로 변환\n        .astype(\"category\")  # category type으로 변환\n        .cat.set_categories(\n            [\"winter\", \"spring\", \"summer\", \"fall\"], ordered=True\n        )  # 순서를 지정\n    )\n    return df\n\n\nbikes = clean_data(bikeshare)\nbikes_daily = clean_data(bikeshare_daily)\n\n\nbikes_daily.head(3)\n\n   instant       date  season  yr  mnth  holiday  weekday  workingday  \\\n0        1 2011-01-01  winter   0     1        0        6           0   \n1        2 2011-01-02  winter   0     1        0        0           0   \n2        3 2011-01-03  winter   0     1        0        1           1   \n\n   weathersit  temp  atemp  hum  windspeed  casual  registered  count  year  \\\n0           2  0.34   0.36 0.81       0.16     331         654    985  2011   \n1           2  0.36   0.35 0.70       0.25     131         670    801  2011   \n2           1  0.20   0.19 0.44       0.25     120        1229   1349  2011   \n\n   day month wday  \n0    1   Jan  Sat  \n1    2   Jan  Sun  \n2    3   Jan  Mon  \n\n\n\n(\n    so.Plot(bikes_daily, x='date')\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(8, 6))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(bikes_daily, x='date')\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.Spline(10))\n    .layout(size=(8, 6))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(bikes_daily, x='day', color=\"year\")\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .layout(size=(7, 7))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(bikes_daily, x='day', color=\"year\")\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.Spline(5))\n    .layout(size=(7, 7))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(bikes_daily, x='day', y='temp', color=\"year\")\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n# 요일에 따라 차이가 있는가?\nfrom sbcustom import boxplot\n(\n    boxplot(bikes_daily, x=\"wday\", y=\"casual\")\n    .facet(\"year\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n(\n    boxplot(bikes_daily, x=\"wday\", y=\"registered\")\n    .facet(\"year\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n# 달에 따라 차이가 있는가?\n(\n    boxplot(bikes_daily, x=\"month\", y=\"casual\")\n    .facet(\"year\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n(\n    boxplot(bikes_daily, x=\"month\", y=\"registered\")\n    .facet(\"year\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n# 기온에 따라 차이가 있는가?\n(\n    so.Plot(bikes_daily, x='temp')\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"year\")\n    .layout(size=(9, 7))\n)\n\n\n\n\n\n\n\n\nY의 분포가 daily, hourly에 따라 매우 다름\n\n# daily\n(\n    so.Plot(bikes_daily)\n    .pair(x=['casual', 'registered'])\n    .add(so.Bars(), so.Hist())\n    .facet(row=\"year\")\n    .layout(size=(6, 5))\n)\n\n\n\n\n\n\n\n\n\n# hourly\n(\n    so.Plot(bikes)\n    .pair(x=['casual', 'registered'])\n    .add(so.Bars(), so.Hist())\n    .facet(row=\"year\")\n    .layout(size=(6, 5))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(bikes_daily, x='temp')\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"year\")\n    .layout(size=(9, 7))\n)\n\n\n\n\n\n\n\n\n\n# 바람의 세기에 따라 차이가 있는가?\n(\n    so.Plot(bikes_daily, x='windspeed')\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"year\")\n    .layout(size=(9, 7))\n)\n\n\n\n\n\n\n\n\n\n# 습도에 따라 차이가 있는가?\n(\n    so.Plot(bikes_daily, x='hum')\n    .pair(y=['casual', 'registered'])\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"year\")\n    .layout(size=(9, 7))\n    .limit(x=(0.4, 1))\n)\n\n\n\n\n\n\n\n\n\n# 기온과 습도의 관계?\n(\n    so.Plot(bikes_daily, x='temp', y='hum')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\nRegistered bikers\n\n(\n    so.Plot(bikes_daily, x='temp', y='registered')\n    .add(so.Dots(color=\".6\"))\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\"red\"), so.PolyFit(2))\n    .add(so.Line(color=\"black\"), so.Spline(5))\n    .facet(\"year\")\n)\n\n\n\n\n\n\n\n\nyear는 예측변수로 의미는 없으나 modeling 방식의 예시를 위해 사용했음.\n\nimport statsmodels.formula.api as smf\nmod_r1 = smf.ols(\"registered ~ temp + I(temp**2) + year\", data=bikes_daily).fit()\nmod_r2 = smf.ols(\"registered ~ (temp + I(temp**2)) * year\", data=bikes_daily).fit()\n\n\n\npredictions\ntemp = np.linspace(bikes_daily[\"temp\"].min(), bikes_daily[\"temp\"].max(), 30)\nyear = np.array([\"2011\", \"2012\"])\n\nfrom itertools import product\ngrid = pd.DataFrame(\n    list(product(temp, year)),\n    columns=[\"temp\", \"year\"],\n)\ngrid[\"pred_r1\"] = mod_r1.predict(grid)\ngrid[\"pred_r2\"] = mod_r2.predict(grid)\n\n(\n    so.Plot(grid, x='temp', color=\"year\")\n    .pair(y=['pred_r1', 'pred_r2'], wrap=1)\n    .add(so.Dots())\n    .layout(size=(7, 4))\n)\n\n\n\n\n\n\n\n\n\n\n\nresiduals\nbikes_daily_resid = bikes_daily.assign(\n    resid_r1 = mod_r1.resid,\n    resid_r2 = mod_r2.resid\n)\n\n(\n    so.Plot(bikes_daily_resid, x='temp')\n    .pair(y=['resid_r1', 'resid_r2'])\n    .add(so.Dots(color='.6'))\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\".2\"), so.Agg(lambda x: 0))\n    .facet(\"year\")\n    .layout(size=(7, 6))\n)\n\n\n\n\n\n\n\n\n\n온도와 달의 밀접한 관계?\n온도로 예측되지 못한 부분을 달이 추가적으로 예측할 수 있는 여지가 있는가?\n\nmod = smf.ols('temp ~ month', data=bikes_daily).fit()\nmod.rsquared\n\n0.8378460473171662\n\n\n\n(\n    boxplot(bikes_daily, x='month', y='temp')\n    .facet(\"year\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(bikes_daily_resid, x='month', y='resid_r2')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"year\")\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n9~11월에는 온도로는 예측되지 않는, 즉 같은 온도라고 해도 자건거를 특별히 더 대여하게 되는 특성이 있을 수 있음.\n반대로, 2~4월에는 온도로는 예측되지 않는 자건거를 특별히 더 적게 대여하게 되는 특성이 있을 수 있음.\n그 이유를 파악할 수 있다면, term을 만들어 추가할 수 있을 것.\n\n(\n    so.Plot(bikes_daily_resid, x='date', y='resid_r2')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5)) # grouping by year\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\nbikes_daily[\"month\"] = bikes_daily[\"month\"].astype(\"category\")\nbikes_daily[\"month\"] = bikes_daily[\"month\"].cat.set_categories(\n    [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n)\n\n\nmod_r3 = smf.ols(\"registered ~ (temp + I(temp**2)) * year + month\", data=bikes_daily).fit()\n\ndisplay(mod_r3.rsquared, mod_r2.rsquared)  # month 추가로 인한 R2가 증가분\n\n\n\n0.7053133858977505\n\n\n0.6681657121983411\n\n\n\n\nprint(mod_r3.summary(slim=True))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             registered   R-squared:                       0.705\nModel:                            OLS   Adj. R-squared:                  0.699\nNo. Observations:                 731   F-statistic:                     106.8\nCovariance Type:            nonrobust   Prob (F-statistic):          5.06e-177\n=============================================================================================\n                                coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                   180.4223    321.376      0.561      0.575    -450.533     811.377\nyear[T.2012]              -1303.6849    477.877     -2.728      0.007   -2241.898    -365.472\nmonth[T.Feb]                 16.8031    163.242      0.103      0.918    -303.688     337.295\nmonth[T.Mar]                 70.8817    178.488      0.397      0.691    -279.542     421.306\nmonth[T.Apr]                291.0581    198.194      1.469      0.142     -98.054     680.170\nmonth[T.May]                649.9687    223.994      2.902      0.004     210.202    1089.735\nmonth[T.Jun]               1021.8253    248.544      4.111      0.000     533.861    1509.789\nmonth[T.Jul]                853.7900    277.468      3.077      0.002     309.039    1398.541\nmonth[T.Aug]                985.2750    256.593      3.840      0.000     481.508    1489.041\nmonth[T.Sep]               1085.0183    229.970      4.718      0.000     633.521    1536.516\nmonth[T.Oct]                995.3377    200.849      4.956      0.000     601.012    1389.663\nmonth[T.Nov]                946.3073    178.028      5.315      0.000     596.786    1295.828\nmonth[T.Dec]                568.6507    166.184      3.422      0.001     242.382     894.919\ntemp                       6120.3556   1655.969      3.696      0.000    2869.205    9371.506\ntemp:year[T.2012]           1.24e+04   2087.372      5.938      0.000    8297.289    1.65e+04\nI(temp ** 2)              -3876.2985   1731.986     -2.238      0.026   -7276.693    -475.904\nI(temp ** 2):year[T.2012] -1.105e+04   2082.530     -5.304      0.000   -1.51e+04   -6958.058\n=============================================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nbikes_daily_resid[\"resid_r3\"] = mod_r3.resid\n\n\n(\n    so.Plot(bikes_daily_resid, x='date', y='resid_r3')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n# 예측값과 실제값 비교\nbikes_daily_resid[\"pred_r3\"] = mod_r3.predict(bikes_daily)\n(\n    so.Plot(bikes_daily_resid, x='registered', y='pred_r3')\n    .add(so.Dots())\n    .add(so.Line(), y='registered')\n    .facet(\"year\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\noutliers = bikes_daily_resid.query('resid_r3 &lt; -2000')[[\"date\", \"holiday\"]]\noutliers\n\n          date  holiday\n238 2011-08-27        0\n365 2012-01-01        0\n448 2012-03-24        0\n477 2012-04-22        0\n499 2012-05-14        0\n517 2012-06-01        0\n567 2012-07-21        0\n596 2012-08-19        0\n610 2012-09-02        0\n626 2012-09-18        0\n645 2012-10-07        0\n667 2012-10-29        0\n668 2012-10-30        0\n691 2012-11-22        1\n692 2012-11-23        0\n693 2012-11-24        0\n723 2012-12-24        0\n724 2012-12-25        1\n725 2012-12-26        0\n\n\n모형의 비교와 각 예측변수의 공헌도\n\nmod_r_year = smf.ols(\"registered ~ year\", data=bikes_daily).fit()\nmod_r_temp = smf.ols(\"registered ~ year + temp + I(temp**2)\", data=bikes_daily).fit()\nmod_r_month = smf.ols(\"registered ~ year + temp + I(temp**2) + month\", data=bikes_daily).fit()\nmod_r_wday = smf.ols(\"registered ~ year + temp + I(temp**2) + month + wday\", data=bikes_daily).fit()\n\nprint(\n    f\" year: {mod_r_year.rsquared:.3f}\\n\",\n    f\"year + temperature: {mod_r_temp.rsquared:.3f}\\n\",\n    f\"year + temperature + months: {mod_r_month.rsquared:.3f}\\n\",\n    f\"year + temperature + months + days of the week: {mod_r_wday.rsquared:.3f}\",\n)\n\n year: 0.353\n year + temperature: 0.653\n year + temperature + months: 0.686\n year + temperature + months + days of the week: 0.757\n\n\n\n\n\nCasual bikers\n\n(\n    so.Plot(bikes_daily, x='temp', y='casual')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n    .facet(\"year\")\n)\n\n\n\n\n\n\n\n\n\n# log scale for y\n(\n    so.Plot(bikes_daily, x='temp', y='casual')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(2))\n    .facet(\"year\")\n    .scale(y=\"log\")  # log scale\n)\n\n\n\n\n\n\n\n\n\n# 온도를 5개 구간으로 나누어 각 온도구간에서의 분포를 살펴봄\nbikes_daily[\"temp_cat\"] = pd.qcut(bikes_daily[\"temp\"], 5)\n\n(\n    so.Plot(bikes_daily, x='casual')\n    .add(so.Bars(), so.Hist())\n    .facet(\"temp_cat\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\nbikes_daily[\"temp_cat\"] = pd.qcut(bikes_daily[\"temp\"], 7)\n\nbikes_daily.assign(casual2 = lambda x: x.casual / 100).groupby(\"temp_cat\")[\"casual2\"].agg([\"mean\", \"var\"])\n\n                 mean   var\ntemp_cat                   \n(0.0581, 0.281]  1.94  2.54\n(0.281, 0.355]   4.33  9.42\n(0.355, 0.447]   6.74 38.09\n(0.447, 0.544]  10.11 46.18\n(0.544, 0.635]  11.71 58.61\n(0.635, 0.716]  12.67 42.25\n(0.716, 0.862]  11.88 29.46\n\n\n\n이렇게 count 데이터가 전형적으로 보이는 “평균에 따라 표준편차가 함께 증가”하는 경우, generalized linear model (GLM)의 하나인 포아송(Poisson) 분포나 이를 보정한 다른 분포를 결합한 모형을 이용하는 것이 권장됨.\nY를 transform하는 것보다 여러 이점이 있는 한편, Y가 내재적으로 log나 sqrt의 의미를 띤다면 transform하는 것이 더 적절할 수도 있음.\n\n\nbikes_daily[\"lcasual\"] = np.log2(bikes_daily[\"casual\"] + 1)\n\n\nmod_c = smf.ols(\"lcasual ~ (temp + I(temp**2)) + year\", data=bikes_daily).fit()\n\n\nbikes_daily_resid[\"resid_c\"] = mod_c.resid\nbikes_daily_resid[\"lcasual\"] = np.log2(bikes_daily_resid[\"casual\"] + 1)\n\n\n(\n    so.Plot(bikes_daily_resid, x='date', y='resid_c')\n    .add(so.Dots())\n    .add(so.Line(), so.Spline(10))\n    .limit(y=(-3, 2))\n)\n\n\n\n\n\n\n\n\n\n# 요일에 따라 차이가 있는가?\nfrom sbcustom import boxplot\n(\n    boxplot(bikes_daily, x=\"wday\", y=\"casual\")\n    .facet(\"year\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n요일의 차이가 추가적으로 설명할 수 있는 부분이 있는가?\n\n(\n    so.Plot(bikes_daily_resid, x='date', y='resid_c')\n    .add(so.Dots())\n    .add(so.Line(), so.Spline(5))\n    .add(so.Line(color=\".6\"), so.Agg(lambda x: 0))\n    .limit(y=(-3, 2))\n    .facet(\"wday\", wrap=4)\n    .layout(size=(9, 5))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(bikes_daily_resid, x='date', y='resid_c')\n    .add(so.Dots())\n    .add(so.Line(), so.Spline(5))\n    .add(so.Line(color=\".6\"), so.Agg(lambda x: 0))\n    .limit(y=(-3, 2))\n    .facet(\"workingday\")\n    .layout(size=(9, 5))\n    .label(title=\"workingday: {}\".format)\n)\n\n\n\n\n\n\n\n\n요일별로 모두 고려할 것인가? 아니면 평일/주말로 나눌 것인가?\n# 요일별로\nmod_c2 = smf.ols(\"lcasual ~ (temp + I(temp**2)) + year + wday\", data=bikes_daily).fit()\ndisplay(mod_c.rsquared, mod_c2.rsquared)\n\n\n\n0.5540585294121951\n\n\n0.7283361326294607\n\n\n\n# 평일/주말\nmod_c2_1 = smf.ols(\"lcasual ~ (temp + I(temp**2)) + year + workingday\", data=bikes_daily).fit()\ndisplay(mod_c.rsquared, mod_c2.rsquared, mod_c2_1.rsquared)\n\n\n\n0.5540585294121951\n\n\n0.7283361326294607\n\n\n\n\n0.7242260371575533\n\n\n\n# 습도 추가\nmod_c3 = smf.ols(\"lcasual ~ (temp + I(temp**2)) + year + wday + bs(hum, 4)\", data=bikes_daily).fit()\ndisplay(mod_c.rsquared, mod_c2.rsquared, mod_c3.rsquared)\n\n\n\n0.5540585294121951\n\n\n0.7283361326294607\n\n\n\n\n0.8008034311536445\n\n\n\n\nmod_c3_robust = sm.RLM.from_formula(\"lcasual ~ (temp + I(temp**2)) + year + wday + bs(hum, 4)\", data=bikes_daily).fit()\n\n마지막 모형(mod_c2)의 예측값\nsmf.ols(\"lcasual ~ (temp + I(temp**2)) + year + wday + bs(hum, 4)\", data=bikes_daily)\n\n\npredictions\ntemp = np.linspace(bikes_daily[\"temp\"].min(), bikes_daily[\"temp\"].max(), 30)\nhum = np.array([bikes_daily[\"hum\"].mean()])\nyear = np.array([\"2011\", \"2012\"])\nwday = np.array([\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"])\n\nfrom itertools import product\ngrid = pd.DataFrame(\n    list(product(temp, year, wday, hum)),\n    columns=[\"temp\", \"year\", \"wday\", \"hum\"],\n)\ngrid[\"log_pred_c\"] = mod_c3.predict(grid)\n\n(\n    so.Plot(grid, x='temp', y='log_pred_c', color=\"year\")\n    .add(so.Dots())\n    .facet(\"wday\", wrap=4)\n    .label(title=\"Pred of mod_c2 {}\".format)\n    .layout(size=(9, 5))\n)\n\n\n\n\n\n\n\n\n\nOriginal scale로 예측값\n\n\npredictions with the original scale\ntemp = np.linspace(bikes_daily[\"temp\"].min(), bikes_daily[\"temp\"].max(), 30)\nhum = np.array([bikes_daily[\"hum\"].mean()])\nyear = np.array([\"2011\", \"2012\"])\nwday = np.array([\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"])\n\nfrom itertools import product\ngrid = pd.DataFrame(\n    list(product(temp, year, wday, hum)),\n    columns=[\"temp\", \"year\", \"wday\", \"hum\"],\n)\ngrid[\"pred_c\"] = 2**mod_c3.predict(grid) - 1\n\n(\n    so.Plot(grid, x='temp', y='pred_c', color=\"year\")\n    .add(so.Dots())\n    .facet(\"wday\", wrap=4)\n    .label(title=\"Pred of mod_c2 {}\".format)\n    .layout(size=(9, 5))\n)\n\n\n\n\n\n\n\n\n\n잔차 확인\n\nbikes_daily_resid[\"resid_c2\"] = mod_c2.resid\nbikes_daily_resid[\"resid_c3\"] = mod_c3.resid\n\n\n\nresidual plot for mod_c2; 습도 추가 전\n(\n    so.Plot(bikes_daily_resid, x='temp', y='resid_c2')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(), so.Spline(4))\n    .add(so.Line(color=\".2\"), so.Agg(lambda x: 0))\n    .facet(\"year\")\n    .layout(size=(7, 5))\n    .limit(y=(-3, 2))\n)\n\n\n\n\n\n\n\n\n\n\n\nresidual plot for mod_c3; 습도까지 포함\n(\n    so.Plot(bikes_daily_resid, x='temp', y='resid_c3')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(), so.Spline(4))\n    .add(so.Line(color=\".2\"), so.Agg(lambda x: 0))\n    .facet(\"year\")\n    .layout(size=(7, 5))\n    .limit(y=(-3, 2))\n)\n\n\n\n\n\n\n\n\n\n\n\nresidual with the original scale\nbikes_daily_resid = bikes_daily.assign(\n    pred_c3_revert = lambda x: 2**mod_c3.predict(x) - 1,\n    resid_c3_revert = lambda x: x[\"casual\"] - x[\"pred_c3_revert\"]\n)\n\n(\n    so.Plot(bikes_daily_resid, x='day', y='resid_c3_revert')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\".2\"), so.Agg(lambda x: 0))\n    .facet(\"year\")\n    .layout(size=(7, 5))\n    .limit(y=(-1000, 1000))\n)\n\n\n\n\n\n\n\n\n\n\n\nPoissong regresion residuals\n# poisson regression\nmod_c3_pois = smf.poisson(\"casual ~ (temp + I(temp**2)) + year + wday + bs(hum, 5)\", data=bikes_daily).fit()\nbikes_daily_resid[\"resid_c3_pois\"] = mod_c3_pois.resid\nbikes_daily_resid[\"pred_c3_pois\"] = mod_c3_pois.predict(bikes_daily)\n(\n    so.Plot(bikes_daily_resid, x='day', y='resid_c3_pois')\n    .add(so.Dots(color='.6'))\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\".2\"), so.Agg(lambda x: 0))\n    .facet(\"year\")\n    .layout(size=(7, 5))\n    .limit(y=(-1000, 1000))\n)\n\n\nOptimization terminated successfully.\n         Current function value: 49.152525\n         Iterations 7\n\n\n\n\n\n\n\n\n\n\n# 예측값과 실제값 비교\n(\n    so.Plot(bikes_daily_resid, x='casual', y='pred_c3_revert')\n    .add(so.Dots())\n    .add(so.Line(color=\".3\"), y='casual')\n    .facet(\"year\")\n)\n\n\n\n\n\n\n\n\n\n# 예측값과 실제값 비교: Poisson regression\n(\n    so.Plot(bikes_daily_resid, x='casual', y='pred_c3_pois')\n    .add(so.Dots())\n    .add(so.Line(color=\".3\"), y='casual')\n    .facet(\"year\")\n)\n\n\n\n\n\n\n\n\n모형의 비교와 각 예측변수의 공헌도\n\nmod_c_year = smf.ols(\"lcasual ~ year\", data=bikes_daily).fit()\nmod_c_temp = smf.ols(\"lcasual ~ year + temp + I(temp**2)\", data=bikes_daily).fit()\nmod_c_month = smf.ols(\n    \"lcasual ~ year + temp + I(temp**2) + month\", data=bikes_daily\n).fit()\nmod_c_hum = smf.ols(\n    \"lcasual ~ year + temp + I(temp**2) + month + bs(hum, 4)\", data=bikes_daily\n).fit()\nmod_c_wday = smf.ols(\n    \"lcasual ~ year + temp + I(temp**2) + month + bs(hum, 4) + wday\", data=bikes_daily\n).fit()\n\nprint(\n    f\" year: {mod_c_year.rsquared:.3f}\\n\",\n    f\"year + temperature: {mod_c_temp.rsquared:.3f}\\n\",\n    f\"year + temperature + months: {mod_c_month.rsquared:.3f}\\n\",\n    f\"year + temperature + months + humidity: {mod_c_hum.rsquared:.3f}\\n\",\n    f\"year + temperature + months + days of the week: {mod_c_wday.rsquared:.3f}\",\n)\n\n year: 0.057\n year + temperature: 0.554\n year + temperature + months: 0.588\n year + temperature + months + humidity: 0.660\n year + temperature + months + days of the week: 0.824\n\n\n예측값을 original scale로 다시 되돌려 R squared 계산\n\n\nR-squared with the original scale\nbikes_daily_resid2 = bikes_daily.assign(\n    pred_c_year = lambda x: 2**mod_c_year.fittedvalues - 1,\n    pred_c_temp = lambda x: 2**mod_c_temp.fittedvalues - 1,\n    pred_c_month = lambda x: 2**mod_c_month.fittedvalues - 1,\n    pred_c_hum = lambda x: 2**mod_c_hum.fittedvalues - 1,\n    pred_c_wday = lambda x: 2**mod_c_wday.fittedvalues - 1,\n)\n\nfrom sklearn.metrics import r2_score\n\nfor col in [\"pred_c_year\", \"pred_c_temp\", \"pred_c_month\", \"pred_c_hum\",  \"pred_c_wday\"]:\n    r2 = r2_score(bikes_daily_resid2[\"casual\"], bikes_daily_resid2[col])\n    print(f\"{col}: {r2:.2f}\")\n\n\npred_c_year: -0.08\npred_c_temp: 0.34\npred_c_month: 0.37\npred_c_hum: 0.42\npred_c_wday: 0.80\n\n\n\n포아송 회귀로 살펴보면,\nR-squared가 존재하지 않고, 대신 pseudo R-squared가 있음; 절편만 있는 모형 대비 log-likelihood가 얼마나 늘었는지의 비율로 구현\n\nX의 값에 따라 Y의 분포가 달라지는 경우, R-squared는 의미가 없음\n데이터를 train/test로 나누어 평가하는 것이 더 적절함\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod_c2_pois = smf.poisson(\"casual ~ (temp + I(temp**2)) + year + wday\", data=bikes_daily).fit()\n\nmod_c_pois_year = smf.poisson(\"casual ~ year\", data=bikes_daily).fit()\nmod_c_pois_temp = smf.poisson(\"casual ~ year + temp + I(temp**2)\", data=bikes_daily).fit()\nmod_c_pois_month = smf.poisson(\n    \"casual ~ year + temp + I(temp**2) + month\", data=bikes_daily\n).fit()\nmod_c_pois_hum = smf.ols(\n    \"lcasual ~ year + temp + I(temp**2) + month + bs(hum, 4)\", data=bikes_daily\n).fit()\nmod_c_pois_wday = smf.poisson(\n    \"casual ~ year + temp + I(temp**2) + month + bs(hum, 4) + wday\", data=bikes_daily\n).fit()\n\nprint(\n    f\" year: {mod_c_pois_year.prsquared:.3f}\\n\",  # prsquared: pseudo R-squared\n    f\"year + temperature: {mod_c_pois_temp.prsquared:.3f}\\n\",\n    f\"year + temperature + months: {mod_c_pois_month.prsquared:.3f}\\n\",\n    f\"year + temperature + months + humidity: {mod_c_pois_hum.rsquared:.3f}\\n\",\n    f\"year + temperature + months + days of the week: {mod_c_pois_wday.prsquared:.3f}\",\n)\n\nOptimization terminated successfully.\n         Current function value: 60.868155\n         Iterations 7\nOptimization terminated successfully.\n         Current function value: 245.676778\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 140.141516\n         Iterations 6\nOptimization terminated successfully.\n         Current function value: 130.926793\n         Iterations 7\nOptimization terminated successfully.\n         Current function value: 43.674208\n         Iterations 7\n year: 0.066\n year + temperature: 0.467\n year + temperature + months: 0.502\n year + temperature + months + humidity: 0.660\n year + temperature + months + days of the week: 0.834",
    "crumbs": [
      "Modelling",
      "Bike Sharing"
    ]
  },
  {
    "objectID": "contents/Statistics/statistics.html",
    "href": "contents/Statistics/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "통계 분석은 크게 세 가지 주제로 나눌 수 있음.\n현대적 접근에서는 모호한 모집단에 대해 추론하기보다는 표본 내의 정보만으로 관계를 파악하고자 함.\n이는 관계가 특정 표본에 너무 overfit되지 않도록 하여 (cross validation과 같은 방법을 이용), 다른 표본에서도 그 관계가 유지되도록 하기 위한 것이고, 전통적인 모집단에 대해 추론하는 것과 유사하다고 볼 수 있음."
  },
  {
    "objectID": "contents/Statistics/statistics.html#simple-regressioncorrelation",
    "href": "contents/Statistics/statistics.html#simple-regressioncorrelation",
    "title": "Statistics",
    "section": "Simple Regression/Correlation",
    "text": "Simple Regression/Correlation\n예측변수가 한 개인 경우,\n\n두 변수 간의 관계(association)을 파악: \\(Y=f(X)\\)\n\n선형인 경우 기울기를 의미\n\n그 관계의 크기(strength)를 측정\n\n\\(f\\)에 의해 \\(X\\)로 \\(Y\\)를 얼마나 정확히 예측할 수 있는가?\n\\(f\\)에 의해 \\(X\\)의 변량이 \\(Y\\)의 변량을 얼마나 설명할 수 있는가?\n\n\n\n\n\n\n\n\n\nPearson’s correlation coefficient: \\(r\\)\nLinear relationships을 측정\n\nx와 y의 선형적 연관성: [-1, 1]\n\nx로부터 y를 얼마나 정확히 예측가능한가?\nx와 y의 정보는 얼마나 중복(redundant)되는가?\n\n\n\n\n\n\n\n\n\nMultiple correlation coefficient: \\(R\\)\nExtented correlation: 예측치와 관측치의 pearson’s correlation\n\n\\(R\\)을 제곱한 \\(R^2\\)가 설명력의 정도를 나타냄\n\n\n\n\n\n\n\\(R\\): Multiple correlation coefficient\n\n\\(Y\\) 와 \\(\\widehat Y\\) 의 correlation 즉, Y와 회귀모형이 예측한 값의 (선형적) 상관 관계의 정도; 회귀모형의 예측의 정확성\n\n다시말하면, 예측변수들의 최적의 선형 조합과 Y의 상관 관계의 정도.\n\n\\(R^2\\): Coefficient of determination, 결정계수, 설명력\n\n(평면의) 선형모형에 의해 설명된 Y 변량의 비율:\n\n또는 예측변수들의 최적의 선형 조합에 의해 설명된 Y 변량의 비율.\n\n  즉, \\(\\displaystyle\\frac{V(\\widehat{Y})}{V(Y)}\\) 또는 \\(\\displaystyle 1 - \\frac{V(e)}{V(Y)}\\)\n\nAssociatiions과 그 strengths 비교"
  },
  {
    "objectID": "contents/Statistics/statistics.html#multiple-regression",
    "href": "contents/Statistics/statistics.html#multiple-regression",
    "title": "Statistics",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n예측변수가 2개 이상인 경우:\n변수들 간의 진실한 관계를 분석\n미혼자에 대한 임금 차별이 있는가? 차별이 의미하는 바는 무엇인가?\n아래 첫번째 그림과 같이 기혼자의 임금이 미혼자에 보다 높은 것으로 나타났다면,\n이는 정말 결혼하지 않은 것이 임금을 책정하는데 영향을 주었는가?\n하지만, 당연하게도 기혼자는 미혼자에 비해 연령이 높으며 (두번째 그림),\n높은 연령은 연차가 높거나 실무능력이 뛰어난 경향으로 인해 임금을 높을 수 있다는 것을 감안하면 (세번째 그림)\n차별처럼 보이는 차이는 차별이라고 볼 수 없을 수도 있음.\n다시 말하면, 연령을 고려한 후에도 기혼자의 임금은 미혼자보다 높은가?\n여전히 높다면, 연령을 고려한 후 혹은 연령을 조정한 후(adjusted for age)의 차이는 얼마라고 봐야하는가?\n연령을 고려한 임금 차이를 조사하는 방법은 무엇이 있겠는가?\n\n연령별로 나누어 비교?\n\nData from the 1985 Current Population Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n연령을 고려한 마라톤 기록?\n70세 노인의 기록 2시간 30분과 20세 청년의 2시간 30분은 마라톤 실력이라는 관점에서 다르게 볼 수도 있음\n예를 들어, “나이 차이가 큰 두 사람의 기록을 비교하는 것은 공평하지 않아”라는 주장에 대해서, 70세 노인의 기록은 “나이를 감안하면 2시간 10분에 해당한다”고 답변할 수 있음\n다시 말하면, 나이와는 무관한/독립적인 마라톤 능력에 대해 말할 수 있음\n이는 동일한 나이의 사람들로만 제한해서 마라톤 기록을 비교하는 것이 공평한 능력의 비교라고 말하는 것과 것이 같은 이치임\n\nSource: https://doi.org/10.1186/2052-1847-6-31\n기혼여부에 따른 임금의 차이가 남녀별로 다른가?\n연령이 올라감에 따라 임금이 올라가는 패턴에 차이가 있는가?\n\n\n\n\n\n\n\n\n\n\n\n\n\n왼편 그림에서 보면, 기혼여부에 따른 임금의 차이가 남녀에 따라 다르게 나타나는 것으로 보임\n이러한 현상을 변수 간에 상호작용(interaction)이 있다고 말함 (moderate라는 표현도 있음)\n말하지면, 기혼여부가 임금에 주는 효과가 성별에 따라 바뀌고, 기혼여부와 성별이 상호작용하여 임금에 영향을 준다라고 표현할 수 있음 (2-way interaction)\n비슷하게, 오른편을 보면, 연령에 따른 임금의 증가 패턴이 남녀에 따라서, 업종에 따라 다르게 나타나는 것으로 보임\n(manag: management, manuf: manufacturing, prof: professional)\n즉, 연령이 임금에 미치는 효과는 성별과 업종에 따라 바뀌고, 연령, 성별, 업종이 상호작용하여 임금에 영향을 준다라고 표현할 수 있음 (3-way interaction)\n\n\n\n\n\n\nWarning\n\n\n\n위의 표현은 모두 효과를 가정한 표현으로 설명을 위해 편의상 그렇게 표현하였음\n또한, 다른 요소들은 단순화를 위해 생략했음. 예를 들어 왼편의 상황에서 나이를 고려하면 다른 양상을 보일 수 있음"
  },
  {
    "objectID": "contents/Statistics/statistics.html#regression-analysis",
    "href": "contents/Statistics/statistics.html#regression-analysis",
    "title": "Statistics",
    "section": "Regression analysis",
    "text": "Regression analysis\n예측 모형 vs. 인과 모형\n\n인과적 연관성을 탐구하고자 한다면 매우 신중한 접근을 요함.\n\n\nCase 1\nSource: Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.)\n교수의 연봉(salary)이 학위를 받은 후 지난 시간(time since Ph.D.)과 출판물의 수(pubs)에 의해 어떻게 영향을 받는가?\n\nData: c0301dt.csv\n\nacad0 = pd.read_csv(\"data/c0301dt.csv\")\nacad0.head(5)\n\n   time  pubs  salary\n0     3    18   51876\n1     6     3   54511\n2     3     2   53425\n3     8    17   61863\n4     9    11   52926\n\n\n\nimport statsmodels.formula.api as smf\n\nmod1 = smf.ols(\"salary ~ time\", data=acad0).fit()\nmod2 = smf.ols(\"salary ~ pubs\", data=acad0).fit()\nmod3 = smf.ols(\"salary ~ time + pubs\", data=acad0).fit()\n\n\n\n\n\n\n\nIntercept   43658.59\ntime         1224.39\ndtype: float64\n\n\nIntercept   46357.45\npubs          335.53\ndtype: float64\n\n\nIntercept   43082.39\ntime          982.87\npubs          121.80\ndtype: float64\n\n\n\n세 모형을 비교하면,\nModel 1: \\(\\widehat{salary} = \\$1,224\\:time + \\$43,659\\)\nModel 2 : \\(\\widehat{salary} = \\$336\\:pubs + \\$46,357\\)\nModel 3: \\(\\widehat{salary} = \\$983\\:time + \\$122\\:pubs + \\$43,082\\)\n\n연차(time)의 효과는 $1,224에서 $984로 낮아졌고,\n논문수(pubs)의 효과는 $336에서 $122로 낮아졌음.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n교수들의 연차와 그들이 쓴 논문 수는 깊이 연관되어 있으며 (r = 0.66), 두 변수의 redunancy가 각 변수들의 효과를 변화시킴.\n두 예측 변수의 산술적 합으로 연봉을 예측하므로 각 예측변수의 효과는 수정될 수 밖에 없음.\n수학적으로 보면, 각 예측변수의 기울기는 다른 예측변수의 값에 상관없이 일정하므로, 다른 예측변수들을 (임의의 값에) 고정시키는 효과를 가짐\n즉, 다른 변수와는 독립적인, 고유한 효과를 추정하게 됨\n\n각 회귀계수를 partial regression coefficient (부분 회귀 계수) 라고 부름.\n부분 회귀 계수의 첫번째 해석:\n\n만약 논문 수가 일정할 때, 예를 들어 10편의 논문을 쓴 경우만 봤을 때, 연차가 1년 늘 때마다 연봉은 $984 증가함; 평면의 선형모형을 가정했기에 이 관계는 논문 수에 상관없음.\n\n연차가 일정할 때, 예를 들어 연차가 12년차인 경우만 봤을 때, 논문이 1편 늘 때마다 연봉은 $336 증가함; 평면의 선형모형을 가정했기에 이 관계는 연차에 상관없음.\n\n이는 다른 변수를 고려 (통제, controlling for) 했을 때 혹은 다른 변수의 효과를 제거 (partial out) 했을 때, 각 변수의 고유한 효과를 의미함; holding constant, controlling for, partialing out, adjusted for, residualizing\n뒤집어 말하면, 연차만 고려했을때 연차가 1년 늘면 $1,224 연봉이 증가하는 효과는 연차가 늘 때 함께 늘어나는 논문 수의 효과가 함께 섞여 나온 효과라고 말할 수 있음.\n이는 인과관계에 있는 변수들의 진정한 효과를 찾는 것이 얼마나 어려운지를 보여줌\n부분 회귀 계수에 대한 두번째 해석\n\n다른 변수들이 partial out 된 후의 효과.\n\n실제로 $122는 연차로 (선형적으로) 예측/설명되지 않는 논문수(residuals)로 [연차로 예측/설명되지 않는] 연봉을 예측할 때의 기울기\n\n  \n\nDirect and Indirect Effects\n만약, 다음과 같은 인과모형을 세운다면,\n\n\n연차가 연봉에 미치는 효과가 두 경로로 나뉘어지고,\n연차 \\(\\rightarrow\\) 연봉: 직접효과 $983\n연차 \\(\\rightarrow\\) 논문 \\(\\rightarrow\\) 연봉: 간접효과 1.98 x $122 = $241.56\n두 효과를 더하면: $983 + $241.56 = $1224.56 = 논문수를 고려하지 않았을 때 연차의 효과\n\n즉, 연차가 1년 늘때 연봉이 $1224 증가하는 것은 연차 자체의 효과($983)와 논문의 증가에 따른 효과($241)가 합쳐져 나온 결과라고 말할 수 있음.\n\n이 때, 논문의 수가 연차와 연봉의 관계를 매개(mediate)한다고 표현.\n\n만약, 연차의 효과 $1224이 논문수를 고려했을 때 줄어든($983) 수준을 훨씬 넘어 통계적으로 유의하지 않을 정도로 0에 가까워진다면, 연차의 효과는 모두 논문의 효과를 거쳐 나타나는 것이라고 말할 수 있음. 이 때, 완전 매개 (fully mediate)한다고 표현함.\n\n이들는 인과관계의 기제/메커니즘의 일부를 설명해 줌.\n반대로, 만약 다음과 같이 논문의 효과가 거의 사라진다면, 논문의 효과는 가짜 효과, spurious effect라고 표현함. 이는 논문과 연봉 간의 관계가 보이는 이유는 연차라는 common cause가 연결하고 있기 때문임. 이를 confounding이라고 함."
  },
  {
    "objectID": "contents/Transform/combine.html",
    "href": "contents/Transform/combine.html",
    "title": "Combine",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Combine"
    ]
  },
  {
    "objectID": "contents/Transform/combine.html#keys-두-쌍의-테이블을-연결하는데-사용되는-변수",
    "href": "contents/Transform/combine.html#keys-두-쌍의-테이블을-연결하는데-사용되는-변수",
    "title": "Combine",
    "section": "Keys: 두 쌍의 테이블을 연결하는데 사용되는 변수",
    "text": "Keys: 두 쌍의 테이블을 연결하는데 사용되는 변수\n\nprimary key ~ foreign key: relation\n\nA primary key: a variable or set of variables that uniquely identifies each observation.\n\nplanes 데이터셋의 tailnum,\nairports 데이터셋의 faa\n\nA foreign key: a variable or set of variables that corresponds to a primary key in another table.\n\nflights 데이터셋의 tailnum in relation to planes 데이터셋의 tailnum,\nflights 데이터셋의 origin in relation to airports 데이터셋의 faa\n\n\nrelation은 1-1, 1-many 일수 있음",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Combine"
    ]
  },
  {
    "objectID": "contents/Transform/combine.html#primary-key를-확인하는-방법",
    "href": "contents/Transform/combine.html#primary-key를-확인하는-방법",
    "title": "Combine",
    "section": "Primary key를 확인하는 방법",
    "text": "Primary key를 확인하는 방법\nplanes의 경우 tailnum가 primary key?\n\nplanes.duplicated(subset=\"tailnum\").sum()\n# 또는\n(planes.value_counts(\"tailnum\") &gt; 1).sum()\n\n0\n\n\nweather의 경우 네 변수 year, month, day, hour, origin의 조합이 primary key일 수 있으나…\n\n(\n    weather[[\"year\", \"month\", \"day\", \"hour\", \"origin\"]]\n    .value_counts()\n    .reset_index(name=\"n\")\n    .query(\"n &gt; 1\")\n)\n\n   year  month  day  hour origin  n\n0  2013     11    3     1    EWR  2\n1  2013     11    3     1    JFK  2\n2  2013     11    3     1    LGA  2\n\n\nprimary key가 되려면 다음과 같이 불명확한 정보를 처리한 후 사용\n\nweather[\n    weather.duplicated(\n        subset=[\"year\", \"month\", \"day\", \"hour\", \"origin\"], keep=False\n    )  # keep=False: 중복된 모든 행을 True로 표시\n]\n\n      origin  year  month  day  hour  temp  dewp  humid  wind_dir  wind_speed  \\\n7318     EWR  2013     11    3     1 51.98 39.02  61.15    310.00        6.90   \n7319     EWR  2013     11    3     1 50.00 39.02  65.80    290.00        5.75   \n16023    JFK  2013     11    3     1 53.96 37.94  54.51    320.00        9.21   \n16024    JFK  2013     11    3     1 51.98 37.94  58.62    310.00        6.90   \n24729    LGA  2013     11    3     1 55.04 39.02  54.67    330.00        9.21   \n24730    LGA  2013     11    3     1 53.96 39.92  58.89    310.00        8.06   \n\n       wind_gust  precip  pressure  visib                 time_hour  \n7318         NaN    0.00   1009.80  10.00 2013-11-03 01:00:00-04:00  \n7319         NaN    0.00   1010.50  10.00 2013-11-03 01:00:00-05:00  \n16023        NaN    0.00   1009.80  10.00 2013-11-03 01:00:00-04:00  \n16024        NaN    0.00   1010.50  10.00 2013-11-03 01:00:00-05:00  \n24729        NaN    0.00   1009.30  10.00 2013-11-03 01:00:00-04:00  \n24730        NaN    0.00   1010.20  10.00 2013-11-03 01:00:00-05:00  \n\n\nflights에 primary key가 있는가?\n\nflights.duplicated(subset=[\"year\", \"month\", \"day\", \"flight\"]).sum()\n\n32610\n\n\n\nflights.duplicated(subset=[\"year\", \"month\", \"day\", \"flight\", \"carrier\"]).sum()\n\n24\n\n\n\nflights.duplicated(subset=[\"year\", \"month\", \"day\", \"flight\", \"carrier\", \"sched_dep_time\"]).sum()\n\n0\n\n\n단, 중복된 값이 없다는 것이 좋은 primary key가 된다는 보장은 없음.\n위의 경우 (날짜/시간, 항공편 번호, 항공사)의 정보가 unique한 것이 자연스러움.",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Combine"
    ]
  },
  {
    "objectID": "contents/Transform/combine.html#merge",
    "href": "contents/Transform/combine.html#merge",
    "title": "Combine",
    "section": "Merge",
    "text": "Merge\nInner join\n\ndf_x = pd.DataFrame({\"key\": [1, 2, 3], \"val_x\": [\"x1\", \"x2\", \"x3\"]})\ndf_y = pd.DataFrame({\"key\": [1, 2, 4], \"val_y\": [\"y1\", \"y2\", \"y3\"]})\n\n\n\n\n\n\n\n   key val_x\n0    1    x1\n1    2    x2\n2    3    x3\n\n\n   key val_y\n0    1    y1\n1    2    y2\n2    4    y3\n\n\n\n\n\npd.merge(df_x, df_y, on=\"key\")  # 공통의 column이 있을 시 \"on=\" 생략 가능\n\n   key val_x val_y\n0    1    x1    y1\n1    2    x2    y2\n\n\n\ndf_x.merge(df_y, on=\"key\")  # as a method\n\n   key val_x val_y\n0    1    x1    y1\n1    2    x2    y2\n\n\n\n\n\n\n\n\nNote\n\n\n\nmerge()는 default로 inner 방식으로 join하고, how=\"inner\"가 위에서 생략되었고, 다음과 동일\npd.merge(df_x, df_y, on=\"key\", how=\"inner\")\n다른 방식으로는 “left”, “right”, “outer”가 있음\n\nleft keeps all observations in x. : 가장 흔하게 쓰는 join. 기준이 되는 데이터가 존재\nright keeps all observations in y.\nouter keeps all observations in x and y.\n\n\n\n\n\n\npd.merge(df_x, df_y, how=\"left\")\n\n   key val_x val_y\n0    1    x1    y1\n1    2    x2    y2\n2    3    x3   NaN\n\n\n\npd.merge(df_x, df_y, how=\"right\")\n\n   key val_x val_y\n0    1    x1    y1\n1    2    x2    y2\n2    4   NaN    y3\n\n\n\npd.merge(df_x, df_y, how=\"outer\")\n\n   key val_x val_y\n0    1    x1    y1\n1    2    x2    y2\n2    3    x3   NaN\n3    4   NaN    y3\n\n\n\nDuplicate keys\n한쪽만 중복이 있는 경우\n\n\nflights2 = flights[\n    [\"year\", \"month\", \"day\", \"hour\", \"origin\", \"dest\", \"tailnum\", \"carrier\"]\n]\nflights2\n\n        year  month  day  hour origin dest tailnum carrier\n0       2013      1    1     5    EWR  IAH  N14228      UA\n1       2013      1    1     5    LGA  IAH  N24211      UA\n2       2013      1    1     5    JFK  MIA  N619AA      AA\n...      ...    ...  ...   ...    ...  ...     ...     ...\n336773  2013      9   30    12    LGA  BNA  N535MQ      MQ\n336774  2013      9   30    11    LGA  CLE  N511MQ      MQ\n336775  2013      9   30     8    LGA  RDU  N839MQ      MQ\n\n[336776 rows x 8 columns]\n\n\n\n# flights에 항공사의 full name을 추가하고자 할때,\n(\n    flights2\n    .merge(airlines, on=\"carrier\", how=\"left\")\n)\n\n        year  month  day  hour origin dest tailnum carrier  \\\n0       2013      1    1     5    EWR  IAH  N14228      UA   \n1       2013      1    1     5    LGA  IAH  N24211      UA   \n2       2013      1    1     5    JFK  MIA  N619AA      AA   \n...      ...    ...  ...   ...    ...  ...     ...     ...   \n336773  2013      9   30    12    LGA  BNA  N535MQ      MQ   \n336774  2013      9   30    11    LGA  CLE  N511MQ      MQ   \n336775  2013      9   30     8    LGA  RDU  N839MQ      MQ   \n\n                          name  \n0        United Air Lines Inc.  \n1        United Air Lines Inc.  \n2       American Airlines Inc.  \n...                        ...  \n336773               Envoy Air  \n336774               Envoy Air  \n336775               Envoy Air  \n\n[336776 rows x 9 columns]\n\n\n두 쪽 모두 중복이 있는 경우: 조심!\ncan’t uniquely identify an observation; 가능한 모든 조합이 나타남\n\n\n\nDefining the key columns\nThe default, uses all variables that appear in both tables, the so called  natural join.\n\nflights2.merge(weather, how=\"left\").head(3)  # on=[\"year\", \"month\", \"day\", \"hour\", \"origin\"]\n\n   year  month  day  hour origin dest tailnum carrier  temp  dewp  humid  \\\n0  2013      1    1     5    EWR  IAH  N14228      UA 39.02 28.04  64.43   \n1  2013      1    1     5    LGA  IAH  N24211      UA 39.92 24.98  54.81   \n2  2013      1    1     5    JFK  MIA  N619AA      AA 39.02 26.96  61.63   \n\n   wind_dir  wind_speed  wind_gust  precip  pressure  visib  \\\n0    260.00       12.66        NaN    0.00   1011.90  10.00   \n1    250.00       14.96      21.86    0.00   1011.40  10.00   \n2    260.00       14.96        NaN    0.00   1012.10  10.00   \n\n                  time_hour  \n0 2013-01-01 05:00:00-05:00  \n1 2013-01-01 05:00:00-05:00  \n2 2013-01-01 05:00:00-05:00  \n\n\nflights의 year와 planes의 year는 다른 의미의 year임\n중복된 이름은 따로 표기\n\n# year_x, year_y로 구분되어 표시\nflights2.merge(planes, on=\"tailnum\", how=\"left\").head(3)\n\n   year_x  month  day  hour origin dest tailnum carrier  year_y  \\\n0    2013      1    1     5    EWR  IAH  N14228      UA 1999.00   \n1    2013      1    1     5    LGA  IAH  N24211      UA 1998.00   \n2    2013      1    1     5    JFK  MIA  N619AA      AA 1990.00   \n\n                      type manufacturer    model  engines  seats  speed  \\\n0  Fixed wing multi engine       BOEING  737-824     2.00 149.00    NaN   \n1  Fixed wing multi engine       BOEING  737-824     2.00 149.00    NaN   \n2  Fixed wing multi engine       BOEING  757-223     2.00 178.00    NaN   \n\n      engine  \n0  Turbo-fan  \n1  Turbo-fan  \n2  Turbo-fan  \n\n\nairports 데이터프레임에서 공항이름이 faa라는 이름의 column으로 존재.\n도착지(dest)의 공항정보를 얻으려면, faa와 dest를 매치시키고,\n출발지(origin)의 공항정보를 얻으려면, faa와 origin를 매치시켜야 함.\n\nairports.head(3)\n\n   faa                           name   lat    lon   alt  tz dst  \\\n0  04G              Lansdowne Airport 41.13 -80.62  1044  -5   A   \n1  06A  Moton Field Municipal Airport 32.46 -85.68   264  -6   A   \n2  06C            Schaumburg Regional 41.99 -88.10   801  -6   A   \n\n              tzone  \n0  America/New_York  \n1   America/Chicago  \n2   America/Chicago  \n\n\n\nflights2.merge(airports, left_on=\"dest\", right_on=\"faa\", how=\"left\").head(5)\n\n   year  month  day  hour origin dest tailnum carrier  faa  \\\n0  2013      1    1     5    EWR  IAH  N14228      UA  IAH   \n1  2013      1    1     5    LGA  IAH  N24211      UA  IAH   \n2  2013      1    1     5    JFK  MIA  N619AA      AA  MIA   \n3  2013      1    1     5    JFK  BQN  N804JB      B6  NaN   \n4  2013      1    1     6    LGA  ATL  N668DN      DL  ATL   \n\n                              name   lat    lon     alt    tz  dst  \\\n0     George Bush Intercontinental 29.98 -95.34   97.00 -6.00    A   \n1     George Bush Intercontinental 29.98 -95.34   97.00 -6.00    A   \n2                       Miami Intl 25.79 -80.29    8.00 -5.00    A   \n3                              NaN   NaN    NaN     NaN   NaN  NaN   \n4  Hartsfield Jackson Atlanta Intl 33.64 -84.43 1026.00 -5.00    A   \n\n              tzone  \n0   America/Chicago  \n1   America/Chicago  \n2  America/New_York  \n3               NaN  \n4  America/New_York  \n\n\n\nflights2.merge(airports, left_on=\"origin\", right_on=\"faa\", how=\"left\").head(5)\n\n   year  month  day  hour origin dest tailnum carrier  faa  \\\n0  2013      1    1     5    EWR  IAH  N14228      UA  EWR   \n1  2013      1    1     5    LGA  IAH  N24211      UA  LGA   \n2  2013      1    1     5    JFK  MIA  N619AA      AA  JFK   \n3  2013      1    1     5    JFK  BQN  N804JB      B6  JFK   \n4  2013      1    1     6    LGA  ATL  N668DN      DL  LGA   \n\n                  name   lat    lon  alt  tz dst             tzone  \n0  Newark Liberty Intl 40.69 -74.17   18  -5   A  America/New_York  \n1           La Guardia 40.78 -73.87   22  -5   A  America/New_York  \n2  John F Kennedy Intl 40.64 -73.78   13  -5   A  America/New_York  \n3  John F Kennedy Intl 40.64 -73.78   13  -5   A  America/New_York  \n4           La Guardia 40.78 -73.87   22  -5   A  America/New_York",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Combine"
    ]
  },
  {
    "objectID": "contents/Transform/combine.html#concatenate",
    "href": "contents/Transform/combine.html#concatenate",
    "title": "Combine",
    "section": "Concatenate",
    "text": "Concatenate\npd.concat([df1, df2, ...], axis=)\n행과 열의 index를 매치시켜 두 DataFrame/Series를 합침\n\ndf1 = pd.DataFrame(\n    np.arange(6).reshape(3, 2), index=[\"a\", \"b\", \"c\"], columns=[\"one\", \"two\"]\n)\ndf2 = pd.DataFrame(\n    5 + np.arange(4).reshape(2, 2), index=[\"a\", \"c\"], columns=[\"three\", \"four\"]\n)\n\n\n\n\n\n\n\n   one  two\na    0    1\nb    2    3\nc    4    5\n\n\n   three  four\na      5     6\nc      7     8\n\n\n\n\npd.concat([df1, df2], axis=1)\n\n   one  two  three  four\na    0    1   5.00  6.00\nb    2    3    NaN   NaN\nc    4    5   7.00  8.00\n\n\n\npd.concat([df1, df2])  # default: axis=0\n\n   one  two  three  four\na 0.00 1.00    NaN   NaN\nb 2.00 3.00    NaN   NaN\nc 4.00 5.00    NaN   NaN\na  NaN  NaN   5.00  6.00\nc  NaN  NaN   7.00  8.00",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Combine"
    ]
  },
  {
    "objectID": "contents/Transform/combine.html#merge-problems",
    "href": "contents/Transform/combine.html#merge-problems",
    "title": "Combine",
    "section": "Merge problems:",
    "text": "Merge problems:\nmerge은 매우 조심스러운 작업!\n\nStart by identifying the variables that form the primary key in each table.\n\nYou should usually do this based on your understanding of the data, not empirically by looking for a combination of variables that give a unique identifier.\n\n# 확인작업은 기본\nweather[\n weather.duplicated(subset=[\"year\", \"month\", \"day\", \"hour\", \"origin\"], keep=False)\n]\n#       origin  year  month  day  hour  temp  dewp  humid  wind_dir  wind_speed  ...\n# 7318     EWR  2013     11    3     1 51.98 39.02  61.15    310.00        6.90   \n# 7319     EWR  2013     11    3     1 50.00 39.02  65.80    290.00        5.75   \n# 16023    JFK  2013     11    3     1 53.96 37.94  54.51    320.00        9.21   \n# 16024    JFK  2013     11    3     1 51.98 37.94  58.62    310.00        6.90   \n# 24729    LGA  2013     11    3     1 55.04 39.02  54.67    330.00        9.21   \n# 24730    LGA  2013     11    3     1 53.96 39.92  58.89    310.00        8.06 \nCheck that none of the variables in the primary key are missing. If a value is missing then it can’t identify an observation!\nCheck that your foreign keys match primary keys in another table.\n\nIt’s common for keys not to match because of data entry errors. Fixing these is often a lot of work.\nIf you do have missing keys, you’ll need to be thoughtful about your use of inner vs. outer joins, carefully considering whether or not you want to drop rows that don’t have a match.",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Combine"
    ]
  },
  {
    "objectID": "contents/Transform/exercise_transform.html",
    "href": "contents/Transform/exercise_transform.html",
    "title": "Exercises",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n# Load the nycflight13 dataset\nflights = sm.datasets.get_rdataset(\"flights\", \"nycflights13\").data.drop(columns=\"time_hour\")"
  },
  {
    "objectID": "contents/Transform/exercise_transform.html#a",
    "href": "contents/Transform/exercise_transform.html#a",
    "title": "Exercises",
    "section": "A",
    "text": "A\n다음 조건을 만족하는 항공편을 필터링 해보세요. (1~6)\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n출발할 때 예정시간보다 1시간 이상 지연되어 출발하였으나 빠르게 비행하여 출발 지연된 시간보다 도착 지연이 30분이상 단축된 항공편들입니다. (예를 들어, 1시간 늦게 출발했는데, 도착은 28분 지연된 항공편)\nDeparted between midnight and 6am (inclusive)\n\n\n\nFind the fastest flights.\nSort flights to find the most delayed flights. Find the flights that left earliest (예정시간보다 가장 일찍 출발한).\nWhich flights travelled the farthest? Which travelled the shortest?\n각 도착지 별로, 뉴욕에서 출항한 항공편이 1년 중 몇 일 있었는가?\n뉴욕에서 1년 중 300일 이상 출항하는 도착지들을 구하면?"
  },
  {
    "objectID": "contents/Transform/exercise_transform.html#b",
    "href": "contents/Transform/exercise_transform.html#b",
    "title": "Exercises",
    "section": "B",
    "text": "B\n\nOur definition of cancelled flights (dep_delay or arr_delay is missing) is slightly suboptimal. Why? Which is the most important column?\n\n예를 들어, 출발지연은 missing이 아니나 도착지연은 missing인 것이 있음\n\nLook at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the (daily) average delay?\n\n취소되는 항공편들이 많은 것과 관계 있는 것은 무엇이 있을까…\n\nWhat time of day should you fly if you want to avoid delays as much as possible?\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nFind all destinations that are flown by at least two carriers. Use that information to rank the carriers.\n\n즉, 적어도 두 항공사가 출항하는 도착지들도 한정한 후,\n다양한 곳으로 출항할수록 높은 순위의 항공사라고 보고, 항공사들의 순위를 정해봄"
  },
  {
    "objectID": "contents/Transform/exercise_transform.html#c",
    "href": "contents/Transform/exercise_transform.html#c",
    "title": "Exercises",
    "section": "C",
    "text": "C\nChallenges:\n\nWhich carrier has the worst arrival delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not?\n\n항공사(carrier)마다 취항하는 곳에 차이가 날 수 있다면, 그건 그 노선 혹은 공항의 문제이지 항공사의 문제는 아닐 수도 있음을 암시하는 것임\n\nWhich plane (tailnum) has the worst on-time record?\n\non-time을 적절히 정의한 후에 진행; 여러 방식이 있을 수 있음\n예를 들어, 늦게 도착하지 않은 항공편의 “갯수”로 보거나\n도착지연의 평균값을 기준으로 볼 수도 있음\n\nLook at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error).\n\n빠르게 비행한 이유: 제트 기류? 정체가 심한 공항?…\n같은 루트를 비행하는 항공편들 안에서 특이점이라면 의심해 볼만함…\n서로 다른 루트를 비행하는 항공편들과의 비교는?\n빠르다는 것을 비교하려면 동일한 루트에서 비교해야 적절함\n다른 루트의 항공편들까지 같이 비교하려면 어떤 방식이 있겠는가?\n\nCompute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\n\n“상대적”의 의미가 값의 차이로 볼지 비율의 차이로 볼지도 고려해 볼 것\n\n** For each plane, count the number of flights before the first delay of greater than 1 hour.\n\nnp.cumsum을 활용"
  },
  {
    "objectID": "contents/Transform/misc.html",
    "href": "contents/Transform/misc.html",
    "title": "Misc.",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Transform",
      "Misc."
    ]
  },
  {
    "objectID": "contents/Transform/misc.html#값을-대체하는-방식",
    "href": "contents/Transform/misc.html#값을-대체하는-방식",
    "title": "Misc.",
    "section": "값을 대체하는 방식",
    "text": "값을 대체하는 방식\n\nflights = pd.read_csv('../data/flights.csv')\n\n\n# np.where\nflights[\"season\"] = np.where(flights[\"month\"].isin([6, 7]), \"summer\", \"other month\")\n\n# np.where in assign()\nflights.assign(\n    season = lambda x: np.where(x.month.isin([6, 7]), \"summer\", \"other month\")\n)\n\n# apply wifh if-else or function\nflights[\"month\"].apply(lambda x: \"summer\" if x in [6, 7] else \"other month\")\n\n# map with dictionary\nflights[\"month\"].map({6: \"summer\", 7: \"summer\"}).fillna(\"other month\")\n\n# pd.eval: query expression을 활용\nflights.assign(\n    season = lambda x: np.where(pd.eval('x.month in [6, 7]'), \"summer\", \"other month\")\n)\n\n# appply with match\ndef get_season(mth):\n    match mth:\n        case 6 | 7:\n            return \"summer\"\n        case _:\n            return \"other month\"\n\nflights[\"month\"].apply(get_season)\n\n0         other month\n1         other month\n2         other month\n             ...     \n336773    other month\n336774    other month\n336775    other month\nName: month, Length: 336776, dtype: object",
    "crumbs": [
      "Transform",
      "Misc."
    ]
  },
  {
    "objectID": "contents/Transform/misc.html#vectorised-operation의-효율성",
    "href": "contents/Transform/misc.html#vectorised-operation의-효율성",
    "title": "Misc.",
    "section": "Vectorised operation의 효율성",
    "text": "Vectorised operation의 효율성\n\nnames = pd.read_csv(\"../data/babynames.csv\")\ndf = names[:100_000]\n\n\nvectorised operation vs. apply()\n값을 대체할 때: np.where\n\nimport time\n\n# np.where\nstart_time = time.time()\n\nnames[\"births3\"] = np.where(names[\"births\"] &lt; 100, 0, names[\"births\"])\n\nend_time = time.time()\ndiff_x = end_time - start_time\nprint(f\"vectorized: {diff_x}\")\n\n# apply\nstart_time = time.time()\n\nnames[\"births2\"] = names[\"births\"].apply(lambda x: 0 if x &lt; 100 else x)\n\nend_time = time.time()\ndiff_y = end_time - start_time\nprint(f\"apply: {diff_y}\")\n\nprint(f\"ratio: {diff_y / diff_x}\")\n\nvectorized: 0.01050114631652832\napply: 0.3479013442993164\nratio: 33.12984447723919\napply: 0.3479013442993164\nratio: 33.12984447723919\n\n\n\n\nvectorised vs. apply()\n비율을 구할 때\n\nstart_time = time.time()\n\ndf[\"births\"] / df.groupby([\"name\", \"sex\"])[\"births\"].transform(\"sum\")\n\nend_time = time.time()\ndiff_x = end_time - start_time\nprint(f\"vectorized: {diff_x}\")\n\nstart_time = time.time()\n\ndf.groupby([\"name\", \"sex\"])[\"births\"].apply(lambda x: x / x.sum())\n\nend_time = time.time()\ndiff_y = end_time - start_time\nprint(f\"apply: {diff_y}\")\n\nprint(f\"ratio: {diff_y / diff_x}\")\n\nvectorized: 0.033573150634765625\napply: 3.183194875717163\nratio: 94.81370724917623",
    "crumbs": [
      "Transform",
      "Misc."
    ]
  },
  {
    "objectID": "contents/Transform/pivot.html",
    "href": "contents/Transform/pivot.html",
    "title": "Pivot",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\nTidy data의 요건\n아래의 데이터는 모두 같은 4가지 정보, country, year, cases, population을 담고 있으나 table1만 tidy\n# tidy data\ntable1 = sm.datasets.get_rdataset(\"table1\", \"tidyr\").data\ntable1\n\n       country  year   cases  population\n0  Afghanistan  1999     745    19987071\n1  Afghanistan  2000    2666    20595360\n2       Brazil  1999   37737   172006362\n3       Brazil  2000   80488   174504898\n4        China  1999  212258  1272915272\n5        China  2000  213766  1280428583\ntable2 = sm.datasets.get_rdataset(\"table2\", \"tidyr\").data\ntable2\n\n        country  year        type       count\n0   Afghanistan  1999       cases         745\n1   Afghanistan  1999  population    19987071\n2   Afghanistan  2000       cases        2666\n..          ...   ...         ...         ...\n9         China  1999  population  1272915272\n10        China  2000       cases      213766\n11        China  2000  population  1280428583\n\n[12 rows x 4 columns]\ntable3 = sm.datasets.get_rdataset(\"table3\", \"tidyr\").data\ntable3\n\n       country  year               rate\n0  Afghanistan  1999       745/19987071\n1  Afghanistan  2000      2666/20595360\n2       Brazil  1999    37737/172006362\n3       Brazil  2000    80488/174504898\n4        China  1999  212258/1272915272\n5        China  2000  213766/1280428583\ntable4a = sm.datasets.get_rdataset(\"table4a\", \"tidyr\").data\ntable4a\n\n       country    1999    2000\n0  Afghanistan     745    2666\n1       Brazil   37737   80488\n2        China  212258  213766\ntable4b = sm.datasets.get_rdataset(\"table4b\", \"tidyr\").data\ntable4b\n\n       country        1999        2000\n0  Afghanistan    19987071    20595360\n1       Brazil   172006362   174504898\n2        China  1272915272  1280428583\n기본적으로 table1의 형태일 때, 효과적으로 데이터를 다룰 수 있음\n# Compute rate per 10,000\ntable1.assign(\n    rate = lambda x: x.cases / x.population * 10_000\n)\n\n       country  year   cases  population  rate\n0  Afghanistan  1999     745    19987071  0.37\n1  Afghanistan  2000    2666    20595360  1.29\n2       Brazil  1999   37737   172006362  2.19\n3       Brazil  2000   80488   174504898  4.61\n4        China  1999  212258  1272915272  1.67\n5        China  2000  213766  1280428583  1.67\n# Compute cases per year\ntable1.groupby(\"year\")[\"cases\"].sum()\n\nyear\n1999    250740\n2000    296920\nName: cases, dtype: int64\n# Visualise changes over time\n(\n    so.Plot(table1, x=\"year\", y=\"cases\")\n    .add(so.Line(), color=\"country\")\n)",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Pivot"
    ]
  },
  {
    "objectID": "contents/Transform/pivot.html#long-form",
    "href": "contents/Transform/pivot.html#long-form",
    "title": "Pivot",
    "section": "Long Form",
    "text": "Long Form\nmelt()\nstack(): index에 적용\n\ntable4a\n\n       country    1999    2000\n0  Afghanistan     745    2666\n1       Brazil   37737   80488\n2        China  212258  213766\n\n\n\n\ntable4a_long = table4a.melt(\n    id_vars=\"country\",            # 고정할 컬럼\n    value_vars=[\"1999\", \"2000\"],  # value로 사용할 컬럼\n    var_name=\"year\",              # 컬럼에 있는 정보에 대한 변수명\n    value_name=\"cases\"            # value에 있는 정보에 대한 변수명\n)\n\ntable4a_long\n\n       country  year   cases\n0  Afghanistan  1999     745\n1       Brazil  1999   37737\n2        China  1999  212258\n3  Afghanistan  2000    2666\n4       Brazil  2000   80488\n5        China  2000  213766\n\n\n\n\n\n\n\n\nNote\n\n\n\n생략시 모든 컬럼을 value_vars로 사용.\n컬럼 이름은 각각 variable, value로 나타남.\ntable4a_long = table4a.melt(id_vars=\"country\")  # 또는 pd.melt() 이용\n#        country variable   value\n# 0  Afghanistan     1999     745\n# 1       Brazil     1999   37737\n# 2        China     1999  212258\n# 3  Afghanistan     2000    2666\n# 4       Brazil     2000   80488\n# 5        China     2000  213766\n\n\n\n\n\n\n\n\nNote\n\n\n\nstack()은 index를 이용해 long form으로 변환시켜 줌. (컬럼 전체가 stack됨)\ntable4a.set_index(\"country\").stack()  # Series\n# country          \n# Afghanistan  1999       745\n#              2000      2666\n# Brazil       1999     37737\n#              2000     80488\n# China        1999    212258\n#              2000    213766\n# dtype: int64\nunstack()은 반대\ntable4a.set_index(\"country\").stack().unstack(level=0)\n# country  Afghanistan  Brazil   China\n# 1999             745   37737  212258\n# 2000            2666   80488  213766\n\n\n\n# 마찬가지로 table4b도 long format으로 변환: population의 정보\ntable4b\n\n       country        1999        2000\n0  Afghanistan    19987071    20595360\n1       Brazil   172006362   174504898\n2        China  1272915272  1280428583\n\n\n\ntable4b_long = table4b.melt(\n    id_vars=\"country\",           # 고정할 컬럼\n    value_vars=[\"1999\", \"2000\"]  # value로 사용할 컬럼\n    var_name=\"year\",             # 컬럼에 있는 정보에 대한 변수명\n    value_name=\"population\",     # value에 있는 정보에 대한 변수명\n) \ntable4b_long\n\n       country  year  population\n0  Afghanistan  1999    19987071\n1       Brazil  1999   172006362\n2        China  1999  1272915272\n3  Afghanistan  2000    20595360\n4       Brazil  2000   174504898\n5        China  2000  1280428583\n\n\n\npd.merge(table4a_long, table4b_long)  # keys: \"country\" & \"year\"\n\n       country  year   cases  population\n0  Afghanistan  1999     745    19987071\n1       Brazil  1999   37737   172006362\n2        China  1999  212258  1272915272\n3  Afghanistan  2000    2666    20595360\n4       Brazil  2000   80488   174504898\n5        China  2000  213766  1280428583",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Pivot"
    ]
  },
  {
    "objectID": "contents/Transform/pivot.html#wide-format",
    "href": "contents/Transform/pivot.html#wide-format",
    "title": "Pivot",
    "section": "Wide Format",
    "text": "Wide Format\npivot()\nunstack(): index에 적용\n\ntable2\n\n        country  year        type       count\n0   Afghanistan  1999       cases         745\n1   Afghanistan  1999  population    19987071\n2   Afghanistan  2000       cases        2666\n..          ...   ...         ...         ...\n9         China  1999  population  1272915272\n10        China  2000       cases      213766\n11        China  2000  population  1280428583\n\n[12 rows x 4 columns]\n\n\n\n\ntable2.pivot(\n    index=[\"country\", \"year\"],  # index로 고정할 컬럼\n    columns=\"type\",             # 컬럼 이름으로 들어갈 컬럼\n    values=\"count\"              # 값로 들어갈 컬럼\n)\n\ntype               cases  population\ncountry     year                    \nAfghanistan 1999     745    19987071\n            2000    2666    20595360\nBrazil      1999   37737   172006362\n            2000   80488   174504898\nChina       1999  212258  1272915272\n            2000  213766  1280428583\n\n\n\n\n\n\n\n\nNote\n\n\n\nunstack()은 index를 이용해 wide form으로 변환시켜 줌\n맨 안쪽 index level (level=2)에 default로 적용\ntable2.set_index([\"country\", \"year\", \"type\"]).unstack() \n#                    count            \n# type               cases  population\n# country     year                    \n# Afghanistan 1999     745    19987071\n#             2000    2666    20595360\n# Brazil      1999   37737   172006362\n#             2000   80488   174504898\n# China       1999  212258  1272915272\n#             2000  213766  1280428583\n\n\n\n\n\n\n\n\nNote\n\n\n\nstack(), unstack()에 대한 자세한 사항은 책을 참고: 8.3 Reshaping and Pivoting in McKinney’s\n\n\nQ: table1을 년도별로 wide form으로 변환하면?\n\n# Use .pivot()\ntable1.pivot(index=\"country\", columns=\"year\", values=[\"cases\", \"population\"])  # values: list로 입력\n\n              cases          population            \nyear           1999    2000        1999        2000\ncountry                                            \nAfghanistan     745    2666    19987071    20595360\nBrazil        37737   80488   172006362   174504898\nChina        212258  213766  1272915272  1280428583\n\n\n\n# Use .unstack() to pivot a level of the index labels\ntable1.set_index([\"country\", \"year\"]).unstack(\"year\")\n\n# table1.set_index([\"country\", \"year\"])\n\n#                    cases  population\n# country     year                    \n# Afghanistan 1999     745    19987071\n#             2000    2666    20595360\n# Brazil      1999   37737   172006362\n#             2000   80488   174504898\n# ...\n\n              cases          population            \nyear           1999    2000        1999        2000\ncountry                                            \nAfghanistan     745    2666    19987071    20595360\nBrazil        37737   80488   172006362   174504898\nChina        212258  213766  1272915272  1280428583",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Pivot"
    ]
  },
  {
    "objectID": "contents/Transform/pivot.html#separating-and-uniting",
    "href": "contents/Transform/pivot.html#separating-and-uniting",
    "title": "Pivot",
    "section": "Separating and uniting",
    "text": "Separating and uniting\n\ntable3\n\n       country  year               rate\n0  Afghanistan  1999       745/19987071\n1  Afghanistan  2000      2666/20595360\n2       Brazil  1999    37737/172006362\n3       Brazil  2000    80488/174504898\n4        China  1999  212258/1272915272\n5        China  2000  213766/1280428583\n\n\nrate에 있는 case와 population 정보를 분리\n\ntable3[\"rate\"].str.split(\"/\", expand=True)  # expand: list의 성분을 분리하여 컬럼으로 만듦\n\n        0           1\n0     745    19987071\n1    2666    20595360\n2   37737   172006362\n3   80488   174504898\n4  212258  1272915272\n5  213766  1280428583\n\n\n\ntable3[[\"cases\", \"population\"]] = \\\n    table3.pop(\"rate\").str.split(\"/\", expand=True)  # pop: 제거와 선택 동시\ntable3 \n\n       country  year   cases  population\n0  Afghanistan  1999     745    19987071\n1  Afghanistan  2000    2666    20595360\n2       Brazil  1999   37737   172006362\n3       Brazil  2000   80488   174504898\n4        China  1999  212258  1272915272\n5        China  2000  213766  1280428583\n\n\nyear를 앞 두자리와 뒤 두자리를 나누고자 하면 (seperate)\n\ntable3[\"year\"].astype(\"string\").str.extract(r'(\\d{2})(\\d{2})')\n\n    0   1\n0  19  99\n1  20  00\n2  19  99\n3  20  00\n4  19  99\n5  20  00\n\n\n\n\n\n\n\n\nNote\n\n\n\nRegular expression등의 string manipulation에 대해서는\n\nA Whirlwind Tour of Python by Jake VanderPlas\nPython for Data Analysis by Wes McKinney\n\n\n\n\n\ntable3[[\"century\", \"year\"]] = table3.pop(\"year\").astype(\"string\").str.extract(r'(\\d{2})(\\d{2})')\n\n\ntable3\n\n       country   cases  population century year\n0  Afghanistan     745    19987071      19   99\n1  Afghanistan    2666    20595360      20   00\n2       Brazil   37737   172006362      19   99\n3       Brazil   80488   174504898      20   00\n4        China  212258  1272915272      19   99\n5        China  213766  1280428583      20   00\n\n\n다시 century와 year 합치기\n\ntable3[\"year_4d\"] = table3[\"century\"].str.cat(table3[\"year\"])\ntable3\n\n       country   cases  population century year year_4d\n0  Afghanistan     745    19987071      19   99    1999\n1  Afghanistan    2666    20595360      20   00    2000\n2       Brazil   37737   172006362      19   99    1999\n3       Brazil   80488   174504898      20   00    2000\n4        China  212258  1272915272      19   99    1999\n5        China  213766  1280428583      20   00    2000",
    "crumbs": [
      "Transform",
      "Transforming II",
      "Pivot"
    ]
  },
  {
    "objectID": "contents/Transform/sol2.html",
    "href": "contents/Transform/sol2.html",
    "title": "Transform 2 Sol",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\n\n\nflights = pd.read_csv('data/flights.csv')\nairlines = pd.read_csv('data/airlines.csv')\nairports = pd.read_csv('data/airports.csv')\nplanes = pd.read_csv('data/planes.csv')\nweather = pd.read_csv('data/weather.csv')\n\n\n1.\n\n# 1. Add the location of the origin and destination (i.e. the lat and lon in airports) to flights.\n\n\nairport_location = airports[['faa', 'lat', 'lon']]\nairport_location\n\n      faa   lat    lon\n0     04G 41.13 -80.62\n1     06A 32.46 -85.68\n2     06C 41.99 -88.10\n...   ...   ...    ...\n1455  ZWI 39.74 -75.55\n1456  ZWU 38.90 -77.01\n1457  ZYP 40.75 -73.99\n\n[1458 rows x 3 columns]\n\n\n\n# Origin의 위치 정보 추가\nflights = flights.merge(airport_location, left_on='origin', right_on='faa').drop(columns=\"faa\")  # faa 컬럼은 중복되어 불필요하므로 삭제\nflights.head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1    517.00             515       2.00    830.00   \n1  2013      1    1    554.00             558      -4.00    740.00   \n2  2013      1    1    555.00             600      -5.00    913.00   \n\n   sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n0             819      11.00      UA    1545  N14228    EWR  IAH    227.00   \n1             728      12.00      UA    1696  N39463    EWR  ORD    150.00   \n2             854      19.00      B6     507  N516JB    EWR  FLL    158.00   \n\n   distance  hour  minute   lat    lon  \n0      1400     5      15 40.69 -74.17  \n1       719     5      58 40.69 -74.17  \n2      1065     6       0 40.69 -74.17  \n\n\n\n# dest의 경우 airports 테이블에 없는 값이 존재\nflights[~flights.dest.isin(airport_location.faa)].head(3)\n\n    year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n21  2013      1    1    701.00             700       1.00   1123.00   \n57  2013      1    1    913.00             918      -5.00   1346.00   \n60  2013      1    1    926.00             929      -3.00   1404.00   \n\n    sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n21            1154     -31.00      UA    1203  N77296    EWR  SJU    188.00   \n57            1416     -30.00      UA    1519  N24715    EWR  STT    189.00   \n60            1421     -17.00      B6     215  N775JB    EWR  SJU    191.00   \n\n    distance  hour  minute   lat    lon  \n21      1608     7       0 40.69 -74.17  \n57      1634     9      18 40.69 -74.17  \n60      1608     9      29 40.69 -74.17  \n\n\n\n# flights 테이블을 유지하기 위해, how=\"left\" 필요\n# suffixes 옵션 사용하면 편리\n\nflights = flights.merge(\n    airport_location,\n    left_on=\"dest\",\n    right_on=\"faa\",\n    how=\"left\",\n    suffixes=(\"_origin\", \"_dest\"),  # lat, lon이 다시 추가되어 중복되므로 suffixes 옵션 사용\n).drop(\"faa\", axis=1)\n\n\nflights.head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1    517.00             515       2.00    830.00   \n1  2013      1    1    554.00             558      -4.00    740.00   \n2  2013      1    1    555.00             600      -5.00    913.00   \n\n   sched_arr_time  arr_delay carrier  ...  origin dest air_time distance  \\\n0             819      11.00      UA  ...     EWR  IAH   227.00     1400   \n1             728      12.00      UA  ...     EWR  ORD   150.00      719   \n2             854      19.00      B6  ...     EWR  FLL   158.00     1065   \n\n   hour  minute  lat_origin  lon_origin  lat_dest  lon_dest  \n0     5      15       40.69      -74.17     29.98    -95.34  \n1     5      58       40.69      -74.17     41.98    -87.90  \n2     6       0       40.69      -74.17     26.07    -80.15  \n\n[3 rows x 22 columns]\n\n\n\n\n2.\n\n# 2. Is there a relationship between the age of a plane and its delays?\n\nplane_age = (\n    planes[[\"tailnum\", \"year\"]]\n    .merge(flights, on=\"tailnum\", how=\"right\", suffixes=(\"_plane\", \"\"))  # 두 테이블의 year의 의미가 다름\n    .assign(age=lambda x: x.year - x.year_plane)\n)\n\n\nplane_age.head(3)\n\n  tailnum  year_plane  year  month  day  dep_time  sched_dep_time  dep_delay  \\\n0  N14228     1999.00  2013      1    1    517.00             515       2.00   \n1  N39463     2012.00  2013      1    1    554.00             558      -4.00   \n2  N516JB     2000.00  2013      1    1    555.00             600      -5.00   \n\n   arr_time  sched_arr_time  ...  dest air_time  distance hour minute  \\\n0    830.00             819  ...   IAH   227.00      1400    5     15   \n1    740.00             728  ...   ORD   150.00       719    5     58   \n2    913.00             854  ...   FLL   158.00      1065    6      0   \n\n   lat_origin  lon_origin  lat_dest  lon_dest   age  \n0       40.69      -74.17     29.98    -95.34 14.00  \n1       40.69      -74.17     41.98    -87.90  1.00  \n2       40.69      -74.17     26.07    -80.15 13.00  \n\n[3 rows x 24 columns]\n\n\n\n# 항공기의 연식(age)와 출발 지연과의 관계\n(\n    so.Plot(plane_age, x='age', y='dep_delay')\n    .add(so.Dots())\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n# 출발 지연의 값이 매우 밀집해 있음: 히스토그램으로 확인\n(\n    so.Plot(plane_age.query('age &lt; 10'), x='dep_delay', color=\"age\")\n    .add(so.Line(), so.Hist(binwidth=3))\n    .facet(\"age\", wrap=5)\n    .layout(size=(10, 6))\n)\n\n\n\n\n\n\n\n\n\n# 출발 지연의 값이 매우 밀집해 있음: 히스토그램으로 확인\n(\n    so.Plot(plane_age.query('age &lt; 10'), x='dep_delay', color=\"age\")\n    .add(so.Line(), so.Hist(binwidth=1))\n    .facet(\"age\", wrap=5)\n    .layout(size=(10, 6))\n    .limit(x=(-20, 20))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(plane_age, x='dep_delay', color=\"age\")\n    .add(so.Line(), so.Hist(stat=\"proportion\", binwidth=1))\n    .limit(x=(-20, 60))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(plane_age, x='age', y='dep_delay')\n    .add(so.Dots(), so.Agg(\"mean\"))\n    .add(so.Dots(color=\"red\"), so.Agg(\"median\"))\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(plane_age.query('dep_delay &gt; 0'), x='age', y='dep_delay')\n    .add(so.Dots(), so.Agg(\"mean\"))\n    .add(so.Dots(color=\"red\"), so.Agg(\"median\"))\n)\n\n\n\n\n\n\n\n\n\nage_delay = (\n    plane_age.query('dep_delay &gt; 0 & dep_delay &lt; 100')\n    .groupby(\"age\")[\"dep_delay\"]\n    .agg([\"mean\", \"median\", \"count\"])\n    .reset_index()\n)\nage_delay\n\n     age  mean  median  count\n0   0.00 22.73   13.00   1686\n1   1.00 19.97   11.00   2761\n2   2.00 22.54   13.00   2092\n..   ...   ...     ...    ...\n43 50.00 19.88   14.50     16\n44 54.00 26.62   15.00     26\n45 57.00 14.91   13.00     11\n\n[46 rows x 4 columns]\n\n\n\n(\n    so.Plot(age_delay.query('count &gt; 30'), x='age')\n    .add(so.Dots(color=\"red\"), y=\"mean\")\n    .add(so.Dots(), y=\"median\")\n    .add(so.Line(color=\"deepskyblue\"), y=age_delay[\"count\"]/1000*3)\n)\n\n\n\n\n\n\n\n\n\n\n3.\n\n# 3. What weather conditions make it more likely to see a delay?\n\n\nflights_weather = flights.merge(weather)\n\n\n# 강수량 precipitation\n(\n    so.Plot(flights_weather, x='precip')\n    .add(so.Bar(), so.Hist())\n    .limit(y=(0, 5000))\n)\n\n\n\n\n\n\n\n\n\nfrom sbcustom import rangeplot\n(\n    rangeplot(flights_weather, \"precip\", \"dep_delay\")\n    .limit(y=(-20, 100), x=(0, 0.6))\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\n# 강수량 precipitation\nprecip = flights_weather.groupby(\"precip\")[\"dep_delay\"].agg([\"mean\", \"median\"]).reset_index()\nprecip\n\n    precip   mean  median\n0     0.00  11.37   -2.00\n1     0.01  29.80    3.00\n2     0.02  24.08    2.00\n..     ...    ...     ...\n52    0.82  94.67   36.00\n53    0.94  27.85   19.00\n54    1.21 113.11   65.50\n\n[55 rows x 3 columns]\n\n\n\n(\n    so.Plot(precip, x='precip', y=\"mean\")\n    .add(so.Line())\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(color=\"red\"), y=\"median\")\n)\n\n\n\n\n\n\n\n\n\n# 시야 visibility\n(\n    so.Plot(flights_weather, x='visib')\n    .add(so.Bar(), so.Hist())\n)\n\n\n\n\n\n\n\n\n\nrangeplot(flights_weather, \"visib\", \"dep_delay\")\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(flights_weather.query('visib &lt; 4'), x='visib', y='dep_delay')\n    .add(so.Dot(), so.Agg())\n    .add(so.Dot(color=\"red\"), so.Agg(\"median\"))\n)\n\n\n\n\n\n\n\n\n\n\n4.\n\n# 4. flights 테이블에서 하루 평균 도착지연(arr_delay)가 가장 큰 10일에 해당하는 항공편을 선택\n\n\ndelay_top = flights.groupby([\"year\", \"month\", \"day\"])[\"arr_delay\"].mean().sort_values(ascending=False).head(10)\ndelay_top\n\nyear  month  day\n2013  3      8     85.86\n      6      13    63.75\n      7      22    62.76\n                    ... \n      12     17    55.87\n      8      8     55.48\n      12     5     51.67\nName: arr_delay, Length: 10, dtype: float64\n\n\n\nflights.merge(delay_top.reset_index(name=\"daily_delay\"), on=[\"year\", \"month\", \"day\"]).head(3)  # inner join!\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013     12    5     32.00            1930     302.00    228.00   \n1  2013     12    5     50.00            2046     244.00    233.00   \n2  2013     12    5    457.00             500      -3.00    637.00   \n\n   sched_arr_time  arr_delay carrier  ...  dest air_time distance hour  \\\n0            2136     292.00      EV  ...   CHS    94.00      628   19   \n1            2224     249.00      EV  ...   GSO    80.00      445   20   \n2             651     -14.00      US  ...   CLT    81.00      529    5   \n\n   minute  lat_x  lon_x  lat_y  lon_y  daily_delay  \n0      30  40.69 -74.17  40.69 -74.17        51.67  \n1      46  40.69 -74.17  40.69 -74.17        51.67  \n2       0  40.69 -74.17  40.69 -74.17        51.67  \n\n[3 rows x 23 columns]\n\n\n\n\n5.\n\n# 5. flights 테이블의 도착지(dest)에 대한 공항정보가 airports 테이블에 없는 그러한 도착지(dest)를 구하면?\n\nidx = ~flights[\"dest\"].isin(airports[\"faa\"])  # boolian index\nflights[idx][\"dest\"].unique()\n\narray(['SJU', 'STT', 'BQN', 'PSE'], dtype=object)\n\n\n\n\n6.\n\n# 6. Filter flights (항공편) in flights to only show flights with planes that have flown at least 100 flights.\n\nn_planes = (\n    flights.groupby(\"tailnum\")[\"dep_delay\"].count()  # 취소된 항공편을 제외하기 위함. size()는 취소된 항공편도 포함\n    .reset_index(name=\"n\")\n    .query('n &gt;= 100')\n)\nn_planes\n\n     tailnum    n\n1     N0EGMQ  354\n2     N10156  146\n6     N10575  272\n...      ...  ...\n4003  N979DL  125\n4036  N996DL  101\n4042  N9EAMQ  238\n\n[1210 rows x 2 columns]\n\n\n\nflights.merge(n_planes).head(3)  # inner join!\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1    1    517.00             515       2.00    830.00   \n1  2013      1    8   1435.00            1440      -5.00   1717.00   \n2  2013      1    9    717.00             700      17.00    812.00   \n\n   sched_arr_time  arr_delay carrier  ...  dest air_time distance hour  \\\n0             819      11.00      UA  ...   IAH   227.00     1400    5   \n1            1746     -29.00      UA  ...   MIA   150.00     1085   14   \n2             815      -3.00      UA  ...   BOS    39.00      200    7   \n\n   minute  lat_x  lon_x  lat_y  lon_y    n  \n0      15  40.69 -74.17  40.69 -74.17  111  \n1      40  40.69 -74.17  40.69 -74.17  111  \n2       0  40.69 -74.17  40.69 -74.17  111  \n\n[3 rows x 23 columns]\n\n\n\n\n7.\n\n# 7. Find the 48 hours (over the course of the whole year) that have the worst (departure) delays. Cross-reference it with the weather data. Can you see any patterns?\n\n\nworst_hours = (\n    flights.groupby([\"origin\", \"year\", \"month\", \"day\", \"hour\"])[\"dep_delay\"]\n    .mean()\n    .nlargest(48)\n    .reset_index(name=\"ave_delay\")\n)\nworst_hours\n\n   origin  year  month  day  hour  ave_delay\n0     LGA  2013      7   28    21     279.67\n1     EWR  2013      2    9    10     269.00\n2     EWR  2013      2    9     9     266.00\n..    ...   ...    ...  ...   ...        ...\n45    LGA  2013      3    8    15     155.81\n46    EWR  2013      7   10    19     155.31\n47    LGA  2013      3    8    11     155.20\n\n[48 rows x 6 columns]\n\n\n\n# Merge with weather\nweather_most_delayed = weather.merge(worst_hours, how=\"left\")  # on=[\"origin\", \"year\", \"month\", \"day\", \"hour\"]\n\nweather_most_delayed.head(3)\n\n  origin  year  month  day  hour  temp  dewp  humid  wind_dir  wind_speed  \\\n0    EWR  2013      1    1     1 39.02 26.06  59.37    270.00       10.36   \n1    EWR  2013      1    1     2 39.02 26.96  61.63    250.00        8.06   \n2    EWR  2013      1    1     3 39.02 28.04  64.43    240.00       11.51   \n\n   wind_gust  precip  pressure  visib            time_hour  ave_delay  \n0        NaN    0.00   1012.00  10.00  2013-01-01 01:00:00        NaN  \n1        NaN    0.00   1012.30  10.00  2013-01-01 02:00:00        NaN  \n2        NaN    0.00   1012.50  10.00  2013-01-01 03:00:00        NaN  \n\n\n\n# Worst에 포함되는지를 표시하는 컬럼 추가 (tag)\nweather_most_delayed[\"tag\"] = np.where(weather_most_delayed[\"ave_delay\"].isna(), \"others\", \"worst\")\n\nweather_most_delayed.head(3)\n\n  origin  year  month  day  hour  temp  dewp  humid  wind_dir  wind_speed  \\\n0    EWR  2013      1    1     1 39.02 26.06  59.37    270.00       10.36   \n1    EWR  2013      1    1     2 39.02 26.96  61.63    250.00        8.06   \n2    EWR  2013      1    1     3 39.02 28.04  64.43    240.00       11.51   \n\n   wind_gust  precip  pressure  visib            time_hour  ave_delay     tag  \n0        NaN    0.00   1012.00  10.00  2013-01-01 01:00:00        NaN  others  \n1        NaN    0.00   1012.30  10.00  2013-01-01 02:00:00        NaN  others  \n2        NaN    0.00   1012.50  10.00  2013-01-01 03:00:00        NaN  others  \n\n\n\n# Normalize the weather conditions\nweather_most_delayed_z = weather_most_delayed.copy()\nweather_most_delayed_z.loc[:, \"temp\":\"visib\"] = weather_most_delayed_z.loc[\n    :, \"temp\":\"visib\"\n].apply(lambda x: (x - x.mean()) / x.std())  # 컬럼별로 정규화\n\nweather_most_delayed_z.head(3)\n\n  origin  year  month  day  hour  temp  dewp  humid  wind_dir  wind_speed  \\\n0    EWR  2013      1    1     1 -0.91 -0.79  -0.16      0.65       -0.02   \n1    EWR  2013      1    1     2 -0.91 -0.75  -0.05      0.47       -0.29   \n2    EWR  2013      1    1     3 -0.91 -0.69   0.10      0.37        0.12   \n\n   wind_gust  precip  pressure  visib            time_hour  ave_delay     tag  \n0        NaN   -0.15     -0.79   0.36  2013-01-01 01:00:00        NaN  others  \n1        NaN   -0.15     -0.75   0.36  2013-01-01 02:00:00        NaN  others  \n2        NaN   -0.15     -0.73   0.36  2013-01-01 03:00:00        NaN  others  \n\n\n\n# 여러 기상 정보에 대해 long form으로 변환\nweather_most_delayed_long = weather_most_delayed_z.melt(\n    id_vars=[\"origin\", \"year\", \"month\", \"day\", \"hour\", \"tag\"],\n    value_vars=[\n        \"temp\",\n        \"dewp\",\n        \"humid\",\n        \"wind_speed\",\n        \"wind_gust\",\n        \"precip\",\n        \"visib\",\n        \"pressure\",\n    ],\n    var_name=\"weather\",\n)\n\nfrom sbcustom import boxplot, rangeplot\n(\n    rangeplot(weather_most_delayed_long, x='tag', y='value')\n    .facet(\"weather\", wrap=4)\n    .layout(size=(8, 5))\n)\n\n\n\n\n\n\n\n\n\n# 참고 anti-join: worst_hours와 키가 매치되지 않는 행만 선택\n(\n    weather\n    .merge(worst_hours, indicator=True, how=\"left\")\n    .query('_merge == \"left_only\"')\n).head(3)\n\n  origin  year  month  day  hour  temp  dewp  humid  wind_dir  wind_speed  \\\n0    EWR  2013      1    1     1 39.02 26.06  59.37    270.00       10.36   \n1    EWR  2013      1    1     2 39.02 26.96  61.63    250.00        8.06   \n2    EWR  2013      1    1     3 39.02 28.04  64.43    240.00       11.51   \n\n   wind_gust  precip  pressure  visib            time_hour  ave_delay  \\\n0        NaN    0.00   1012.00  10.00  2013-01-01 01:00:00        NaN   \n1        NaN    0.00   1012.30  10.00  2013-01-01 02:00:00        NaN   \n2        NaN    0.00   1012.50  10.00  2013-01-01 03:00:00        NaN   \n\n      _merge  \n0  left_only  \n1  left_only  \n2  left_only  \n\n\n\n\n8.\n\n# 8. You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above.\n\n\n# 즉, 각 항공기는 특정 항공사에서만 운행되는가의 질문임. 2개 이상의 항공사에서 운항되는 항공기가 있는지 확인해 볼 것\n\nplanes_share = (\n    flights.groupby(\"tailnum\")[\"carrier\"].nunique()\n    .reset_index(name=\"n\")\n    .query(\"n &gt; 1\")\n)\nplanes_share\n\n     tailnum  n\n195   N146PQ  2\n224   N153PQ  2\n342   N176PQ  2\n...      ... ..\n4021  N989AT  2\n4023  N990AT  2\n4031  N994AT  2\n\n[17 rows x 2 columns]\n\n\n\n# 그리고, 2개 이상의 항공사에서 운항되는 항공기들만 포함하고, 그 항공사들의 full name을 함께 포함하는 테이블을 만들어 볼 것\n\nflights.merge(planes_share).merge(airlines).head(3)\n\n   year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n0  2013      1   11   1244.00            1250      -6.00   1459.00   \n1  2013      1   11   1821.00            1830      -9.00   2014.00   \n2  2013      1   15    612.00             615      -3.00    927.00   \n\n   sched_arr_time  arr_delay carrier  ...  air_time distance hour minute  \\\n0            1449      10.00      9E  ...     92.00      488   12     50   \n1            2044     -30.00      9E  ...     79.00      488   18     30   \n2             855      32.00      9E  ...    134.00      760    6     15   \n\n   lat_x  lon_x  lat_y  lon_y  n               name  \n0  40.69 -74.17  40.69 -74.17  2  Endeavor Air Inc.  \n1  40.69 -74.17  40.69 -74.17  2  Endeavor Air Inc.  \n2  40.64 -73.78  40.64 -73.78  2  Endeavor Air Inc.  \n\n[3 rows x 24 columns]",
    "crumbs": [
      "Solutions",
      "Transforming II"
    ]
  },
  {
    "objectID": "contents/Transform/transform.html",
    "href": "contents/Transform/transform.html",
    "title": "Transforming I",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n이번 장에서는 시각화를 하기 전후로, 중요한 데이터 패턴을 보기 위해서 새로운 변수를 만들거나 요약한 통계치를 만들 필요가 있는데 이를 다루는 핵심적인 함수들에 대해 익힙니다.\n대략 다음과 같은 transform들을 조합하여 분석에 필요한 상태로 바꿉니다.\nOn-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013\n# import the dataset\nflights_data = sm.datasets.get_rdataset(\"flights\", \"nycflights13\")\nflights = flights_data.data\nflights = flights.drop(columns=\"time_hour\")  # drop the \"time_hour\" column\n# Description\nprint(flights_data.__doc__)\nflights\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n1\n2013\n1\n1\n533.00\n529\n4.00\n850.00\n830\n20.00\nUA\n1714\nN24211\nLGA\nIAH\n227.00\n1416\n5\n29\n\n\n2\n2013\n1\n1\n542.00\n540\n2.00\n923.00\n850\n33.00\nAA\n1141\nN619AA\nJFK\nMIA\n160.00\n1089\n5\n40\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336773\n2013\n9\n30\nNaN\n1210\nNaN\nNaN\n1330\nNaN\nMQ\n3461\nN535MQ\nLGA\nBNA\nNaN\n764\n12\n10\n\n\n336774\n2013\n9\n30\nNaN\n1159\nNaN\nNaN\n1344\nNaN\nMQ\n3572\nN511MQ\nLGA\nCLE\nNaN\n419\n11\n59\n\n\n336775\n2013\n9\n30\nNaN\n840\nNaN\nNaN\n1020\nNaN\nMQ\n3531\nN839MQ\nLGA\nRDU\nNaN\n431\n8\n40\n\n\n\n\n336776 rows × 18 columns\nflights.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 336776 entries, 0 to 336775\nData columns (total 18 columns):\n #   Column          Non-Null Count   Dtype  \n---  ------          --------------   -----  \n 0   year            336776 non-null  int64  \n 1   month           336776 non-null  int64  \n 2   day             336776 non-null  int64  \n 3   dep_time        328521 non-null  float64\n 4   sched_dep_time  336776 non-null  int64  \n 5   dep_delay       328521 non-null  float64\n 6   arr_time        328063 non-null  float64\n 7   sched_arr_time  336776 non-null  int64  \n 8   arr_delay       327346 non-null  float64\n 9   carrier         336776 non-null  object \n 10  flight          336776 non-null  int64  \n 11  tailnum         334264 non-null  object \n 12  origin          336776 non-null  object \n 13  dest            336776 non-null  object \n 14  air_time        327346 non-null  float64\n 15  distance        336776 non-null  int64  \n 16  hour            336776 non-null  int64  \n 17  minute          336776 non-null  int64  \ndtypes: float64(5), int64(9), object(4)\nmemory usage: 46.2+ MB",
    "crumbs": [
      "Transform",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Transform/transform.html#rows",
    "href": "contents/Transform/transform.html#rows",
    "title": "Transforming I",
    "section": "Rows",
    "text": "Rows\n\nquery()\nThe query() Method 참조\n\nConditional operators\n&gt;,  &gt;=,  &lt;,  &lt;=,\n==  (equal to),  !=  (not equal to)\nand, & (and)\nor, | (or)\nnot, ~ (not)\nin (includes), not in (not included)\n\n\n# Flights that arrived more than 120 minutes (two hours) late\nflights.query('arr_delay &gt; 120')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n119\n2013\n1\n1\n811.00\n630\n101.00\n1047.00\n830\n137.00\nMQ\n4576\nN531MQ\nLGA\nCLT\n118.00\n544\n6\n30\n\n\n151\n2013\n1\n1\n848.00\n1835\n853.00\n1001.00\n1950\n851.00\nMQ\n3944\nN942MQ\nJFK\nBWI\n41.00\n184\n18\n35\n\n\n218\n2013\n1\n1\n957.00\n733\n144.00\n1056.00\n853\n123.00\nUA\n856\nN534UA\nEWR\nBOS\n37.00\n200\n7\n33\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336724\n2013\n9\n30\n2053.00\n1815\n158.00\n2310.00\n2054\n136.00\nEV\n5292\nN600QX\nEWR\nATL\n91.00\n746\n18\n15\n\n\n336757\n2013\n9\n30\n2159.00\n1845\n194.00\n2344.00\n2030\n194.00\n9E\n3320\nN906XJ\nJFK\nBUF\n50.00\n301\n18\n45\n\n\n336763\n2013\n9\n30\n2235.00\n2001\n154.00\n59.00\n2249\n130.00\nB6\n1083\nN804JB\nJFK\nMCO\n123.00\n944\n20\n1\n\n\n\n\n10034 rows × 18 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n외부 변수/함수를 참조하려면 @와 함께\ndelay_cutoff = 120\nflights.query('arr_delay &gt; @delay_cutoff')\ndef cut_off(df):\n    return df[\"dep_delay\"].min()\n\nflights.query('arr_delay &lt; @cut_off(@flights)')\n위의 query 방식의 filtering은 다음과 같은 boolean indexing의 결과와 같음\nflights[flights[\"arr_delay\"] &gt; 120]\n\n\n\n# Flights that departed on January 1\nflights.query('month == 1 and day == 1')  # == 과 = 을 혼동하지 말것!\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n1\n2013\n1\n1\n533.00\n529\n4.00\n850.00\n830\n20.00\nUA\n1714\nN24211\nLGA\nIAH\n227.00\n1416\n5\n29\n\n\n2\n2013\n1\n1\n542.00\n540\n2.00\n923.00\n850\n33.00\nAA\n1141\nN619AA\nJFK\nMIA\n160.00\n1089\n5\n40\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n839\n2013\n1\n1\nNaN\n1935\nNaN\nNaN\n2240\nNaN\nAA\n791\nN3EHAA\nLGA\nDFW\nNaN\n1389\n19\n35\n\n\n840\n2013\n1\n1\nNaN\n1500\nNaN\nNaN\n1825\nNaN\nAA\n1925\nN3EVAA\nLGA\nMIA\nNaN\n1096\n15\n0\n\n\n841\n2013\n1\n1\nNaN\n600\nNaN\nNaN\n901\nNaN\nB6\n125\nN618JB\nJFK\nFLL\nNaN\n1069\n6\n0\n\n\n\n\n842 rows × 18 columns\n\n\n\n\n# Flights that departed in January or February\nflights.query('month == 1 or month == 2')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n1\n2013\n1\n1\n533.00\n529\n4.00\n850.00\n830\n20.00\nUA\n1714\nN24211\nLGA\nIAH\n227.00\n1416\n5\n29\n\n\n2\n2013\n1\n1\n542.00\n540\n2.00\n923.00\n850\n33.00\nAA\n1141\nN619AA\nJFK\nMIA\n160.00\n1089\n5\n40\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n136244\n2013\n2\n28\nNaN\n1115\nNaN\nNaN\n1310\nNaN\nMQ\n4485\nN725MQ\nLGA\nCMH\nNaN\n479\n11\n15\n\n\n136245\n2013\n2\n28\nNaN\n830\nNaN\nNaN\n1205\nNaN\nUA\n1480\nNaN\nEWR\nSFO\nNaN\n2565\n8\n30\n\n\n136246\n2013\n2\n28\nNaN\n840\nNaN\nNaN\n1147\nNaN\nUA\n443\nNaN\nJFK\nLAX\nNaN\n2475\n8\n40\n\n\n\n\n51955 rows × 18 columns\n\n\n\n\n# A shorter way to select flights that departed in January or February\nflights.query('month in [1, 2]')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n1\n2013\n1\n1\n533.00\n529\n4.00\n850.00\n830\n20.00\nUA\n1714\nN24211\nLGA\nIAH\n227.00\n1416\n5\n29\n\n\n2\n2013\n1\n1\n542.00\n540\n2.00\n923.00\n850\n33.00\nAA\n1141\nN619AA\nJFK\nMIA\n160.00\n1089\n5\n40\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n136244\n2013\n2\n28\nNaN\n1115\nNaN\nNaN\n1310\nNaN\nMQ\n4485\nN725MQ\nLGA\nCMH\nNaN\n479\n11\n15\n\n\n136245\n2013\n2\n28\nNaN\n830\nNaN\nNaN\n1205\nNaN\nUA\n1480\nNaN\nEWR\nSFO\nNaN\n2565\n8\n30\n\n\n136246\n2013\n2\n28\nNaN\n840\nNaN\nNaN\n1147\nNaN\nUA\n443\nNaN\nJFK\nLAX\nNaN\n2475\n8\n40\n\n\n\n\n51955 rows × 18 columns\n\n\n\n\nflights.query('arr_delay &gt; 120 and not (origin == \"JFK\")')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n119\n2013\n1\n1\n811.00\n630\n101.00\n1047.00\n830\n137.00\nMQ\n4576\nN531MQ\nLGA\nCLT\n118.00\n544\n6\n30\n\n\n218\n2013\n1\n1\n957.00\n733\n144.00\n1056.00\n853\n123.00\nUA\n856\nN534UA\nEWR\nBOS\n37.00\n200\n7\n33\n\n\n268\n2013\n1\n1\n1114.00\n900\n134.00\n1447.00\n1222\n145.00\nUA\n1086\nN76502\nLGA\nIAH\n248.00\n1416\n9\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336529\n2013\n9\n30\n1738.00\n1529\n129.00\n1906.00\n1649\n137.00\nEV\n4580\nN12563\nEWR\nMKE\n110.00\n725\n15\n29\n\n\n336668\n2013\n9\n30\n1951.00\n1649\n182.00\n2157.00\n1903\n174.00\nEV\n4294\nN13988\nEWR\nSAV\n95.00\n708\n16\n49\n\n\n336724\n2013\n9\n30\n2053.00\n1815\n158.00\n2310.00\n2054\n136.00\nEV\n5292\nN600QX\nEWR\nATL\n91.00\n746\n18\n15\n\n\n\n\n6868 rows × 18 columns\n\n\n\n\nflights.query('dep_time &lt; sched_dep_time')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n3\n2013\n1\n1\n544.00\n545\n-1.00\n1004.00\n1022\n-18.00\nB6\n725\nN804JB\nJFK\nBQN\n183.00\n1576\n5\n45\n\n\n4\n2013\n1\n1\n554.00\n600\n-6.00\n812.00\n837\n-25.00\nDL\n461\nN668DN\nLGA\nATL\n116.00\n762\n6\n0\n\n\n5\n2013\n1\n1\n554.00\n558\n-4.00\n740.00\n728\n12.00\nUA\n1696\nN39463\nEWR\nORD\n150.00\n719\n5\n58\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336766\n2013\n9\n30\n2240.00\n2250\n-10.00\n2347.00\n7\n-20.00\nB6\n2002\nN281JB\nJFK\nBUF\n52.00\n301\n22\n50\n\n\n336767\n2013\n9\n30\n2241.00\n2246\n-5.00\n2345.00\n1\n-16.00\nB6\n486\nN346JB\nJFK\nROC\n47.00\n264\n22\n46\n\n\n336769\n2013\n9\n30\n2349.00\n2359\n-10.00\n325.00\n350\n-25.00\nB6\n745\nN516JB\nJFK\nPSE\n196.00\n1617\n23\n59\n\n\n\n\n184782 rows × 18 columns\n\n\n\n\nflights.query('arr_delay + dep_delay &lt; 0')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n3\n2013\n1\n1\n544.00\n545\n-1.00\n1004.00\n1022\n-18.00\nB6\n725\nN804JB\nJFK\nBQN\n183.00\n1576\n5\n45\n\n\n4\n2013\n1\n1\n554.00\n600\n-6.00\n812.00\n837\n-25.00\nDL\n461\nN668DN\nLGA\nATL\n116.00\n762\n6\n0\n\n\n7\n2013\n1\n1\n557.00\n600\n-3.00\n709.00\n723\n-14.00\nEV\n5708\nN829AS\nLGA\nIAD\n53.00\n229\n6\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336766\n2013\n9\n30\n2240.00\n2250\n-10.00\n2347.00\n7\n-20.00\nB6\n2002\nN281JB\nJFK\nBUF\n52.00\n301\n22\n50\n\n\n336767\n2013\n9\n30\n2241.00\n2246\n-5.00\n2345.00\n1\n-16.00\nB6\n486\nN346JB\nJFK\nROC\n47.00\n264\n22\n46\n\n\n336769\n2013\n9\n30\n2349.00\n2359\n-10.00\n325.00\n350\n-25.00\nB6\n745\nN516JB\nJFK\nPSE\n196.00\n1617\n23\n59\n\n\n\n\n188401 rows × 18 columns\n\n\n\n\nflights.query('not dep_delay.isna() and arr_delay.isna()')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n471\n2013\n1\n1\n1525.00\n1530\n-5.00\n1934.00\n1805\nNaN\nMQ\n4525\nN719MQ\nLGA\nXNA\nNaN\n1147\n15\n30\n\n\n477\n2013\n1\n1\n1528.00\n1459\n29.00\n2002.00\n1647\nNaN\nEV\n3806\nN17108\nEWR\nSTL\nNaN\n872\n14\n59\n\n\n615\n2013\n1\n1\n1740.00\n1745\n-5.00\n2158.00\n2020\nNaN\nMQ\n4413\nN739MQ\nLGA\nXNA\nNaN\n1147\n17\n45\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n334495\n2013\n9\n28\n1214.00\n1225\n-11.00\n1801.00\n1510\nNaN\nAA\n300\nN488AA\nEWR\nDFW\nNaN\n1372\n12\n25\n\n\n335534\n2013\n9\n29\n1734.00\n1711\n23.00\n2159.00\n2020\nNaN\nUA\n327\nN463UA\nEWR\nPDX\nNaN\n2434\n17\n11\n\n\n335805\n2013\n9\n30\n559.00\n600\n-1.00\nNaN\n715\nNaN\nWN\n464\nN411WN\nEWR\nMDW\nNaN\n711\n6\n0\n\n\n\n\n1175 rows × 18 columns\n\n\n\n\ndelay_time = 900\nflights.query('arr_delay &gt; @delay_time')\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n7072\n2013\n1\n9\n641.00\n900\n1301.00\n1242.00\n1530\n1272.00\nHA\n51\nN384HA\nJFK\nHNL\n640.00\n4983\n9\n0\n\n\n8239\n2013\n1\n10\n1121.00\n1635\n1126.00\n1239.00\n1810\n1109.00\nMQ\n3695\nN517MQ\nEWR\nORD\n111.00\n719\n16\n35\n\n\n151974\n2013\n3\n17\n2321.00\n810\n911.00\n135.00\n1020\n915.00\nDL\n2119\nN927DA\nLGA\nMSP\n167.00\n1020\n8\n10\n\n\n173992\n2013\n4\n10\n1100.00\n1900\n960.00\n1342.00\n2211\n931.00\nDL\n2391\nN959DL\nJFK\nTPA\n139.00\n1005\n19\n0\n\n\n235778\n2013\n6\n15\n1432.00\n1935\n1137.00\n1607.00\n2120\n1127.00\nMQ\n3535\nN504MQ\nJFK\nCMH\n74.00\n483\n19\n35\n\n\n270376\n2013\n7\n22\n845.00\n1600\n1005.00\n1044.00\n1815\n989.00\nMQ\n3075\nN665MQ\nJFK\nCVG\n96.00\n589\n16\n0\n\n\n327043\n2013\n9\n20\n1139.00\n1845\n1014.00\n1457.00\n2210\n1007.00\nAA\n177\nN338AA\nJFK\nSFO\n354.00\n2586\n18\n45\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n.query()의 결과는 view이므로 수정하려면 SettingWithCopyWarning; .copy() 후 사용 권장\nPython에서 유효하지 않은 변수명의 경우, 예를 들어, 빈칸이나 .이 있는 경우; income total, income.total\nBacktick으로 감싸줘야 함; `income total`, `income.total`, `class`\nIndex값을 사용하여 query할 경우, 문서 참고\n\n\n\n\n\n\n\n\n\nquery() 조건의 참거짓에 상관없이 NA값은 모두 제외함\n\n\n\n\n\n# df가 다음과 같을 때,\n#      A     B\n# 0 1.00  2.00\n# 1  NaN  5.00\n# 2 3.00  3.00\n# 3 4.00   NaN\n\ndf.query('A &gt; 1')\n#      A     B\n# 2 3.00  3.00\n# 3 4.00   NaN\n\n# NA를 포함하고자 할 때,\ndf.query('A &gt; 1 | A.isna()') # .isna() : NA인지 여부\n#      A     B\n# 1  NaN  5.00\n# 2 3.00  3.00\n# 3 4.00   NaN\nNA(missing)에 대해서는 뒤에서 자세히\n\n\n\n\n\nsort_values()\n\n# \"year\", \"month\", \"day\", \"dep_time\" 순서대로 내림차순으로 정렬\nflights.sort_values(by=[\"year\", \"month\", \"day\", \"dep_time\"], ascending=False)   # default: ascending=True\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n111279\n2013\n12\n31\n2356.00\n2359\n-3.00\n436.00\n445\n-9.00\nB6\n745\nN665JB\nJFK\nPSE\n200.00\n1617\n23\n59\n\n\n111278\n2013\n12\n31\n2355.00\n2359\n-4.00\n430.00\n440\n-10.00\nB6\n1503\nN509JB\nJFK\nSJU\n195.00\n1598\n23\n59\n\n\n111277\n2013\n12\n31\n2332.00\n2245\n47.00\n58.00\n3\n55.00\nB6\n486\nN334JB\nJFK\nROC\n60.00\n264\n22\n45\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n839\n2013\n1\n1\nNaN\n1935\nNaN\nNaN\n2240\nNaN\nAA\n791\nN3EHAA\nLGA\nDFW\nNaN\n1389\n19\n35\n\n\n840\n2013\n1\n1\nNaN\n1500\nNaN\nNaN\n1825\nNaN\nAA\n1925\nN3EVAA\nLGA\nMIA\nNaN\n1096\n15\n0\n\n\n841\n2013\n1\n1\nNaN\n600\nNaN\nNaN\n901\nNaN\nB6\n125\nN618JB\nJFK\nFLL\nNaN\n1069\n6\n0\n\n\n\n\n336776 rows × 18 columns\n\n\n\n\n# pd.DataFrame.nlargest()를 이용할 수도 있음\nflights.nlargest(3, columns=[\"year\", \"month\", \"day\", \"dep_time\"], keep=\"all\")\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n111279\n2013\n12\n31\n2356.00\n2359\n-3.00\n436.00\n445\n-9.00\nB6\n745\nN665JB\nJFK\nPSE\n200.00\n1617\n23\n59\n\n\n111278\n2013\n12\n31\n2355.00\n2359\n-4.00\n430.00\n440\n-10.00\nB6\n1503\nN509JB\nJFK\nSJU\n195.00\n1598\n23\n59\n\n\n111277\n2013\n12\n31\n2332.00\n2245\n47.00\n58.00\n3\n55.00\nB6\n486\nN334JB\nJFK\nROC\n60.00\n264\n22\n45\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nmethod의 설명이 잘 나타나지 않을 시, 함수를 직접 써서 확인\n예를 들어,\npd.DataFrame.nlargest\npd.Series.nlargest\n\n\n\n# \"dep_time\"은 내림차순으로, \"arr_delay\"는 오름차순으로\nflights.sort_values(by=[\"dep_time\", \"arr_delay\"], ascending=[False, True])\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n150301\n2013\n3\n15\n2400.00\n2359\n1.00\n324.00\n338\n-14.00\nB6\n727\nN636JB\nJFK\nBQN\n186.00\n1576\n23\n59\n\n\n87893\n2013\n12\n5\n2400.00\n2359\n1.00\n427.00\n440\n-13.00\nB6\n1503\nN587JB\nJFK\nSJU\n182.00\n1598\n23\n59\n\n\n212941\n2013\n5\n21\n2400.00\n2359\n1.00\n339.00\n350\n-11.00\nB6\n739\nN527JB\nJFK\nPSE\n199.00\n1617\n23\n59\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336773\n2013\n9\n30\nNaN\n1210\nNaN\nNaN\n1330\nNaN\nMQ\n3461\nN535MQ\nLGA\nBNA\nNaN\n764\n12\n10\n\n\n336774\n2013\n9\n30\nNaN\n1159\nNaN\nNaN\n1344\nNaN\nMQ\n3572\nN511MQ\nLGA\nCLE\nNaN\n419\n11\n59\n\n\n336775\n2013\n9\n30\nNaN\n840\nNaN\nNaN\n1020\nNaN\nMQ\n3531\nN839MQ\nLGA\nRDU\nNaN\n431\n8\n40\n\n\n\n\n336776 rows × 18 columns\n\n\n\n query()와 sort_values()을 함께 이용하여 좀 더 복잡한 문제를 해결할 수 있음\n예를 들어, 다음과 같이 거의 제시간에 출발한(+- 10분 이내) 항공편들 중 가장 도착 지연이 큰 항공편을 찾을 수 있음\n\n# Method chaining\n(\n    flights\n    .query('dep_delay &lt;= 10 & dep_delay &gt;= -10')\n    .sort_values(\"arr_delay\", ascending=False)\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n55985\n2013\n11\n1\n658.00\n700\n-2.00\n1329.00\n1015\n194.00\nVX\n399\nN629VA\nJFK\nLAX\n336.00\n2475\n7\n0\n\n\n181270\n2013\n4\n18\n558.00\n600\n-2.00\n1149.00\n850\n179.00\nAA\n707\nN3EXAA\nLGA\nDFW\n234.00\n1389\n6\n0\n\n\n256340\n2013\n7\n7\n1659.00\n1700\n-1.00\n2050.00\n1823\n147.00\nUS\n2183\nN948UW\nLGA\nDCA\n64.00\n214\n17\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n334354\n2013\n9\n28\n847.00\n839\n8.00\n1130.00\n959\nNaN\nEV\n4510\nN14542\nEWR\nMKE\nNaN\n725\n8\n39\n\n\n334412\n2013\n9\n28\n1010.00\n1020\n-10.00\n1344.00\n1222\nNaN\nEV\n4412\nN12175\nEWR\nDSM\nNaN\n1017\n10\n20\n\n\n335805\n2013\n9\n30\n559.00\n600\n-1.00\nNaN\n715\nNaN\nWN\n464\nN411WN\nEWR\nMDW\nNaN\n711\n6\n0\n\n\n\n\n239109 rows × 18 columns\n\n\n\n\n\n\n\n\n\nsort_values() 파라미터\n\n\n\n\nRow가 재정렬되는 operation을 한 후에는 index 순서가 바뀌는데, 이를 reset하려면,\n.sort_values(ignore_index=True)\nDataFrame updated: inplace=True\nNA는 sort 후 맨 뒤로\n맨 앞으로 오게하려면 na_position='first'\n\n\n\n\n\nunique()\nSeries method\n\nflights[\"origin\"].unique()  # return as a NumPy array, but depends on the dtypes\n\narray(['EWR', 'LGA', 'JFK'], dtype=object)\n\n\n\nflights[\"origin\"].nunique()  # return the number of unique values\n\n3\n\n\n\n# finds all unique origin and destination pairs.\nflights[[\"origin\", \"dest\"]].value_counts(sort=False)  # default: dropna=True\n\norigin  dest\nEWR     ALB      439\n        ANC        8\n        ATL     5022\n                ... \nLGA     TVC       77\n        TYS      308\n        XNA      745\nName: count, Length: 224, dtype: int64\n\n\n\nflights[[\"origin\", \"dest\"]].value_counts(sort=False).reset_index(name=\"n\")\n\n    origin dest     n\n0      EWR  ALB   439\n1      EWR  ANC     8\n2      EWR  ATL  5022\n..     ...  ...   ...\n221    LGA  TVC    77\n222    LGA  TYS   308\n223    LGA  XNA   745\n\n[224 rows x 3 columns]",
    "crumbs": [
      "Transform",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Transform/transform.html#columns",
    "href": "contents/Transform/transform.html#columns",
    "title": "Transforming I",
    "section": "Columns",
    "text": "Columns\n\nSelect\n기본적인 column selection은 이전 섹션 참고: subsetting\n\n# Select columns by name\nflights[[\"year\", \"month\", \"day\"]]\n\n        year  month  day\n0       2013      1    1\n1       2013      1    1\n2       2013      1    1\n...      ...    ...  ...\n336773  2013      9   30\n336774  2013      9   30\n336775  2013      9   30\n\n[336776 rows x 3 columns]\n\n\n\n# Select all columns between year and day (inclusive)\nflights.loc[:, \"year\":\"day\"]\n\n        year  month  day\n0       2013      1    1\n1       2013      1    1\n2       2013      1    1\n...      ...    ...  ...\n336773  2013      9   30\n336774  2013      9   30\n336775  2013      9   30\n\n[336776 rows x 3 columns]\n\n\n\n# Select all columns except those from year to day (inclusive)\n# .isin(): includes\nflights.loc[:, ~flights.columns.isin([\"year\", \"month\", \"day\"])]  # Boolean indexing\n\n\n\n\n\n\n\n\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n0\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n1\n533.00\n529\n4.00\n850.00\n830\n20.00\nUA\n1714\nN24211\nLGA\nIAH\n227.00\n1416\n5\n29\n\n\n2\n542.00\n540\n2.00\n923.00\n850\n33.00\nAA\n1141\nN619AA\nJFK\nMIA\n160.00\n1089\n5\n40\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336773\nNaN\n1210\nNaN\nNaN\n1330\nNaN\nMQ\n3461\nN535MQ\nLGA\nBNA\nNaN\n764\n12\n10\n\n\n336774\nNaN\n1159\nNaN\nNaN\n1344\nNaN\nMQ\n3572\nN511MQ\nLGA\nCLE\nNaN\n419\n11\n59\n\n\n336775\nNaN\n840\nNaN\nNaN\n1020\nNaN\nMQ\n3531\nN839MQ\nLGA\nRDU\nNaN\n431\n8\n40\n\n\n\n\n336776 rows × 15 columns\n\n\n\n\ncols = [col for col in flights.columns if col not in [\"year\", \"month\", \"day\"]]\nflights[cols]\n\n        dep_time  sched_dep_time  dep_delay  arr_time  sched_arr_time  \\\n0         517.00             515       2.00    830.00             819   \n1         533.00             529       4.00    850.00             830   \n2         542.00             540       2.00    923.00             850   \n...          ...             ...        ...       ...             ...   \n336773       NaN            1210        NaN       NaN            1330   \n336774       NaN            1159        NaN       NaN            1344   \n336775       NaN             840        NaN       NaN            1020   \n\n        arr_delay carrier  flight tailnum origin dest  air_time  distance  \\\n0           11.00      UA    1545  N14228    EWR  IAH    227.00      1400   \n1           20.00      UA    1714  N24211    LGA  IAH    227.00      1416   \n2           33.00      AA    1141  N619AA    JFK  MIA    160.00      1089   \n...           ...     ...     ...     ...    ...  ...       ...       ...   \n336773        NaN      MQ    3461  N535MQ    LGA  BNA       NaN       764   \n336774        NaN      MQ    3572  N511MQ    LGA  CLE       NaN       419   \n336775        NaN      MQ    3531  N839MQ    LGA  RDU       NaN       431   \n\n        hour  minute  \n0          5      15  \n1          5      29  \n2          5      40  \n...      ...     ...  \n336773    12      10  \n336774    11      59  \n336775     8      40  \n\n[336776 rows x 15 columns]\n\n\n Series/Index object의 method와 함께 특정 string을 기준으로 선택 (string method와는 구별)\n.str.contains(), .str.startswith(), .str.endswith() ; True/False\n\n# Select all columns that begin with “dep”.\nflights.loc[:, flights.columns.str.startswith(\"dep\")]  # Boolean indexing\n\n        dep_time  dep_delay\n0         517.00       2.00\n1         533.00       4.00\n2         542.00       2.00\n...          ...        ...\n336773       NaN        NaN\n336774       NaN        NaN\n336775       NaN        NaN\n\n[336776 rows x 2 columns]\n\n\n\n# Select all columns that are characters\nflights.select_dtypes(\"object\")  # dtype: object, number, ...\n\n       carrier tailnum origin dest\n0           UA  N14228    EWR  IAH\n1           UA  N24211    LGA  IAH\n2           AA  N619AA    JFK  MIA\n...        ...     ...    ...  ...\n336773      MQ  N535MQ    LGA  BNA\n336774      MQ  N511MQ    LGA  CLE\n336775      MQ  N839MQ    LGA  RDU\n\n[336776 rows x 4 columns]\n\n\nindex selection: reindex\n\n\nrename()\n\npd.DataFrame.rename\n\n\nflights.rename(\n    columns={\"dep_time\": \"dep_t\", \"arr_time\": \"arr_t\"},  \n    # 첫번째 인자 index=\n    # inplace=True: dataframe is updated\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_t\nsched_dep_time\ndep_delay\narr_t\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n1\n2013\n1\n1\n533.00\n529\n4.00\n850.00\n830\n20.00\nUA\n1714\nN24211\nLGA\nIAH\n227.00\n1416\n5\n29\n\n\n2\n2013\n1\n1\n542.00\n540\n2.00\n923.00\n850\n33.00\nAA\n1141\nN619AA\nJFK\nMIA\n160.00\n1089\n5\n40\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336773\n2013\n9\n30\nNaN\n1210\nNaN\nNaN\n1330\nNaN\nMQ\n3461\nN535MQ\nLGA\nBNA\nNaN\n764\n12\n10\n\n\n336774\n2013\n9\n30\nNaN\n1159\nNaN\nNaN\n1344\nNaN\nMQ\n3572\nN511MQ\nLGA\nCLE\nNaN\n419\n11\n59\n\n\n336775\n2013\n9\n30\nNaN\n840\nNaN\nNaN\n1020\nNaN\nMQ\n3531\nN839MQ\nLGA\nRDU\nNaN\n431\n8\n40\n\n\n\n\n336776 rows × 18 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nindex를 rename할 수도 있음\nflights.rename(\n    index={0: \"a\", 1: \"b\"},\n).head(3)\n#    year  month  day  dep_time  ...\n# a  2013      1    1    517.00  ...\n# b  2013      1    1    533.00  ...\n# 2  2013      1    1    542.00  ...\n\n\n\n\n\n\n\n\nTip\n\n\n\nSeries의 경우\ns = flights.dep_delay.head(3)\n# 0   2.00\n# 1   4.00\n# 2   2.00\n# Name: dep_delay, dtype: float64\n\ns.rename(\"what\")\n# 0   2.00\n# 1   4.00\n# 2   2.00\n# Name: what, dtype: float64\n\n\n함수 str.capitalize, str.lower, str.upper과 함께.\n\nflights.rename(str.capitalize, axis=\"columns\").head(3)  # axis=1\n\n\n\n\n\n\n\n\nYear\nMonth\nDay\nDep_time\nSched_dep_time\nDep_delay\nArr_time\nSched_arr_time\nArr_delay\nCarrier\nFlight\nTailnum\nOrigin\nDest\nAir_time\nDistance\nHour\nMinute\n\n\n\n\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n1\n2013\n1\n1\n533.00\n529\n4.00\n850.00\n830\n20.00\nUA\n1714\nN24211\nLGA\nIAH\n227.00\n1416\n5\n29\n\n\n2\n2013\n1\n1\n542.00\n540\n2.00\n923.00\n850\n33.00\nAA\n1141\nN619AA\nJFK\nMIA\n160.00\n1089\n5\n40\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n일반적으로 임의의 함수가 적용됨.\nflights.rename(lambda x: x[:3], axis=\"columns\")\n\n#     yea  mon  day    dep  sch  dep    arr  sch   arr car   fli     tai  ori  ...\n# 0  2013    1    1 517.00  515 2.00 830.00  819 11.00  UA  1545  N14228  EWR   \n# 1  2013    1    1 533.00  529 4.00 850.00  830 20.00  UA  1714  N24211  LGA   \n# 2  2013    1    1 542.00  540 2.00 923.00  850 33.00  AA  1141  N619AA  JFK  \n# ...\n\n\n\n\nassign()\n\ncols = [\"year\", \"month\", \"day\", \"distance\", \"air_time\"] + \\\n        [col for col in flights.columns if col.endswith(\"delay\")]  # string method .endswith\nflights_sml = flights[cols].copy()\nflights_sml\n\n        year  month  day  distance  air_time  dep_delay  arr_delay\n0       2013      1    1      1400    227.00       2.00      11.00\n1       2013      1    1      1416    227.00       4.00      20.00\n2       2013      1    1      1089    160.00       2.00      33.00\n...      ...    ...  ...       ...       ...        ...        ...\n336773  2013      9   30       764       NaN        NaN        NaN\n336774  2013      9   30       419       NaN        NaN        NaN\n336775  2013      9   30       431       NaN        NaN        NaN\n\n[336776 rows x 7 columns]\n\n\n\n# 새로 만들어진 변수는 맨 뒤로\nflights_sml.assign(\n    gain=lambda x: x.dep_delay - x.arr_delay,   # x: DataFrame, flights_sml\n    speed=flights_sml[\"distance\"] / flights_sml[\"air_time\"] * 60    # 직접 DataFrame 참조할 수도 있음\n)\n\n        year  month  day  distance  air_time  dep_delay  arr_delay   gain  \\\n0       2013      1    1      1400    227.00       2.00      11.00  -9.00   \n1       2013      1    1      1416    227.00       4.00      20.00 -16.00   \n2       2013      1    1      1089    160.00       2.00      33.00 -31.00   \n...      ...    ...  ...       ...       ...        ...        ...    ...   \n336773  2013      9   30       764       NaN        NaN        NaN    NaN   \n336774  2013      9   30       419       NaN        NaN        NaN    NaN   \n336775  2013      9   30       431       NaN        NaN        NaN    NaN   \n\n        speed  \n0      370.04  \n1      374.27  \n2      408.38  \n...       ...  \n336773    NaN  \n336774    NaN  \n336775    NaN  \n\n[336776 rows x 9 columns]\n\n\n\n# 앞에서 만든 변수나 함수를 이용할 수 있음\nflights_sml.assign(\n    gain=lambda x: x.dep_delay - x.arr_delay,\n    hours=lambda x: x.air_time / 60,\n    gain_per_hour=lambda x: x.gain / x.hours,\n    rounded=lambda x: np.round(x.gain_per_hour, 1)  # use a numpy function\n)\n\n        year  month  day  distance  air_time  dep_delay  arr_delay   gain  \\\n0       2013      1    1      1400    227.00       2.00      11.00  -9.00   \n1       2013      1    1      1416    227.00       4.00      20.00 -16.00   \n2       2013      1    1      1089    160.00       2.00      33.00 -31.00   \n...      ...    ...  ...       ...       ...        ...        ...    ...   \n336773  2013      9   30       764       NaN        NaN        NaN    NaN   \n336774  2013      9   30       419       NaN        NaN        NaN    NaN   \n336775  2013      9   30       431       NaN        NaN        NaN    NaN   \n\n        hours  gain_per_hour  rounded  \n0        3.78          -2.38    -2.40  \n1        3.78          -4.23    -4.20  \n2        2.67         -11.62   -11.60  \n...       ...            ...      ...  \n336773    NaN            NaN      NaN  \n336774    NaN            NaN      NaN  \n336775    NaN            NaN      NaN  \n\n[336776 rows x 11 columns]\n\n\n\n# Find the fastest flights\n(\n    flights_sml\n    .assign(speed=lambda x: x.distance / x.air_time)\n    .sort_values(by=\"speed\", ascending=False)\n    .head(5)\n)\n\n        year  month  day  distance  air_time  dep_delay  arr_delay  speed\n216447  2013      5   25       762     65.00       9.00     -14.00  11.72\n251999  2013      7    2      1008     93.00      45.00      26.00  10.84\n205388  2013      5   13       594     55.00      15.00      -1.00  10.80\n157516  2013      3   23       748     70.00       4.00       2.00  10.69\n10223   2013      1   12      1035    105.00      -1.00     -28.00   9.86",
    "crumbs": [
      "Transform",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Transform/transform.html#groups",
    "href": "contents/Transform/transform.html#groups",
    "title": "Transforming I",
    "section": "Groups",
    "text": "Groups\n\ngroupby()\n\ngroupby()는 데이터를 의미있는 그룹으로 나누어 분석할 수 있도록 해줌\n.count(), .sum(), .mean(), .min(), .max()과 같은 통계치를 구하는 methods와 함께 효과적으로, 자주 활용됨\n\n\nSource: Ch.10 in Python for Data Analysis (3e) by Wes McKinney\n아래 표는 groupby()와 함께 자주 쓰이는 효율적인 methods\n\nSource: Ch.10 in Python for Data Analysis (3e) by Wes McKinney\n\nflights.groupby(\"month\")  # “GroupBy” object\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x157d8de10&gt;\n\n\n\nflights_sml.groupby(\"month\").mean()\n\n         year   day  distance  air_time  dep_delay  arr_delay\nmonth                                                        \n1     2013.00 15.99   1006.84    154.19      10.04       6.13\n2     2013.00 14.74   1000.98    151.35      10.82       5.61\n3     2013.00 16.00   1011.99    149.08      13.23       5.81\n...       ...   ...       ...       ...        ...        ...\n10    2013.00 15.98   1038.88    148.89       6.24      -0.17\n11    2013.00 15.29   1050.31    155.47       5.44       0.46\n12    2013.00 15.72   1064.66    162.59      16.58      14.87\n\n[12 rows x 6 columns]\n\n\n\n# 보통은 다음과 같이 특정 columns을 선택\nflights.groupby(\"month\")[\"dep_delay\"]  # [[\"dep_delay\"]] 처럼 list로 입력하면 DataFrameGroupBy object\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x157d88890&gt;\n\n\n\nflights.groupby(\"month\")[\"dep_delay\"].mean()  # Series GroupBy object에 적용된 결과는 Series\n\nmonth\n1    10.04\n2    10.82\n3    13.23\n      ... \n10    6.24\n11    5.44\n12   16.58\nName: dep_delay, Length: 12, dtype: float64\n\n\n\nflights.groupby(\"month\")[[\"dep_delay\", \"arr_delay\"]].mean().head(3)\n\n       dep_delay  arr_delay\nmonth                      \n1          10.04       6.13\n2          10.82       5.61\n3          13.23       5.81\n\n\n\nflights.groupby([\"month\", \"day\"])[\"arr_delay\"].nsmallest(1)\n\nmonth  day        \n1      1    696      -48.00\n       2    919      -59.00\n       3    2035     -65.00\n                      ...  \n12     29   108914   -60.00\n       30   110330   -45.00\n       31   111113   -44.00\nName: arr_delay, Length: 365, dtype: float64\n\n\n\n\n\n\n\n\nTip\n\n\n\nMulti-index의 level을 drop하려면 droplevel()\nflights.groupby([\"month\", \"day\"])[\"arr_delay\"].nsmallest(1).droplevel(2)\n# month  day\n# 1      1     -48.00\n#        2     -59.00\n#        3     -65.00\n#               ...  \n# 12     29    -60.00\n#        30    -45.00\n#        31    -44.00\n# Name: arr_delay, Length: 365, dtype: float64\n\n\n\nflights.groupby([\"origin\", \"dest\"])[\"arr_delay\"].count()\n\norigin  dest\nEWR     ALB      418\n        ANC        8\n        ATL     4876\n                ... \nLGA     TVC       73\n        TYS      265\n        XNA      709\nName: arr_delay, Length: 224, dtype: int64\n\n\n\n\n\n\n\n\nTip\n\n\n\nas_index=False: grouping 변수들을 index가 아닌 columns으로\nflights.groupby([\"month\", \"day\"], as_index=False)[\"arr_delay\"].mean().head(3)\n#    month  day  arr_delay\n# 0      1    1      12.65\n# 1      1    2      12.69\n# 2      1    3       5.73\n물론, 결과물에 .reset_index() method를 사용해도 됨\n\n\n\n\n\n\n\n\nNote\n\n\n\n원칙적으로 grouping은 같은 DataFrame의 변수일 필요없이 match만 되면 됨\nSource: Wes McKinney’s\n# df\n#    key1  key2  data1  data2\n# 0     a     1   0.36  -0.42\n# 1     a     2  -1.51   0.04\n# 2  None     1   0.75  -0.28\n# 3     b     2   0.57   0.25\n# 4     b     1   1.30  -0.77\n# 5     a  &lt;NA&gt;  -0.53  -0.73\n# 6  None     1   2.04  -0.37\n\nstates = np.array([\"OH\", \"CA\", \"CA\", \"OH\", \"OH\", \"CA\", \"OH\"])\nyears = [2005, 2005, 2006, 2005, 2006, 2005, 2006]\n\ndf[\"data1\"].groupby([states, years]).mean()\n# CA  2005   -1.02\n#     2006    0.75\n# OH  2005    0.46\n#     2006    1.67\n# Name: data1, dtype: float64\n\n\n\nflights.groupby(\"dest\").size()  # .size(): group의 사이즈/열의 갯수\n\ndest\nABQ     254\nACK     265\nALB     439\n       ... \nTVC     101\nTYS     631\nXNA    1036\nLength: 105, dtype: int64\n\n\n\nflights.groupby(\"tailnum\", dropna=False).size()  # groupby는 기본적으로 NA 무시\n\ntailnum\nD942DN       4\nN0EGMQ     371\nN10156     153\n          ... \nN999DN      61\nN9EAMQ     248\nNaN       2512\nLength: 4044, dtype: int64\n\n\n\n\n\n\n\n\nNote\n\n\n\n.size()는 .value_counts()와 유사하게 사용될 수 있음\nflights[\"tailnum\"].value_counts(dropna=False)\n# NaN       2512\n# N725MQ     575\n# N722MQ     513\n#           ... \n# N318AS       1\n# N651UA       1\n# N557AS       1\n# Name: tailnum, Length: 4044, dtype: int64\n\n\n\n\n\n\n\n\nNote\n\n\n\nIndex에 grouping하는 방식에 대해서는 Wes McKineey’s Chapter 10 참고\n\nGrouping with Dictionaries and Series\nGrouping with Functions\nGrouping by Index Levels\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTime series 데이터의 경우 다양한 grouping 방식이 존재\nStephanie Molin의 ch.4 Working with time series data 참고\n예를 들어, facebook stock에 대한 자료에서 2018년 4분기에 해당하는 날을 week단위로 그룹핑하여 volume을 다음과 같이 계산할 수 있음\nfb.loc['2018-Q4'].groupby(pd.Grouper(freq='W')).volume.sum()\n\n\n\n\n\n\n\n\nTip\n\n\n\ngroupby filtering\nflights.groupby([\"year\", \"month\", \"day\"]).filter(lambda x: x[\"arr_delay\"].mean() &lt; 0)\n좀 더 복잡한 groupby filtering은 이후 apply를 이용\n\n\n\n\n\nagg()\nAggregations: data transformation that produces scalar values from arrays\n앞서 GroupBy object에 직접 stats function을 적용하였는데, agg()를 이용하여 더 확장, 일반화할 수 있음\n\n# 모두 동일\nflights_sml.groupby(\"month\").mean()\nflights_sml.groupby(\"month\").agg(\"mean\")  # function names\nflights_sml.groupby(\"month\").agg(np.mean)  # numpy functions\nflights_sml.groupby(\"month\").agg(lambda x: x.sum() / x.count())  # general functions; not optimized!\n\n\n\n         year   day  distance  air_time  dep_delay  arr_delay\nmonth                                                        \n1     2013.00 15.99   1006.84    154.19      10.04       6.13\n2     2013.00 14.74   1000.98    151.35      10.82       5.61\n3     2013.00 16.00   1011.99    149.08      13.23       5.81\n...       ...   ...       ...       ...        ...        ...\n10    2013.00 15.98   1038.88    148.89       6.24      -0.17\n11    2013.00 15.29   1050.31    155.47       5.44       0.46\n12    2013.00 15.72   1064.66    162.59      16.58      14.87\n\n[12 rows x 6 columns]\n\n\n\nflights_sml.groupby(\"month\")[\"arr_delay\"].agg(\"mean\")  # Series에 string으로 함수 적용\n\nmonth\n1     6.13\n2     5.61\n3     5.81\n      ... \n10   -0.17\n11    0.46\n12   14.87\nName: arr_delay, Length: 12, dtype: float64\n\n\n\n\n\n\n\n\nTip\n\n\n\n차이 참고\nflights_sml.groupby(\"month\")[\"arr_delay\"].agg([\"mean\"])  # list로 함수를 입력하는 경우\nflights_sml.groupby(\"month\")[[\"arr_delay\"]].agg(\"mean\")  # DataFrame에 적용될 때\n#        mean                    arr_delay\n# month                 # month\n# 1      6.13           # 1           6.13\n# 2      5.61           # 2           5.61\n# 3      5.81           # 3           5.81\n# ...                   # ...\n\n\n\nflights.groupby(\"month\")[\"dep_delay\"].agg([\"mean\", \"count\"])\n\n       mean  count\nmonth             \n1     10.04  26483\n2     10.82  23690\n3     13.23  27973\n...     ...    ...\n10     6.24  28653\n11     5.44  27035\n12    16.58  27110\n\n[12 rows x 2 columns]\n\n\n\nflights_sml.groupby(\"month\")[[\"arr_delay\"]].agg([\"mean\", \"median\"])\n\n      arr_delay       \n           mean median\nmonth                 \n1          6.13  -3.00\n2          5.61  -3.00\n3          5.81  -6.00\n...         ...    ...\n10        -0.17  -7.00\n11         0.46  -6.00\n12        14.87   2.00\n\n[12 rows x 2 columns]\n\n\n\n# DataFrame Groupby object의 경우 column 별로 적용\nflights_sml.groupby(\"month\")[[\"arr_delay\", \"dep_delay\"]].agg(\"mean\")\n\n       arr_delay  dep_delay\nmonth                      \n1           6.13      10.04\n2           5.61      10.82\n3           5.81      13.23\n...          ...        ...\n10         -0.17       6.24\n11          0.46       5.44\n12         14.87      16.58\n\n[12 rows x 2 columns]\n\n\n\nflights_sml.groupby(\"month\")[[\"arr_delay\", \"dep_delay\"]].agg([\"mean\", \"count\"])\n\n      arr_delay        dep_delay       \n           mean  count      mean  count\nmonth                                  \n1          6.13  26398     10.04  26483\n2          5.61  23611     10.82  23690\n3          5.81  27902     13.23  27973\n...         ...    ...       ...    ...\n10        -0.17  28618      6.24  28653\n11         0.46  26971      5.44  27035\n12        14.87  27020     16.58  27110\n\n[12 rows x 4 columns]\n\n\n컬럼 별로 다른 함수를 적용하고자 할 때 (컬럼 이름을 지정): tuple\n\nflights_agg = (\n    flights.groupby(\"month\")\n    .agg(\n        air_min=(\"air_time\", \"min\"),  # 컬럼 이름 지정 = (컬럼, 함수)\n        air_max=(\"air_time\", \"max\"),\n        dep_mean=(\"dep_delay\", \"mean\"),\n        arr_median=(\"arr_delay\", \"median\"),\n    )\n)\n\nflights_agg\n\n       air_min  air_max  dep_mean  arr_median\nmonth                                        \n1        20.00   667.00     10.04       -3.00\n2        21.00   691.00     10.82       -3.00\n3        21.00   695.00     13.23       -6.00\n...        ...      ...       ...         ...\n10       23.00   642.00      6.24       -7.00\n11       24.00   676.00      5.44       -6.00\n12       21.00   661.00     16.58        2.00\n\n[12 rows x 4 columns]\n\n\n\n\n\n\n\n\nNote\n\n\n\n컬럼별 다른 함수를 적용하는 다른 방식: dictionary\nflights_agg = flights.groupby(\"month\").agg({\n    \"air_time\": [\"min\", \"max\"],\n    \"dep_delay\": \"mean\",\n    \"arr_delay\": \"median\"\n})\n#       air_time        dep_delay arr_delay\n#            min    max      mean    median\n# month                                    \n# 1        20.00 667.00     10.04     -3.00\n# 2        21.00 691.00     10.82     -3.00\n# 3        21.00 695.00     13.23     -6.00\n# ...\nMultiIndex를 collapse하는 팁\nflights_agg.columns\n# MultiIndex([( 'air_time',    'min'),\n#             ( 'air_time',    'max'),\n#             ('dep_delay',   'mean'),\n#             ('arr_delay', 'median')],\n#            )\n\nflights_agg.columns = ['_'.join(col_agg) for col_agg in flights_agg.columns]\nflights_agg.head(3)\n#        air_time_min  air_time_max  dep_delay_mean  arr_delay_median\n# month                                                              \n# 1             20.00        667.00           10.04             -3.00\n# 2             21.00        691.00           10.82             -3.00\n# 3             21.00        695.00           13.23             -6.00\n\n\n\n\n\n\n\n\nNote\n\n\n\nagg()에는 custom function을 pass할 수 있음\n단, the optimized functions (Table 10-1)에 비해 일반적으로 훨씬 느림\n\n\n\ndef peak_to_peak(arr):\n    return arr.max() - arr.min()\n\n\ngrouped_dist = flights_sml.groupby([\"month\", \"day\"])[\"distance\"]\n\ngrouped_dist.agg([\"std\", peak_to_peak])  # a list of functions\n\n             std  peak_to_peak\nmonth day                     \n1     1   727.73          4889\n      2   721.72          4889\n      3   714.95          4903\n...          ...           ...\n12    29  728.78          4887\n      30  723.88          4887\n      31  731.36          4887\n\n[365 rows x 2 columns]\n\n\n\n# Naming a function as a tuple\ngrouped_dist.agg([(\"sd\", \"std\"), (\"range\", peak_to_peak)])\n\n              sd  range\nmonth day              \n1     1   727.73   4889\n      2   721.72   4889\n      3   714.95   4903\n...          ...    ...\n12    29  728.78   4887\n      30  723.88   4887\n      31  731.36   4887\n\n[365 rows x 2 columns]\n\n\n\n\n\n\n\n\nNote\n\n\n\n.describe()는 aggregation은 아니나 grouped objects에 적용가능\nflights_sml.groupby(\"month\")[[\"dep_delay\", \"arr_delay\"]].describe()\n\n#       dep_delay                                              arr_delay        \\\n#           count  mean   std    min   25%   50%   75%     max     count  mean   \n# month                                                                          \n# 1      26483.00 10.04 36.39 -30.00 -5.00 -2.00  8.00 1301.00  26398.00  6.13   \n# 2      23690.00 10.82 36.27 -33.00 -5.00 -2.00  9.00  853.00  23611.00  5.61   \n# 3      27973.00 13.23 40.13 -25.00 -5.00 -1.00 12.00  911.00  27902.00  5.81   \n# ...         ...   ...   ...    ...   ...   ...   ...     ...       ...   ...    \n                                               \n#         std    min    25%   50%   75%     max  \n# month                                          \n# 1     40.42 -70.00 -15.00 -3.00 13.00 1272.00  \n# 2     39.53 -70.00 -15.00 -3.00 13.00  834.00  \n# 3     44.12 -68.00 -18.00 -6.00 13.00  915.00  \n# ...     ...    ...    ...   ...   ...     ...  \n\n# [12 rows x 16 columns]\n\n\n\n\ntransform()\n앞서 group별로 통계치가 summary되어 원래 reduced 데이터프레임으로 변형됐다면,\ntransform()은 group별로 얻은 통계치가 원래 데이터의 형태를 그대로 보존하면서 출력\n만약, 전달되는 함수가 Series를 반환하려면, 동일한 사이즈로 반환되어야 함.\n\n\nflights_sml.groupby([\"month\"])[\"arr_delay\"].mean()\n\nmonth\n1     6.13\n2     5.61\n3     5.81\n      ... \n10   -0.17\n11    0.46\n12   14.87\nName: arr_delay, Length: 12, dtype: float64\n\n\n\ngrouped_delay = flights_sml.groupby([\"month\"])[\"arr_delay\"].transform(\"mean\")\ngrouped_delay\n\n0         6.13\n1         6.13\n2         6.13\n          ... \n336773   -4.02\n336774   -4.02\n336775   -4.02\nName: arr_delay, Length: 336776, dtype: float64\n\n\n\nflights_sml[\"monthly_delay\"] = grouped_delay\nflights_sml\n\n        year  month  day  distance  air_time  dep_delay  arr_delay  \\\n0       2013      1    1      1400    227.00       2.00      11.00   \n1       2013      1    1      1416    227.00       4.00      20.00   \n2       2013      1    1      1089    160.00       2.00      33.00   \n...      ...    ...  ...       ...       ...        ...        ...   \n336773  2013      9   30       764       NaN        NaN        NaN   \n336774  2013      9   30       419       NaN        NaN        NaN   \n336775  2013      9   30       431       NaN        NaN        NaN   \n\n        monthly_delay  \n0                6.13  \n1                6.13  \n2                6.13  \n...               ...  \n336773          -4.02  \n336774          -4.02  \n336775          -4.02  \n\n[336776 rows x 8 columns]\n\n\nQ: 1년에 10000편 이상 운항편이 있는 도착지로 가는 항공편들만 추리면,\n\ndest_size =  flights.groupby(\"dest\").transform(\"size\")\ndest_size\n\n# 또는 flights.groupby(\"dest\")[\"dest\"].transform(\"count\")\n\n0          7198\n1          7198\n2         11728\n          ...  \n336773     6333\n336774     4573\n336775     8163\nLength: 336776, dtype: int64\n\n\n\n# 1년에 10000편 이상 운항편이 있는 도착지에 대한 항공편\nflights[dest_size &gt;= 10000]\n\n        year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n2       2013      1    1    542.00             540       2.00    923.00   \n4       2013      1    1    554.00             600      -6.00    812.00   \n5       2013      1    1    554.00             558      -4.00    740.00   \n...      ...    ...  ...       ...             ...        ...       ...   \n336762  2013      9   30   2233.00            2113      80.00    112.00   \n336763  2013      9   30   2235.00            2001     154.00     59.00   \n336768  2013      9   30   2307.00            2255      12.00   2359.00   \n\n        sched_arr_time  arr_delay carrier  flight tailnum origin dest  \\\n2                  850      33.00      AA    1141  N619AA    JFK  MIA   \n4                  837     -25.00      DL     461  N668DN    LGA  ATL   \n5                  728      12.00      UA    1696  N39463    EWR  ORD   \n...                ...        ...     ...     ...     ...    ...  ...   \n336762              30      42.00      UA     471  N578UA    EWR  SFO   \n336763            2249     130.00      B6    1083  N804JB    JFK  MCO   \n336768            2358       1.00      B6     718  N565JB    JFK  BOS   \n\n        air_time  distance  hour  minute  \n2         160.00      1089     5      40  \n4         116.00       762     6       0  \n5         150.00       719     5      58  \n...          ...       ...   ...     ...  \n336762    318.00      2565    21      13  \n336763    123.00       944    20       1  \n336768     33.00       187    22      55  \n\n[131440 rows x 18 columns]\n\n\nQ: 하루 중 출발 지연이 가장 늦은 두 항공편들을 매일 각각 구하면,\n\ndef get_ranks(group):\n    return group.rank(ascending=False, method=\"min\") # method: 동일 등수에 대한 처리방식\n\ndelay_rank = flights.groupby([\"month\", \"day\"])[\"dep_delay\"].transform(get_ranks)\n# 또는 .transform(\"rank\", ascending=False, method=\"min\")  # using \"rank\" 함수\n\ndelay_rank\n\n0        302.00\n1        269.00\n2        302.00\n          ...  \n336773      NaN\n336774      NaN\n336775      NaN\nName: dep_delay, Length: 336776, dtype: float64\n\n\n\nflights[delay_rank &lt; 3].head(6)\n\n      year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n151   2013      1    1    848.00            1835     853.00   1001.00   \n834   2013      1    1   2343.00            1724     379.00    314.00   \n1440  2013      1    2   1607.00            1030     337.00   2003.00   \n1749  2013      1    2   2131.00            1512     379.00   2340.00   \n2598  2013      1    3   2008.00            1540     268.00   2339.00   \n2637  2013      1    3   2056.00            1605     291.00   2239.00   \n\n      sched_arr_time  arr_delay carrier  flight tailnum origin dest  air_time  \\\n151             1950     851.00      MQ    3944  N942MQ    JFK  BWI     41.00   \n834             1938     456.00      EV    4321  N21197    EWR  MCI    222.00   \n1440            1355     368.00      AA     179  N324AA    JFK  SFO    346.00   \n1749            1741     359.00      UA     488  N593UA    LGA  DEN    228.00   \n2598            1909     270.00      DL    2027  N338NW    JFK  FLL    158.00   \n2637            1754     285.00      9E    3459  N928XJ    JFK  BNA    125.00   \n\n      distance  hour  minute  \n151        184    18      35  \n834       1092    17      24  \n1440      2586    10      30  \n1749      1620    15      12  \n2598      1069    15      40  \n2637       765    16       5  \n\n\nQ: Standarize air time by destination\n\ndest_air = flights.groupby(\"dest\")[\"air_time\"]\n\n\n# Z = (x - mean) / std\n(flights['air_time'] - dest_air.transform('mean')) / dest_air.transform('std')\n\n0        1.73\n1        1.73\n2        0.61\n         ... \n336773    NaN\n336774    NaN\n336775    NaN\nName: air_time, Length: 336776, dtype: float64\n\n\n\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\n\ndest_air.transform(standardize)\n\n0        1.73\n1        1.73\n2        0.61\n         ... \n336773    NaN\n336774    NaN\n336775    NaN\nName: air_time, Length: 336776, dtype: float64\n\n\n\n\napply()\nApply: General split-apply-combine in McKineey’s Chapter 10.3 참고\n\nThe most general-purpose GroupBy method is apply, which is the subject of this section. apply splits the object being manipulated into pieces, invokes the passed function on each piece, and then attempts to concatenate the pieces.\n\n\ntips = sns.load_dataset(\"tips\")\ntips = tips.assign(tip_pct = lambda x: x.tip / x.total_bill)\ntips.head(3)\n\n   total_bill  tip     sex smoker  day    time  size  tip_pct\n0       16.99 1.01  Female     No  Sun  Dinner     2     0.06\n1       10.34 1.66    Male     No  Sun  Dinner     3     0.16\n2       21.01 3.50    Male     No  Sun  Dinner     3     0.17\n\n\n\ndef top(df, n=5, column=\"tip_pct\"):\n    return df.sort_values(column, ascending=False)[:n]  # DataFrame을 반환\n\n\ntop(tips, n=4)\n\n     total_bill  tip     sex smoker  day    time  size  tip_pct\n172        7.25 5.15    Male    Yes  Sun  Dinner     2     0.71\n178        9.60 4.00  Female    Yes  Sun  Dinner     2     0.42\n67         3.07 1.00  Female    Yes  Sat  Dinner     1     0.33\n232       11.61 3.39    Male     No  Sat  Dinner     2     0.29\n\n\n\n# DataFrame Groupby ojbect에 적용\ntips.groupby(\"time\", observed=False).apply(top)\n\n            total_bill  tip     sex smoker   day    time  size  tip_pct\ntime                                                                   \nLunch  149        7.51 2.00    Male     No  Thur   Lunch     2     0.27\n       221       13.42 3.48  Female    Yes   Fri   Lunch     2     0.26\n       194       16.58 4.00    Male    Yes  Thur   Lunch     2     0.24\n...                ...  ...     ...    ...   ...     ...   ...      ...\nDinner 67         3.07 1.00  Female    Yes   Sat  Dinner     1     0.33\n       232       11.61 3.39    Male     No   Sat  Dinner     2     0.29\n       183       23.17 6.50    Male    Yes   Sun  Dinner     4     0.28\n\n[10 rows x 8 columns]\n\n\n\ntips.groupby([\"time\", \"day\"], observed=False).apply(top, n=1, column=\"total_bill\")\n\n                 total_bill   tip     sex smoker   day    time  size  tip_pct\ntime   day                                                                   \nLunch  Thur 197       43.11  5.00  Female    Yes  Thur   Lunch     4     0.12\n       Fri  225       16.27  2.50  Female    Yes   Fri   Lunch     2     0.15\nDinner Thur 243       18.78  3.00  Female     No  Thur  Dinner     2     0.16\n       Fri  95        40.17  4.73    Male    Yes   Fri  Dinner     4     0.12\n       Sat  170       50.81 10.00    Male    Yes   Sat  Dinner     3     0.20\n       Sun  156       48.17  5.00    Male     No   Sun  Dinner     6     0.10\n\n\nGroupBy안에서 describe()와 같은 method를 적용하면, 사실 다음과 같은 shortcut임\ndef f(group):\n    return group.describe()\n\ntips.groupby(\"sex\", observed=False)[\"tip_pct\"].describe().T  # T: transpose\n\nsex     Male  Female\ncount 157.00   87.00\nmean    0.16    0.17\nstd     0.06    0.05\n...      ...     ...\n50%     0.15    0.16\n75%     0.19    0.19\nmax     0.71    0.42\n\n[8 rows x 2 columns]\n\n\n\ndef mean2(group):\n    return group.mean()  # Series를 반환!\n\n\ntips.groupby(\"time\", observed=False)[[\"total_bill\", \"tip\"]].apply(mean2)\n\n        total_bill  tip\ntime                   \nLunch        17.17 2.73\nDinner       20.80 3.10\n\n\n\n\n\n\n\n\nImportant\n\n\n\n적용되는 함수는 pandas object나 scalar value를 반환해야 함\n\n\n\n\n\n\n\n\nNote\n\n\n\n위에서 agg()에 custom function을 pass할 수 있지만, 일반적으로 훨씬 느리다고 했는데,\n이는 numeric array에 apply() method를 사용할 때에도 해당됨.\n가능하다면, apply()를 피하는 것이 실행 속도를 크게 높일 수 있음. (단, “string”에 적용될 때는 차이 없음)\n\n\n\n\n\n\n\n\nSuppressing the Group Keys\n\n\n\ntips.groupby(\"time\", group_keys=False).apply(top)\n#      total_bill  tip     sex smoker   day    time  size  tip_pct\n# 149        7.51 2.00    Male     No  Thur   Lunch     2     0.27\n# 221       13.42 3.48  Female    Yes   Fri   Lunch     2     0.26\n# 194       16.58 4.00    Male    Yes  Thur   Lunch     2     0.24\n# ...\n\n\n\n\n\n\n\n\nHide warnings\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\npandas method 비교\n\napplymap: element-wise, DataFrame method\nmap: element-wise, Series method\napply: column/row-wise, DataFrame method, 또는 element-wise, Series method\n\n\nSource, 비교 참고\n\ndef my_format(x):\n    return f\"{x:.1f}\"\n\n\ntips_num = tips.select_dtypes(\"number\")  # select numeric columns\n\n# element-wise, DataFrame method\ntips_num.applymap(my_format)\n\n    total_bill  tip size tip_pct\n0         17.0  1.0  2.0     0.1\n1         10.3  1.7  3.0     0.2\n2         21.0  3.5  3.0     0.2\n..         ...  ...  ...     ...\n241       22.7  2.0  2.0     0.1\n242       17.8  1.8  2.0     0.1\n243       18.8  3.0  2.0     0.2\n\n[244 rows x 4 columns]\n\n\n\n# element-wise, Series method\ntips[\"tip\"].map(my_format)\n\n0      1.0\n1      1.7\n2      3.5\n      ... \n241    2.0\n242    1.8\n243    3.0\nName: tip, Length: 244, dtype: object\n\n\n\ndef peak_to_peak(arr):\n    return arr.max() - arr.min()\n\n\n# column-wise operation\ntips_num.apply(peak_to_peak)\n\n# row-wise operation\ntips_num.apply(peak_to_peak, axis=\"columns\")\n\n\n\n\n\n\n\ntotal_bill   47.74\ntip           9.00\nsize          5.00\ntip_pct       0.67\ndtype: float64\n\n\n0     16.93\n1     10.18\n2     20.84\n       ... \n241   22.58\n242   17.72\n243   18.62\nLength: 244, dtype: float64\n\n\n\nGroupby object에 적용할 때 비교\napply에 패스되는 함수가 Series나 DataFrame를 반환하는 경우\n\ndef f2(x):\n    return pd.Series([x.min(), x.max()], index=[\"min\", \"max\"])  # Series를 반환\n\n\n# apply에 패스되는 함수가 Series를 반환하는 경우\ntips_num.apply(f2)\n\n     total_bill   tip  size  tip_pct\nmin        3.07  1.00     1     0.04\nmax       50.81 10.00     6     0.71\n\n\n\n# Series Groupby object의 경우\ntips.groupby(\"time\", observed=False)[\"tip\"].apply(f2)\n\ntime       \nLunch   min    1.25\n        max    6.70\nDinner  min    1.00\n        max   10.00\nName: tip, dtype: float64\n\n\n\n# DataFrame GroupBy object의 경우\ndef f3(g):\n    x = g[\"tip\"]\n    return pd.Series([x.min(), x.max()], index=[\"min\", \"max\"])\n\ntips.groupby(\"time\", observed=False).apply(f3)\n\n        min   max\ntime             \nLunch  1.25  6.70\nDinner 1.00 10.00\n\n\nGroupby filtering\n앞서 다음과 같이 .filter()를 이용하여 groupby filtering을 했는데, apply를 이용하여 좀 더 복잡한 filtering을 할 수 있음\nflights.groupby([\"year\", \"month\", \"day\"]).filter(lambda x: x[\"arr_delay\"].mean() &lt; 0)\n다음과 같은 방식으로 apply를 이용하여 filtering하면 위와 동일함.\n\ndef less_than(group, t):\n    condition = group[\"arr_delay\"].mean() &lt; t\n    return group if condition else None\n    \nflights.groupby([\"year\", \"month\", \"day\"]).apply(less_than, 0)\n\n가령, 도착지연이 그날 하루 평균보다 낮았던 항공편들만 추리기\n\ndef less_than_mean(group):\n    idx = group[\"arr_delay\"] &lt; group[\"arr_delay\"].mean()\n    return group[idx]\n\nflights.groupby([\"year\", \"month\", \"day\"]).apply(less_than_mean)\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\nyear\nmonth\nday\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2013\n1\n1\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n3\n2013\n1\n1\n544.00\n545\n-1.00\n1004.00\n1022\n-18.00\nB6\n725\nN804JB\nJFK\nBQN\n183.00\n1576\n5\n45\n\n\n4\n2013\n1\n1\n554.00\n600\n-6.00\n812.00\n837\n-25.00\nDL\n461\nN668DN\nLGA\nATL\n116.00\n762\n6\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12\n31\n111276\n2013\n12\n31\n2328.00\n2330\n-2.00\n412.00\n409\n3.00\nB6\n1389\nN651JB\nEWR\nSJU\n198.00\n1608\n23\n30\n\n\n111278\n2013\n12\n31\n2355.00\n2359\n-4.00\n430.00\n440\n-10.00\nB6\n1503\nN509JB\nJFK\nSJU\n195.00\n1598\n23\n59\n\n\n111279\n2013\n12\n31\n2356.00\n2359\n-3.00\n436.00\n445\n-9.00\nB6\n745\nN665JB\nJFK\nPSE\n200.00\n1617\n23\n59\n\n\n\n\n207909 rows × 18 columns\n\n\n\ngrouping 변수를 제거하고 깔끔하게 하려면; group_keys=False\n\nflights.groupby([\"year\", \"month\", \"day\"], group_keys=False).apply(less_than_mean)\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\n\n\n\n\n0\n2013\n1\n1\n517.00\n515\n2.00\n830.00\n819\n11.00\nUA\n1545\nN14228\nEWR\nIAH\n227.00\n1400\n5\n15\n\n\n3\n2013\n1\n1\n544.00\n545\n-1.00\n1004.00\n1022\n-18.00\nB6\n725\nN804JB\nJFK\nBQN\n183.00\n1576\n5\n45\n\n\n4\n2013\n1\n1\n554.00\n600\n-6.00\n812.00\n837\n-25.00\nDL\n461\nN668DN\nLGA\nATL\n116.00\n762\n6\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n111276\n2013\n12\n31\n2328.00\n2330\n-2.00\n412.00\n409\n3.00\nB6\n1389\nN651JB\nEWR\nSJU\n198.00\n1608\n23\n30\n\n\n111278\n2013\n12\n31\n2355.00\n2359\n-4.00\n430.00\n440\n-10.00\nB6\n1503\nN509JB\nJFK\nSJU\n195.00\n1598\n23\n59\n\n\n111279\n2013\n12\n31\n2356.00\n2359\n-3.00\n436.00\n445\n-9.00\nB6\n745\nN665JB\nJFK\nPSE\n200.00\n1617\n23\n59\n\n\n\n\n207909 rows × 18 columns\n\n\n\n\n\n\n\n\n\nCustom methods\n\n\n\n다음과 같이 pandas object에 custom method를 추가할 수 있음\nimport pandas as pd\n\ndef select_cols(self, *cols):\n    \"\"\"\n    Select a subset of columns from the DataFrame.\n\n    Parameters:\n    - cols (tuple of str): Column names to select. Use a colon (:) to specify a range of columns.\n\n    Returns:\n    - pd.DataFrame: The DataFrame with selected columns.\n\n    Example:\n    - tips.select(\"tip\", \"day:size\")\n    \"\"\"\n    selected_cols = []\n    for col in cols:\n        if \":\" in col:\n            start, end = col.split(\":\")\n            columns = self.loc[:, start:end].columns\n            selected_cols.extend(columns)\n        else:\n            selected_cols.append(col)\n\n    return self[selected_cols]\n\n# Add the custom method to the DataFrame class\npd.DataFrame.select = select_cols\n\n\nflights.select(\"origin\", \"dest\", \"dep_delay:arr_delay\").head()\n#   origin dest  dep_delay  arr_time  sched_arr_time  arr_delay\n# 0    EWR  IAH       2.00    830.00             819      11.00\n# 1    LGA  IAH       4.00    850.00             830      20.00\n# 2    JFK  MIA       2.00    923.00             850      33.00\n# 3    JFK  BQN      -1.00   1004.00            1022     -18.00\n# 4    LGA  ATL      -6.00    812.00             837     -25.00\n\n# Build useful methods\nflights.deselect()\nflights.relocate()",
    "crumbs": [
      "Transform",
      "Transforming I"
    ]
  },
  {
    "objectID": "contents/Visualize/alt_plots.html",
    "href": "contents/Visualize/alt_plots.html",
    "title": "Alternative plots",
    "section": "",
    "text": "Load packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")",
    "crumbs": [
      "Visualize",
      "Plots",
      "Alternative plots"
    ]
  },
  {
    "objectID": "contents/Visualize/alt_plots.html#fitted-lines",
    "href": "contents/Visualize/alt_plots.html#fitted-lines",
    "title": "Alternative plots",
    "section": "Fitted lines",
    "text": "Fitted lines\nalternatives: plotly, seaborn\n\nmpg = sm.datasets.get_rdataset(\"mpg\", \"ggplot2\").data\n\n\nPlotly\nLinear and Non-Linear Trendlines in Python\n\nimport plotly.express as px\n(\n    px.scatter(mpg, x=\"displ\", y=\"hwy\", color=\"drv\", trendline=\"lowess\")\n    .update_layout(width=700, height=500)\n)\n\n                                                \n\n\n다음과 같이 smoothing parameter를 지정할 수 있음\n\n(\n    px.scatter(mpg, x=\"displ\", y=\"hwy\", color=\"drv\", \n               trendline=\"lowess\", trendline_options=dict(frac=0.3)) # smoothing parameter \n    .update_layout(width=700, height=500)\n)\n\n                                                \n\n\n\npenguins = sns.load_dataset(\"penguins\")\n\n(\n    px.scatter(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\",\n               trendline=\"lowess\", trendline_options=dict(frac=0.5),\n               facet_col=\"island\", # faceting\n               opacity=0.5) # alpha\n    .update_layout(width=900, height=400)\n)\n\n                                                \n\n\n\n\n\nSeaborn: lmplot()\nsns.lmplot(mpg, x=\"displ\", y=\"hwy\", hue=\"drv\", # color대신 hue\n           lowess=True, \n           scatter_kws={\"alpha\":.5, \"s\":20}, # s: point size\n           height=3, aspect=5/3) \nplt.show() # 생략해도 무방\n\n\n\n\n\n\n\n\npenguins = sns.load_dataset(\"penguins\")\n\nsns.lmplot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", \n           lowess=True, \n           col=\"sex\", # faceting: col, row\n           height=3, scatter_kws={\"alpha\":.5, \"s\":5})\nplt.show()",
    "crumbs": [
      "Visualize",
      "Plots",
      "Alternative plots"
    ]
  },
  {
    "objectID": "contents/Visualize/alt_plots.html#box-plot",
    "href": "contents/Visualize/alt_plots.html#box-plot",
    "title": "Alternative plots",
    "section": "Box plot",
    "text": "Box plot\n\ntips = sns.load_dataset(\"tips\")\n\n\npx.box(tips, x=\"sex\", y=\"tip\", color=\"day\", facet_col=\"time\")",
    "crumbs": [
      "Visualize",
      "Plots",
      "Alternative plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html",
    "href": "contents/Visualize/plots.html",
    "title": "Plots",
    "section": "",
    "text": "source: R for Data Science\n\nTransform (데이터 변형)\n\n데이터의 변수들 중 일부를 선택하기\n필요한 부분를 필터링하기\n기존의 변수들로 새로운 변수 만들기\n요약자료를 계산하기\n\nVisualise (시각화)\n\n시각화를 통해 데이터가 품고 있는 정보를 파악하여 데이터에 대한 이해를 높임\n\nModel (모형)\n\n시각화와 데이터 변형의 두 가지를 병행하면서 호기심과 의구심을 갖고 연구자가 자신의 관심사에 답을 구하는 탐색적 분석을 하는 과정\n이 과정에서 모형을 세우고 데이터를 얼마나 잘 설명하는지를 살펴보고, 모형을 수정해 나가는 과정을 거침",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#데이터-분석의-과정",
    "href": "contents/Visualize/plots.html#데이터-분석의-과정",
    "title": "Plots",
    "section": "",
    "text": "source: R for Data Science\n\nTransform (데이터 변형)\n\n데이터의 변수들 중 일부를 선택하기\n필요한 부분를 필터링하기\n기존의 변수들로 새로운 변수 만들기\n요약자료를 계산하기\n\nVisualise (시각화)\n\n시각화를 통해 데이터가 품고 있는 정보를 파악하여 데이터에 대한 이해를 높임\n\nModel (모형)\n\n시각화와 데이터 변형의 두 가지를 병행하면서 호기심과 의구심을 갖고 연구자가 자신의 관심사에 답을 구하는 탐색적 분석을 하는 과정\n이 과정에서 모형을 세우고 데이터를 얼마나 잘 설명하는지를 살펴보고, 모형을 수정해 나가는 과정을 거침",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#first-steps",
    "href": "contents/Visualize/plots.html#first-steps",
    "title": "Plots",
    "section": "First steps",
    "text": "First steps\n\n\nLoad packages\n# numerical calculation & data frames\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\n# statistics\nimport statsmodels.api as sm\n\n# pandas options\npd.set_option('mode.copy_on_write', True)  # pandas 2.0\npd.options.display.float_format = '{:.2f}'.format  # pd.reset_option('display.float_format')\npd.options.display.max_rows = 7  # max number of rows to display\n\n# NumPy options\nnp.set_printoptions(precision = 2, suppress=True)  # suppress scientific notation\n\n# For high resolution display\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n\n\nData: Fuel economy data from 1999 to 2008 for 38 popular models of cars\n\n# import the dataset\nmpg_data = sm.datasets.get_rdataset(\"mpg\", \"ggplot2\")\nmpg = mpg_data.data\n\n\n# Description\nprint(mpg_data.__doc__)\n\n\nmpg\n\n    manufacturer   model  displ  year  cyl       trans drv  cty  hwy fl  \\\n0           audi      a4   1.80  1999    4    auto(l5)   f   18   29  p   \n1           audi      a4   1.80  1999    4  manual(m5)   f   21   29  p   \n2           audi      a4   2.00  2008    4  manual(m6)   f   20   31  p   \n..           ...     ...    ...   ...  ...         ...  ..  ...  ... ..   \n231   volkswagen  passat   2.80  1999    6    auto(l5)   f   16   26  p   \n232   volkswagen  passat   2.80  1999    6  manual(m5)   f   18   26  p   \n233   volkswagen  passat   3.60  2008    6    auto(s6)   f   17   26  p   \n\n       class  \n0    compact  \n1    compact  \n2    compact  \n..       ...  \n231  midsize  \n232  midsize  \n233  midsize  \n\n[234 rows x 11 columns]\n\n\nQ: 엔진의 크기(displ)와 연비(hwy)는 어떤 관계에 있는가?\n\n# Scatter plot: 산포도\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\") # empty plot을 생성하고, x, y축에 mapping할 mpg 데이터의 변수를 지정\n    .add(so.Dot()) # layer를 추가하여, points들을 Dot이라는 mark object를 써서 표현\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayer-specific mappings\n\n\n\nGlobal vs. local mapping\n다음과 같이 첫번째 layer 안에서 x, y를 mapping하는 경우, 이후 새로 추가되는 layer에는 그 mapping이 적용되지 않음\n(\n    so.Plot(mpg)\n    .add(so.Dot(), x=\"displ\", y=\"hwy\") # 이 layer에서만 mapping이 유효\n)\n\n\n\n\n\n\n\n\nTip\n\n\n\n다음과 같이 x, y를 생략하거나 간략히 할 수 있으나…\nso.Plot(mpg, \"displ\", \"hwy\").add(so.Dot())\n\n\n\n카테고리 변수인 경우\n\ncyl (실린더 개수), hwy (고속도로 연비)의 관계를 scatterplot으로 살펴볼 수 있는가? (left)\nclass (차량 타입), drv (전륜 구동, 후륜 구동, 4륜 구동 타입)의 관계는 어떠한가? (right)",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#aesthetic-mappings",
    "href": "contents/Visualize/plots.html#aesthetic-mappings",
    "title": "Plots",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\nQ: 엔진의 크기와 연비와의 관계에서 보이는 트렌드 라인에서 심하게 벗어난 것이 있는가?\n\n\n\n\n\n\n\n\n\n 변수들을 x, y라는 position에 mapping하는 것에 추가하여 다음과 같은 속성(aesthetic)에 mapping할 수 있음\n색(color), 크기(pointsize), 모양(marker), 선 종류(linestyle), 투명도(alpha)\n\n\nColor\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\", color=\"class\")\n    .add(so.Dot())\n)\n\n\n\n\n\n\n\n\n\n\nPointsize\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\", pointsize=\"class\")\n    .add(so.Dot())\n)\n\n\n\n\n\n\n\n\n\n\nMarker\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\", marker=\"class\")\n    .add(so.Dot())\n)\n\n\n\n\n\n\n\n\n\n\nAlpha\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\", alpha=\"class\")\n    .add(so.Dot())\n)\n\n\n\n\n\n\n\n\n\n\nLinestyle\n\nhealthexp = sns.load_dataset(\"healthexp\")\n\np = so.Plot(healthexp, x=\"Spending_USD\", y=\"Life_Expectancy\", linestyle=\"Country\")\np.add(so.Line())\n\n\n\n\n\n\n\n\n\n\n두 가지 이상의 속성\nex. color & marker\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\", color=\"class\", marker=\"drv\")\n    .add(so.Dot())\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\", color=\"class\", pointsize=\"drv\")\n    .add(so.Dot())\n    .scale(pointsize=(5, 15))  # pointsize의 range설정\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n아래 그림에서처럼 연속 vs. 카테고리 변수 여부에 따라 다르게 작동\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n어떤 속성을 어떤 변수에 할당하는 것이 적절한지를 선택하는 것이 기술\n예를 들어, 아래 두 플랏은 동일한 정보를 품고 있으나, 시각적 인식에 큰 차이를 만듦",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#setting-properties",
    "href": "contents/Visualize/plots.html#setting-properties",
    "title": "Plots",
    "section": "Setting properties",
    "text": "Setting properties\nSetting properties vs. mapping properties (aesthetic)\n\n변수에 속성을 할당하는 것이 아니라, graphical objects (Marks)의 속성을 지정\nMarks (Dot(), Line(), Bar(), …) 마다 설정할 수 있는 속성이 다름\n주로 쓰이는 속성들: color, pointsize, alpha\n\n\n\n\n\n\nTip\n\n\n\n다양한 Mark properties에 대해서는 홈페이지 참고\nProperties of Mark objects\n\n\n\n\n\n\n\n\nNote\n\n\n\n.Dot()의 경우\nclass seaborn.objects.Dot(artist_kws=, marker=&lt;‘o’&gt;, pointsize=&lt;6&gt;, stroke=&lt;0.75&gt;, color=&lt;‘C0’&gt;, alpha=&lt;1&gt;, fill=, edgecolor=, edgealpha=, edgewidth=&lt;0.5&gt;, edgestyle=&lt;‘-’&gt;)\n.Dots()의 경우\nclass seaborn.objects.Dots(artist_kws=, marker=&lt;rc:scatter.marker&gt;, pointsize=&lt;4&gt;, stroke=&lt;0.75&gt;, color=&lt;‘C0’&gt;, alpha=&lt;1&gt;, fill=, fillcolor=, fillalpha=&lt;0.2&gt;)\nAPI reference 참고\n\n\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot(color=\"deepskyblue\")) # Mark object 안에 지정!\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot(color=\"deepskyblue\", pointsize=12, edgecolor=\"white\", edgewidth=1)) # Mark object 안에 지정!\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot(color=\"orange\", pointsize=12, marker=\"&gt;\", alpha=.4))  # Mark object 안에 지정!\n)",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#faceting",
    "href": "contents/Visualize/plots.html#faceting",
    "title": "Plots",
    "section": "Faceting",
    "text": "Faceting\n카테고리 변수들이 지니는 카테고리들(레벨)별로 나누어 그리기\nThe penguins data: penguins in the Palmer Archipelago, Antarctica.\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\n\npenguins = sns.load_dataset(\"penguins\") # load a dataset: penguins\npenguins.head()\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen           39.10          18.70             181.00   \n1  Adelie  Torgersen           39.50          17.40             186.00   \n2  Adelie  Torgersen           40.30          18.00             195.00   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen           36.70          19.30             193.00   \n\n   body_mass_g     sex  \n0      3750.00    Male  \n1      3800.00  Female  \n2      3250.00  Female  \n3          NaN     NaN  \n4      3450.00  Female  \n\n\n\n(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .add(so.Dot(alpha=.5))\n    .facet(\"sex\")  # 기본적으로 columns으로 나누어져 그림, wrap: column에 몇 개까지 그릴지\n)\n\n\n\n\n\n\n\n\n\np = (\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .facet(col=\"species\", row=\"sex\")\n    .add(so.Dot(alpha=.5))\n)\np\n\n\n\n\n\n\n\n\n\n# x, y축의 눈금을 일치할지 여부\np.share(x=False, y=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFacet과 Color 중 어떤 방식으로 표현하는 것이 유리한가? 밸런스를 잘 선택!\n\n\n\nleft = (\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .facet(col=\"species\")\n    .add(so.Dot(alpha=.5))\n)\nright = (\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\", color=\"species\")\n    .add(so.Dot(alpha=.5))\n)\n\nbottom = (\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .facet(row=\"species\")\n    .add(so.Dot(alpha=.5))\n)\n\ndisplay(left, right, bottom)\n\n\n\n\n\n\n\n\n\nfacetting; horizontal\n\n\n\n\n\n\n\ncolors\n\n\n\n\n\n\n\n\n\nfacetting; vertical\n\n\n\n\n\n\nFigure 1: Facetting by row, column, or color\n\n\n\n\nPairing\nFaceting이 변수 내에 다른 레벨에 따라 그려지는데 반해,\nparing은 x, y축에 다른 변수를 지정하여 그림\n\n(\n    so.Plot(penguins, y=\"body_mass_g\", color=\"species\")  # y축은 공유\n    .pair(x=[\"bill_length_mm\", \"bill_depth_mm\"])  # x축에 다른 변수를 mapping\n    .add(so.Dots())  # .Dots()! overploting에 유리. .Dot(alpha=.)로도 비슷\n)\n\n\n\n\n\n\n\n\nFacet & pair 동시\n\n(\n    so.Plot(penguins, y=\"body_mass_g\", color=\"sex\")\n    .pair(x=[\"bill_length_mm\", \"bill_depth_mm\"])\n    .facet(row=\"species\")\n    .add(so.Dots())\n)\n\n\n\n\n\n\n\n\n\n\nMultiple plots\n개발 중? Matplotlib을 통해 구현\n\nimport matplotlib as mpl\n\nf = mpl.figure.Figure(figsize=(8, 4))\nsf1, sf2 = f.subfigures(1, 2)\n(\n    so.Plot(penguins, x=\"body_mass_g\", y=\"flipper_length_mm\")\n    .add(so.Dots())\n    .on(sf1)\n    .plot()\n)\n(\n    so.Plot(penguins, x=\"bill_length_mm\", y=\"flipper_length_mm\")\n    .facet(row=\"sex\")\n    .add(so.Dots())\n    .on(sf2)\n    .plot()\n)\n\n\n\n\n\n\n\n\n여러 변수들에 대해 loop를 통해 플롯을 그리려면,\n\nf = mpl.figure.Figure(figsize=(6, 6), layout=\"constrained\")\nsubfigs = f.subfigures(2, 2)\n\nXs = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\nfor sf, xvar in zip(subfigs.flatten(), Xs):\n    p = (\n            so.Plot(penguins, x=xvar, y=\"body_mass_g\", color=\"species\")\n            .add(so.Dots())\n            .on(sf)\n            .plot()\n        )\np\n\n\n\n\n\n\n\n\n플롯 파일로 저장하기\np.save(\"data/filename.png\") # p: a plot oject",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#geometric-objects",
    "href": "contents/Visualize/plots.html#geometric-objects",
    "title": "Plots",
    "section": "Geometric objects",
    "text": "Geometric objects\n\nDot marks: Dot, Dots\nLine marks: Line, Lines, Path, Paths, Dash, Range\nBar marks: Bar, Bars\nFill marks: Area, Band\nText marks: Text\n\nAPI reference: Mark objects",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#statistical-transformations",
    "href": "contents/Visualize/plots.html#statistical-transformations",
    "title": "Plots",
    "section": "Statistical transformations",
    "text": "Statistical transformations\nAgg, Est, Count, Hist, KDE, Perc, PolyFit\n\n\n\n\n\n\nImportant\n\n\n\n위의 통계 함수들을 이용하여 변형된 데이터 값을 geometric objects에 mapping하여 다양한 플랏을 그릴 수 있음\n원칙적으로는 직접 통계치을 계산한 후에 그 데이터로 플랏을 그릴 수 있으나, 신속한 탐색적 분석을 위해 사용\n\n\n\n\n\n\n\n\nNote\n\n\n\n현재 seaborn.objects에서 다음 두 가지 중요한 statistical transformations이 제공되지 않고 있음\n\n(non-parametirc) fitted line을 보여주는 loess or GAM line\n분포의 간략한 summary인 boxplot\n\n이 부분에 대해서는 아래 몇 가지 대안이 있음; 아래에서 설명\n\n\n\n\n\n\n\n\nFitted Lines 구하기: Machine Learning Algorithms\n\n\n\nData에 fitted line를 구하는 방식에는 여러 방법이 있음\n\nLinear fit: 1차 함수형태로 fit\nSmoothing fit\n\nPolynominal fit: n차 다항함수형태로 fit\nGAM: generalized additive model\n\nSpline: piece-wise polynominal regression\n\nLoess/lowess: locally estimated/weighted scatterplot smoothing\n\n\n나중에 좀 더 자세히 알아봄\n현재 seaborn.objects에서는 polynomial fit만 제공\n\n\n\nFitted lines\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot())\n    .add(so.Line(), so.PolyFit(5))  # PolyFit(n): n차 다항식으로 fit\n)\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Line(), so.PolyFit(5))  # PolyFit(n): n차 다항식으로 fit\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\", color=\"drv\")  # color mapping이 이후 모든 layer에 적용\n    .add(so.Dot())\n    .add(so.Line(), so.PolyFit(5))\n)\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot(), color=\"drv\")  # color mapping이 이 layer에만 적용\n    .add(so.Line(), so.PolyFit(5))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) color가 모든 layers에 적용: global mapping\n\n\n\n\n\n\n\n\n\n\n\n(b) color가 두번째 layer에만 적용: local mapping\n\n\n\n\n\n\n\nFigure 2: Inherited mapping\n\n\n\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot(), color=\"drv\")\n    .add(so.Line(), so.PolyFit(5), group=\"drv\") # color가 아닌 group으로 grouping\n)\n# 다항함수 fit의 특징 및 주의점\n\n\n\n\n\n\n\n\nLinear fit vs. smoothing fit:\n선형적인 트렌드에서 얼마나 벗어나는가?\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot(color=\".6\"))\n    .add(so.Line(), so.PolyFit(5))\n    .add(so.Line(), so.PolyFit(1))\n)",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#seaborn.objects-요약",
    "href": "contents/Visualize/plots.html#seaborn.objects-요약",
    "title": "Plots",
    "section": "Seaborn.objects 요약",
    "text": "Seaborn.objects 요약\n(\n    so.Plot(df, x=, y=, color=, ...)  # global mapping\n    .add(so.Dot(color=, pointsize=,...))  # mark object + setting properties\n    .add(so.Line(), x=, y=, color=, ...)  # local mapping\n    .add(so.Line(), so.Polyfit(5))  # 통계적으로 변환한 값을 Line plot으로 표현\n    .add(so.Bar(), so.Hist(stat=\"proportion\"))  # 통계적으로 변환한 값을 Bar plot로 표현\n    ...\n    .facet(col=, row=, wrap=) # 카테고리의 levels에 따라 나누어 표현\n)\n\nAesthetic mapping\n\n위치(position): x축, y축\n색(color), 크기(pointsize), 모양(marker), 선 종류(linestyle), 투명도(alpha)\nglobal vs. local mapping\n\nGeometric objects\n\nDot marks: Dot, Dots\nLine marks: Line, Path, Dash, Range\nBar marks: Bar, Bars\nFill marks: Area, Band\nText marks: Text\n\nSetting properties\n\nMarks (.Dot(), .Line(), .Bar(), …) 내부에 속성을 지정하고, marks마다 설정할 수 있는 속성이 다름.\n주로 쓰이는 속성들: color, pointsize, alpha\n\nStatistical transformations\n\n변수들을 통계적 변환 후 그 값을 이용\nAgg, Est, Count, Hist, KDE, Perc, PolyFit\n\nFaceting: 카테고리 변수들의 levels에 따라 나누어 그림",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#visualizing-distributions",
    "href": "contents/Visualize/plots.html#visualizing-distributions",
    "title": "Plots",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\n분포를 살펴보는데 변수가 연속인지 카테고리인지에 따라 다른 방식\n\nA categorical variable\n\ntips = sns.load_dataset(\"tips\")\ntips.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\n(\n    so.Plot(tips, x=\"day\")\n    .add(so.Bar(), so.Count())  # category type의 변수는 순서가 존재. \n                                # 그렇지 않은 경우 알바벳 순서로. \n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n복잡한 통계치의 경우 직접 구한후 plot을 그리는 것이 용이\ncount_day = tips.value_counts(\"day\", normalize=True).reset_index(name=\"pct\")\n#     day  pct\n# 0   Sat 0.36\n# 1   Sun 0.31\n# 2  Thur 0.25\n# 3   Fri 0.08\n(\n    so.Plot(count_day, x=\"day\", y=\"pct\")\n    .add(so.Bar())\n)\n\n\npenguins = sns.load_dataset(\"penguins\") # load a dataset: penguins\n\n# Species에 inherent order가 없음; 알파벳 순으로 정렬\n(\n    so.Plot(penguins, x=\"species\")\n    .add(so.Bar(), so.Count())\n)\n\n\n\n\n\n\n\n(\n    so.Plot(penguins, x=\"species\")\n    .add(so.Bar(), so.Hist(\"proportion\"))  # Hist()의 default는 stat=\"count\"\n)\n\n# grouping의 처리에 대해서는 뒤에... 에를 들어, color=\"sex\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n표시 순서를 변경하는 일은 의미있는 플랏을 만드는데 중요\n나중에 좀 더 자세히 다룸\n\n\n\n# value_counts()는 크기대로 sorting!\nreorder = penguins.value_counts(\"species\").index.values\n#&gt; array(['Adelie', 'Gentoo', 'Chinstrap'], dtype=object)\n\n(\n    so.Plot(penguins, x=\"species\")\n    .add(so.Bar(), so.Count())\n    .scale(x=so.Nominal(order=reorder))  # x축의 카테고리 순서를 변경\n)\n\n# 직접 개수를 구해 그리는 경우, 테이블의 순서대로 그려짐\n(\n    so.Plot(penguins.value_counts(\"species\").reset_index(), \n            x=\"species\", y=\"count\")\n    .add(so.Bar())\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA numerical variable\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .add(so.Bars(), so.Hist())  # Histogram; x값을 bins으로 나누어 count를 계산!\n                                # .Bars()는 .Bar()에 비해 연속변수에 더 적합: 얇은 경계선으로 나란히 붙혀서 그려짐\n)\n\n\n\n\n\n\n\n\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .add(so.Bars(), so.Hist(binwidth=100))  # binwidth vs. bins\n)\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .add(so.Bars(), so.Hist(bins=10))  # binwidth vs. bins\n)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) binwidth=100\n\n\n\n\n\n\n\n\n\n\n\n(b) bins=10\n\n\n\n\n\n\n\nFigure 3: binwidth vs. bins\n\n\n\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .add(so.Bars(), so.Hist(\"proportion\"))  # 비율을 계산; stat=\"count\"가 default\n)\n\n\n\n\n\n\n\n# Density plot: 넓이가 1이 되도록\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .add(so.Area(), so.KDE())  # Density plot\n)\n\n\n\n\n\n\n\n# Density plot: 넓이가 1이 되도록\n(\n    so.Plot(penguins, x=\"body_mass_g\")\n    .add(so.Line(color=\"orange\"), so.KDE(bw_adjust=.2))  # Density bandwidth: binwidth에 대응\n    .add(so.Bars(alpha=.3), so.Hist(\"density\", binwidth=100))  # stat=\"density\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\npandas의 method를 이용한 여러 히스토그램 그리기\n\n\n\npandas의 hist() method: 모든 연속 변수에 대해 histogram을 그림\npenguins.hist(bins=20);  # 파라미터 figsize=(6, 5)\n\n# 세미콜론(;) 대신 plt.show() 쓸수도 있음\n\n\n\n\n\n\n\n\n\nseaborn 스타일로 histogram 그리기\n\n\n\nsns.set_theme()\npenguins.hist(bins=20);\nsns.set_theme(): set aspects of the visual theme for all matplotlib and seaborn plots.\nhist() 메서드가 matplotlib 대신 seaborn 스타일로 그려짐.",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#visualizing-relationships",
    "href": "contents/Visualize/plots.html#visualizing-relationships",
    "title": "Plots",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\n\nA numerical and a categorical variable\n\nBoxplot\nGrouped distribution: histogram, frequency polygon, density plot\n\nBoxplot\n\nsource: R for Data Science\n\n\n\n\n\n\n\n\n\n\n각각에 상응하는 분포(KDE, kernel density plot)\n\n한쪽으로 쏠린 정도; 왜도(skewness)\n퍼져 있는 정도 정도; 표준편차\n\n정규분포로부터 벗어나는 정도: 첨도(kurtosis)\n\n\n\n\n\n\n\n\n\n\n\n\nSeaborn 함수인 boxplot()을 이용하면,\nsns.boxplot(penguins, x=\"species\", y=\"body_mass_g\", fill=False);  # fill: box의 색을 채울지 여부\n\n\n\n\n\n\n\nSeaborn.object를 활용하면,\n(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\")\n    .add(so.Dot(pointsize=8), so.Agg(\"median\"))  # Agg(): aggregation, default는 mean\n    .add(so.Range(), so.Est(errorbar=(\"pi\", 50)))   # Range(): 기본 min/max range, \n                                                    # Est(): estimator\n)\n\n\n\n\n\n\n\n(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\")\n    .add(so.Dots(color=\".5\"), so.Jitter()) # so.Jitter(): 흐트려뜨려 그리기\n    .add(so.Dot(pointsize=8), so.Agg(\"median\"))\n    .add(so.Range(), so.Est(errorbar=(\"pi\", 50)))\n)\n\n\n\n\n\n\n\nError bars에 대해서는 seaborn/statistical estimation and error bars\n\n(\n    so.Plot(penguins, x=\"species\", y=\"body_mass_g\", color=\"sex\")\n    .add(so.Dots(), so.Jitter(), so.Dodge())\n    .add(so.Dot(pointsize=5), so.Agg(\"median\"), so.Dodge())\n    .add(so.Range(), so.Est(errorbar=(\"pi\", 50)), so.Dodge())\n)\n\n\n\n\n\n\n\n\n# seabron boxplot()에서는 hue로 color mapping\nsns.boxplot(penguins, x=\"species\", y=\"body_mass_g\", hue=\"sex\", fill=False);\n\n\n\n\n\n\n\n\n# facetting을 하려면 catplot()을 이용\nsns.catplot(\n    data=penguins, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"island\",\n    kind=\"box\", fill=False, height=3.5, aspect=.7,\n);\n\n\n\n\n\n\n\n\n\n# Build a boxplot!\ndef boxplot(df, x, y, color=None, alpha=0.1):\n    return (\n        so.Plot(df, x=x, y=y, color=color)\n        .add(so.Dots(alpha=alpha, color=\".6\"), so.Jitter(), so.Dodge())\n        .add(so.Range(), so.Est(errorbar=(\"pi\", 50)), so.Dodge())\n        .add(so.Dots(pointsize=8, marker=\"&lt;\"), so.Agg(\"median\"), so.Dodge())\n        .scale(color=\"Dark2\")\n        .theme({**sns.axes_style(\"whitegrid\")})\n    )\n\n(\n    boxplot(penguins, x=\"species\", y=\"flipper_length_mm\", color=\"sex\")\n    .facet(\"island\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n# Build a rangeplot!\ndef rangeplot(df, x, y, color=None):\n    return (\n        so.Plot(df, x=x, y=y, color=color)\n        .add(so.Range(), so.Est(errorbar=(\"pi\", 50)), so.Dodge())\n        .add(so.Dots(pointsize=8, marker=\"&lt;\"), so.Agg(\"median\"), so.Dodge())\n        .scale(color=\"Dark2\")\n        .theme({**sns.axes_style(\"whitegrid\")})\n    )\n\n(\n    rangeplot(penguins, x=\"species\", y=\"flipper_length_mm\", color=\"sex\")\n    .facet(\"island\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\nHistogram\n# 별로 유용하지 못함\n(\n    so.Plot(penguins, x=\"body_mass_g\", color=\"species\")\n    .add(so.Bars(), so.Hist(common_bins=False))  # bins을 공유하지 않도록\n)\n# Hist(): 다양한 parameter가 있음.\n\n\n\n\n\n\n\nFrequency polygon\n(\n    so.Plot(penguins, x=\"body_mass_g\", color=\"species\")\n    .add(so.Line(marker=\".\"), so.Hist(binwidth=200))  # Line에 maker \".\"을 표시\n)\n\n\n\n\n\n\n\n# 비율로 표시 & 각 카테고리/레벨 별로 비율 계산\n(\n    so.Plot(penguins, x=\"body_mass_g\", color=\"species\")\n    .add(\n        so.Line(marker=\".\"), \n        so.Hist(binwidth=200, stat=\"proportion\", common_norm=False)\n    )  \n)\n\n\n\n\n\n\n\nDensity plot\n(\n    so.Plot(penguins, x=\"body_mass_g\", color=\"species\")\n    .add(so.Area(), so.KDE(common_norm=False))  # Density plot, species별로 넓이가 1이 되도록\n)\n\n\n\n\n\n\n\n\n\nTwo categorical variables\np = so.Plot(penguins, x=\"island\", color=\"species\")\np.add(so.Bar(), so.Count()) # Bar() mark + Count() transformation\n\n\n\n\n\n\n\n\np.add(so.Bar(), so.Count(), so.Dodge())   # 나란히 표시\np.add(so.Bar(), so.Count(), so.Stack())  # stacking\n\n\n\n\n\n\n\n\n\n\n\n\n(a) dodge\n\n\n\n\n\n\n\n\n\n\n\n(b) stack\n\n\n\n\n\n\n\nFigure 4: dodge vs. stack\n\n\n\nCount 대신 proportion을 표시하는 경우: Hist()를 사용\nCount()는 Hist(stat=\"count\")와 동일함.\n# 각 비율값의 합이 1이 되도록, 즉 모든 카테고리에 걸쳐 normalize\np.add(\n    so.Bar(width=.5), \n    so.Hist(\"proportion\"),  # proportion; stat=\"count\"로 하면 앞서 so.Count()와 동일\n    so.Stack()  # stacking\n)\n\n\n\n\n\n\n\n# x축 기준으로 normalize\np.add(\n    so.Bar(width=.5), \n    so.Hist(\"proportion\", common_norm=[\"x\"]),  # proportion; \n    so.Stack()  # stacking\n)\n# warning이 뜰 수 있음!\n\n\n\n\n\n\n\n\n# x축, facet의 column 기준으로 normalize\np.add(\n    so.Bar(width=.8), \n    so.Hist(\"proportion\", common_norm=[\"x\", \"col\"]),  # proportion\n    so.Stack(),  # stacking\n).facet(col=\"sex\")  # faceting\n\n\n\n\n\n\n\n\ncommon_norm: True vs. False 비교\n\n# 각 비율값의 합이 1이 되도록, 즉 모든 카테고리에 걸쳐 normalize\np.add(\n    so.Bar(width=.5), so.Hist(\"proportion\", common_norm=True),  # default\n    so.Stack()\n)\n\n# 각 species별로 normalize \np.add(\n    so.Bar(width=.5), so.Hist(\"proportion\", common_norm=False),\n    so.Stack()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo numerical variables\nScatterplot\n(\n    so.Plot(penguins, x=\"flipper_length_mm\", y=\"body_mass_g\")\n    .add(so.Dot())  # overplotting에는 so.Dots()가 유리 \n)\n\n\n\n\n\n\n\n\n\nThree or more variables\n\n(\n    so.Plot(penguins, x=\"flipper_length_mm\", y=\"body_mass_g\",\n            color=\"species\", marker=\"island\")\n    .add(so.Dot())\n    .layout(size=(6, 4))  # plot size 조정\n)\n\n\n\n\n\n\n\n\nFacet의 활용\n\n(\n    so.Plot(penguins, x=\"flipper_length_mm\", y=\"body_mass_g\",\n            color=\"species\")\n    .add(so.Dot(alpha=.5))\n    .facet(\"island\")\n    .layout(size=(8, 4))\n)\n\n\n\n\n\n\n\n\n\n\nTime series\n\nhealthexp = sns.load_dataset(\"healthexp\")\n\n(\n    so.Plot(healthexp, x=\"Year\", y=\"Spending_USD\", color=\"Country\")\n    .add(so.Lines())\n)\n\n\n\n\n\n\n\n\n\n# 전체와 각 그룹의 상태를 동시에 파악\n(\n    so.Plot(healthexp, x=\"Year\", y=\"Spending_USD\", color=\"Country\")\n    .add(so.Area(), so.Stack()) # add stacking\n)\n\n\n\n\n\n\n\n\n\n(\n    so.Plot(healthexp, x=\"Year\", y=\"Life_Expectancy\")\n    .add(so.Line(alpha=.3), group=\"Country\", col=None)\n    .add(so.Line(linewidth=3))\n    .facet(\"Country\", wrap=3)  # wrap!!!\n)\n\n\n\n\n\n\n\n\n\nfmri = sns.load_dataset(\"fmri\")\n\np = so.Plot(fmri, x=\"timepoint\", y=\"signal\", color=\"region\", linestyle=\"event\")\np.add(so.Line(), so.Agg())  # Agg()의 default 함수는 mean\n\n\n\n\n\n\n\n\n\np.add(so.Line(marker=\"o\", edgecolor=\"w\"), so.Agg(), linestyle=None)  # linestyle을 overwrite!",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#overploting",
    "href": "contents/Visualize/plots.html#overploting",
    "title": "Plots",
    "section": "Overploting",
    "text": "Overploting\n대표적으로 다음과 같은 방식으로 해결할 수 있음.\n\nalpha property: 투명도를 조절\n\nso.Jitter() mark: 흐트려뜨려 그리기\n\nso.Dots() mark: 불투명, 테두리 선명한 점들\n\n.facet() facet: 다른 면에 그리기\n\n특별히 overplotting에 특화된 독립적인 plots도 있음. 예를 들어,\nBeeswarm plot: 겹치지 않게 그리기\n\nsns.catplot(\n    data=penguins, kind=\"swarm\",\n    x=\"species\", y=\"body_mass_g\", hue=\"sex\", col=\"island\",\n    height=4.5, aspect=.6\n);\n\n\n\n\n\n\n\n\n2d histogram/hexbin plot: x, y모두 binning하여 상대적 개수를 컬러로 표시\n\nsns.jointplot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"hex\", gridsize=30, height=5);  # gridsize: bin 개수\n\n\n\n\n\n\n\n\n\nMatplotlib을 이용\n\nplt.figure(figsize=(6, 4), dpi=100)\nplt.hexbin(x=penguins[\"bill_depth_mm\"], y=penguins[\"body_mass_g\"], gridsize=30, cmap=\"Blues\")\n\nplt.colorbar()\nplt.xlabel(\"Bill Depth (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.show()",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#new-data",
    "href": "contents/Visualize/plots.html#new-data",
    "title": "Plots",
    "section": "New data",
    "text": "New data\n새로운 데이터 값을 이용하고자 할 때, 변수이름 대신 직접 데이터(series, array, list, …) 입력\n\nmpg_suv = mpg.query('`class` == \"suv\"')\n\n(\n    so.Plot(mpg, x=\"displ\", y=\"hwy\")\n    .add(so.Dot(), color=\"class\")\n    .add(so.Line(), so.PolyFit(5), \n         x=mpg_suv[\"displ\"], y=mpg_suv[\"hwy\"])\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n원칙적으로 다음과 같이 matplotlib의 스타일로 데이터를 직접 입력해도 됨.\n(\n    so.Plot()\n    .add(so.Dot(), x=mpg[\"displ\"], y=mpg[\"hwy\"], color=mpg[\"class\"])\n)\n\n\n\n\n\n\n\n\n\nMatplotlib\n\n\n\nMatplotlib에 대해서는 다음 교재의 4.Visualization with Matplotlib를 참고\nPython Data Science Handbook by Jake VanderPlas\n\n각 변수의 값을 직접 입력: Series나 NumPy array\n두 가지 interface를 제공\n\nMATLAB 스타일로 직접 함수를 호출하는 방법\nplt.scatter(x=tips[\"total_bill\"], y=tips[\"tip\"])\n객체를 만들어서 메서드를 호출하는 방법\nfig, ax = plt.subplots()\nax.scatter(x=tips[\"total_bill\"], y=tips[\"tip\"])",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/Visualize/plots.html#exercises",
    "href": "contents/Visualize/plots.html#exercises",
    "title": "Plots",
    "section": "Exercises",
    "text": "Exercises\n다음 데이터로 위에서 다룬 시각화를 연습해보세요.\nData on houses in Saratoga County, New York, USA in 2006\nhouses_data = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\")\n\nhouses = houses_data.data   # data\nprint(houses_data.__doc__)  # description of the data\n\nhouses\n\n       price  lotSize  age  landValue  livingArea  pctCollege  bedrooms  \\\n0     132500     0.09   42      50000         906          35         2   \n1     181115     0.92    0      22300        1953          51         3   \n2     109000     0.19  133       7300        1944          51         4   \n...      ...      ...  ...        ...         ...         ...       ...   \n1725  194900     0.39    9      20400        1099          51         2   \n1726  125000     0.24   48      16800        1225          51         3   \n1727  111300     0.59   86      26000        1959          51         3   \n\n      fireplaces  bathrooms  rooms          heating      fuel  \\\n0              1       1.00      5         electric  electric   \n1              0       2.50      6  hot water/steam       gas   \n2              1       1.00      8  hot water/steam       gas   \n...          ...        ...    ...              ...       ...   \n1725           0       1.00      3          hot air       gas   \n1726           1       1.00      7          hot air       gas   \n1727           0       1.00      6          hot air       gas   \n\n                  sewer waterfront newConstruction centralAir  \n0                septic         No              No         No  \n1                septic         No              No         No  \n2     public/commercial         No              No         No  \n...                 ...        ...             ...        ...  \n1725  public/commercial         No              No         No  \n1726  public/commercial         No              No         No  \n1727             septic         No              No         No  \n\n[1728 rows x 16 columns]",
    "crumbs": [
      "Visualize",
      "Plots"
    ]
  },
  {
    "objectID": "contents/actfinal5122.html",
    "href": "contents/actfinal5122.html",
    "title": "Final Exam",
    "section": "",
    "text": "총점 250"
  },
  {
    "objectID": "contents/actfinal5122.html#a.-다음은-houses-in-saratoga-county-2006-saratogahouses를-이용한-질문들입니다.",
    "href": "contents/actfinal5122.html#a.-다음은-houses-in-saratoga-county-2006-saratogahouses를-이용한-질문들입니다.",
    "title": "Final Exam",
    "section": "A. 다음은 Houses in Saratoga County (2006) SaratogaHouses를 이용한 질문들입니다.",
    "text": "A. 다음은 Houses in Saratoga County (2006) SaratogaHouses를 이용한 질문들입니다.\n\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\n\n# 변수 설명\n# print(sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").__doc__)\n\n\n직접 다운로드 링크: SaratogaHouses.csv\n\n\nA-1. 우선, 데이터셋 houses을 다음과 같이 필터링하여 이후 계속 사용합니다. (5)\n\n집의 연령(age)은 100년(포함) 이하이고,\n욕실의 개수(bathrooms): 1개인 이상(&gt; 0), 4개 미만(&lt; 4)인 것으로 필터링하여, 이후 계속 사용합니다.\n\n\n\nA-2. 집값(price)에 가장 크게 영향을 주는 것은 거주공간의 넓이(livingArea)인 것으로 보이는데, (20)\n\n그 관계가 대략 선형인지를 확인할 수 있는 시각화를 하고 (scatterplot과 polynominal fit을 이용),\n\n거주공간의 넓이(livingArea)로 집값(price)을 예측하는 선형모형을 세우고,\n\n거주공간의 넓이가 100 (square feet) 증가할 때마다 집값($)이 얼마나 증가하는지 기술하고,\n\n거주공간의 넓이가 집값 변량의 몇 %를 설명해주는지 기술해보세요.\n\n\n\n집의 연령과 집값의 관계\n\n\n\n\n\n\n오래된 집일수록 가격이 낮아지는 현상이 있는데, 이는 집이 낡아서라기보다는 오래된 집들의 거주공간이 작은 경향이 있어, 간접적으로 집값이 낮아보이는 것일 수 있음을 살펴보고자 합니다.\n다시 말하면, 거주공간의 넓이를 고려한 후 (혹은 거주공간의 넓이와는 독립적인) 집의 연령과 집값의 관계를 살펴봅니다.\n(오른쪽 도식을 참고하세요. )\n\n\n\n\n\n\n\nA-3. 우선, 집의 연령(age)에 따른 집값(price)의 변화를 살펴보는데, (10)\n\n시각화를 이용해 그 관계를 파악하고; scatterplot과 polynominal fit을 이용\n대략적인 경향성을 기술하고, 선형관계로 모형을 만드는 것이 적절한지 기술해보세요.\n\n\n\nA-4. 이번에는 오래된 집일수록 거주공간도 작다는 것을 확인하는데,\n\n아래와 같이 집의 연령(age)에 따른 거주공간의 넓이(livingArea)와 집값(price)의 변화를 나란히 비교하도록 플롯을 그려보세요. (20)\n\n.pair(y=['livingArea', 'price'])을 이용하고,\n관계를 파악할 수 있도록 scatterplot과 polynominal fit들을 추가하고,\n.layout()를 이용하여, 플롯의 크기를 적절히 조정하세요.\n\n이 플롯을 보고, 오래된 집의 가격이 낮아지는 현상에 대한 가능한 설명을 기술해보세요.\n\n\n\n\n\n\n\n\n\n\n\n\nA-5. 구체적인 선형모형을 세워 위 가설을 검증해보는데, 아래 세 모형에 대한 파라미터를 확인해보고, 파라미터의 변화를 기술하여 거주공간의 넓이(livingArea)를 고려하면 집의 연령(age)이 집값에 미치는 영향이 어떠할지 다음에 따라 알아봅니다. (20)\nimport statsmodels.formula.api as smf\n\nmod1 = smf.ols('price ~ livingArea', data = houses).fit()\nmod2 = smf.ols('price ~ age', data = houses).fit()\nmod3 = smf.ols('price ~ livingArea + age', data = houses).fit()\n\n\n\n\n\n\n\n집의 연령이 1년 늘때 집값이 떨어지는 정도가 mod2와 비교해서 mod3에서 추정할 때, 어떻게 바뀌는지 기술해보세요.\n반면, 거주공간의 넓이가 1(square feet) 늘때 집값이 올라가는 정도가 mod1와 비교해서 mod3에서 추정할 때 어떻게 바뀌는지 기술해보세요.\n왜 이런 변화가 생겼을지 앞서 알아본 (A-1) ~ (A-4)의 내용과 옆 도식을 참고해서 간단히 설명해보세요.\n\n\n\n\n\n\n\n\n\nA-6. 다른 한편으로는 오래된 집일수록 집값이 떨어지는 것처럼 보이는 것은, 큰 집들 혹은 비싼 집들에서 심하게 나타나는 현상일 수도 있음을 살펴보고자 합니다. 다시 말하면, 작은 집들의 경우 오래되었다고 집값이 떨어지지 않을 수 있습니다. (15)\n\n우선, 거주공간의 넓이(livingArea)를 6개의 구간으로 나누어, 각 구간별로 집의 연령(age)에 따른 집값(price)의 변화를 scatterplot과 polynominal fit을 이용해 다음과 같이 확인합니다. (디테일 무시)\n\npd.qcut()을 이용하고,\n집의 연령(age)을 0 ~ 50년까지로 제한해서 자세히 살펴봅니다.\n.layout()를 이용하여, 플롯의 크기를 적절히 조정하세요.\nfitted line은 3차 다항함수로 그립니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA-7. 위의 현상을 모델링하기 위해서 다음과 같이 상호작용(interaction)을 위한 추가항을 모형에 최소한으로 포함시켜야 합니다.\nsmf.ols('price ~ livingArea * age', data = houses)\n# 또는 smf.ols('price ~ livingArea + age + livingArea:age', data = houses)\n\n두 예측변수의 다음 값들로 grid을 만들어, 위 모형이 예측하는 값을 아래와 같이 시각화해 보고, (디테일은 무시)\n오래된 집일수록 집값이 낮아보였던 처음의 관찰에 대해 어떻게 해석할 수 있을지 간략히 설명해보세요. (20)\n\ngrid를 만들기 위해 itertools 패키지의 product() 함수를 이용합니다.\n\n\n# age: 0년에서 100년까지 1년 단위로\nage = np.arange(0, 101)\n\n# 거주공간의 넓이: 분포에서 10%, 25%, 50%, 75%, 90% 지점의 값들\nlivingArea = np.quantile(houses[\"livingArea\"], [0.10, 0.25, 0.50, 0.75, 0.90])\n\n\n\n\n\n\n\n\n\n\n아래와 같이 livingArea와 age의 위치를 바꿔 시각화해보고,\n거주공간이 넓어질수록 가격이 높아지는 경향이 집의 연식에 따라 어떻게 달라질 수 있는지 간략히 기술해보세요. (5)\n\n\n\n\n\n\n\n\n\n\n\n\nA-8. (A-7)에서 집값(price)을 거주공간의 넓이(livingArea)와 집의 연령(age)으로 예측하는 모형을 세웠는데, 이 두 변수로 설명되지 않는 집값의 변량을 욕실의 수(bathrooms)가 얼마나, 어떻게 추가로 설명할 수 있을지 살펴보고자 합니다. (20)\n\n우선, (A-7)의 모형으로 설명되지 않는 집값(price)의 변량, 즉 residualized된 집값을 resid라는 변수로 추가하고,\n다음과 같이, 거주공간의 넓이(livingArea)와 residualized된 집값(resid)의 관계를 욕실의 수(bathrooms)에 따라 나누어 살펴봅니다. (디테일은 무시)\n\nfitted line은 1차 다항함수로 그립니다.\ny축은 (-100,000, 100,000)로 제한해 확대해서 봅니다.\n\n욕실의 수가 적을 때를 중심으로 아래 플롯이 암시하는 바를 기술해보세요.\n\nY축은 거주공간의 넓이(livingArea)가 이미 residualized된 집값(resid)임을 고려하세요."
  },
  {
    "objectID": "contents/actfinal5122.html#b.-다음은-bike-sharing-in-washington-d.c.-dataset를-이용한-질문들입니다.",
    "href": "contents/actfinal5122.html#b.-다음은-bike-sharing-in-washington-d.c.-dataset를-이용한-질문들입니다.",
    "title": "Final Exam",
    "section": "B. 다음은 Bike Sharing in Washington D.C. Dataset를 이용한 질문들입니다.",
    "text": "B. 다음은 Bike Sharing in Washington D.C. Dataset를 이용한 질문들입니다.\nData: hour.csv, day.csv\nDescriptions: Bike Sharing in Washington D.C. Dataset\n아래 코드를 이용해 cleaning 후\nbikes는 hourly 데이터셋, bikes_daily는 daily 데이터셋으로 사용합니다.\n\nbikes = pd.read_csv(\"../data/hour.csv\")\nbikes_daily = pd.read_csv(\"../data/day.csv\")\n\ndef clean_data(df):\n    df.rename({\"dteday\": \"date\", \"cnt\": \"count\"}, axis=1, inplace=True)\n\n    df = df.assign(\n        date=lambda x: pd.to_datetime(x[\"date\"]),  # datetime type으로 변환\n        year=lambda x: x[\"date\"].dt.year.astype(str),  # year 추출\n        day=lambda x: x[\"date\"].dt.day_of_year,  # day of the year 추출\n        month=lambda x: x[\"date\"].dt.month_name().str[:3],  # month 추출\n        wday=lambda x: x[\"date\"].dt.day_name().str[:3],  # 요일 추출\n    )\n    \n    return df\n\nbikes = clean_data(bikes)  # hourly data\nbikes_daily = clean_data(bikes_daily)  # daily data\n\n B-1 ~ B-5는 코딩없이 수업시간에 다룬 내용을 기술하는 문제입니다.\n\nB-1. 수업시간에 살펴본 바와 같이, 2011년의 daily 데이터에서, 다음과 같은 요일에 따른 자전거 대여량의 변화를 보았을 때, casual 사용자들과 registered 사용자들은 각각 어떤 특징의 집단이라고 추론해볼 수 있을지 간략히 기술해보세요. (10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB-2. 마찬가지로, 수업에서 살펴본 바와 같이,\n\n왼편 플롯처럼 월별에 따른 자전거 대여량이 변하는데, 다음과 같이 온도로 예측하는 모형을 세운 후\n\n   registered ~ temp + I(temp**2)\n\n그 잔차를 달에 따라 살펴봤을 때 (오른편 플롯), 어떤 추론을 할 수 있는지 온도와 달의 관계를 중심으로 기술해보세요. (10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB-3. 수업에서 설명한 바와 같이, casual bikers들에 대해서는 특히나 count 데이터의 특성이 왼편과 같이 잘 나타납니다.\n이때, 대여수를 log 스케일로 바꾸면 오른편과 같이 변하는 것을 보았을 때, log 스케일로 바꾸면 어떤 이점이 있는지 대략 설명해보세요. (10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB-4. 아래 플롯에서 다음과 같은 모델의 잔차를 요일별로 살펴보았는데, 여기서 두드러진 특징은 무엇이며, 요일을 어떻게 모델에 포함시켜야 하는지에 대해 간략히 기술해보세요. (10)\nlcasual ~ (temp + I(temp**2)) + year\n\n\n\nB-5. 일년에 걸친 자전거 대여량의 변화가 2011년과 2012년이 매우 다름을 살펴보았는데, 오른쪽의 연간 온도의 변화를 함께 살펴본 이유는 무엇이며, 결론적으로 2011년과 2012년의 데이터를 어떻게 이용해야 할지 간단히 기술해보세요. (10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이후부터는 2012년 hourly 데이터셋(bikes)에 대해, 자전거 대여수를 log 변환한 값을 다음과 같이 가공한 후 진행합니다.\n\nbikes = (\n    bikes.query('year == \"2012\"')  # 2012년 데이터만 사용\n    .assign(\n        lregistered = np.log2(bikes[\"registered\"] + 1),  # log2 변환\n        lcasual = np.log2(bikes[\"casual\"] + 1),          # log2 변환\n    )\n)\n\n\n\nB-6. 자전거 대여수가 하루의 시간(hr)에 따라 어떻게 변하는지 rangeplot()을 통해 살펴보세요. (10)\n\n이 때, 등록된 사용자들(lregistered)과 비등록사용자들(lcasual)을 따로 살펴보고,\n이 두 그룹을 따로 분석해야 하는지에 대해 간단히 기술해보세요.\n\n아래 rangeplot() 함수를 써도 되고, 자신만의 rangeplot 함수를 써도 됩니다.\n\n\n\n\n\n\nrangeplot()\n\n\n\n\n\ndef rangeplot(df, x, y, color=None, marker=\"&lt;\"):\n\n    return (\n        so.Plot(df, x=x, y=y, color=color)\n        .add(so.Range(), so.Est(errorbar=(\"pi\", 50)), so.Dodge())\n        .add(so.Dot(pointsize=8, marker=marker), so.Agg(\"median\"), so.Dodge())\n        .scale(color=\"Dark2\")\n        .theme({**sns.axes_style(\"whitegrid\")})\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이제부터는 등록된 사용자들(lregistered)에 대해서만 분석합니다.\n\n\nB-7. (B-4)에서 살펴본 관계가 요일(wday)에 따라 다른지 rangeplot()으로 다음과 같이 facetting하여 나누어 살펴보는데 (10)\n\n주말과 평일이 뚜렷이 구분되는 패턴으로 나타남을 확인하고,\n\n\n\n\n\n\n\n\n\n\n\n공휴일은(holiday == 1) 요일에 상관없이 주말과 비슷한 패턴으로 나타남을 확인합니다.\n\n\n\n\n\n\n\n\n\n\n\n\nB-8. 이제 주말(토,일)과 공휴일이 0으로 표시되어있는 근무일 여부(workingday)를 다음과 같이 recode합니다. (15)\n\nworkingday가 1이면 “workday”로 표시하고, 0이면, “day-off”로 표시하도록 바꿉니다.\n확인차 workingday 여부에 따라 “하루 시간(hr)에 따른 자전거 대여수(lregistered)의 변화”를 마찬가지로 rangeplot()으로 아래와 같이 살펴보고,\n두드러진 특징을 간략히 기술하고, 왜 이런 패턴이 나타날지 대략 추측해보세요.\n\n\n\n\n\n\n\n\n\n\n\n\nB-9. 시간(hr)에 따른 자전거 대여수(lregistered)의 변화는 다항함수로는 fit하기 어려운 곡선의 형태로 나타나므로 이 패턴을 잘 잡아낼 수 있도록 natural spline으로 fit한 모형을 다음과 같이 만듭니다. (10)\n\nmod_ns = smf.ols(\"lregistered ~ cr(hr, df=10) * workingday\", data=bikes).fit()\n\n이 모형에 대해, 예측변수의 다음 값들에 대해 모형의 예측값을 그려보는데, lregistered가 아닌 원래 단위인 대여 수(registered)로 환원하여 예측값을 계산 후 아래와 같이 그려보세요. (디테일은 무시)\n\nlregistered는 \\(log_2{(\\text{registered} + 1)}\\)으로 변환되었습니다.\n\n\nhr = np.arange(0, 24, 5/60)  # 5분 단위로 24시간\nworkingday = [\"workday\", \"day-off\"]  # 근무일, 휴일\n\n\n\n\n\n\n\n\n\n\n\n\nB-10. 기온(temp)에 따른 자전거 대여수(lregistered)의 변화 패턴이 근무일 여부(workingday)에 따라 다른지를 살펴봅니다. (10)\n\n아래 플롯과 같이, 기온의 영향력에 대한 패턴이 근무일 여부에 따라 다른지 확인하여(5차 다항함수로)\n기온(temp)과 근무일 여부(workingday) 두 변수가 상호작용하는지 판단한 후, 위에서 세운 mod_ns 모형에 기온(temp) 변수를 추가한 모형 mod_ns2를 세웁니다.\n(최적은 아니지만) 2차 다항함수의 형태로 가정해 모형을 세워보세요.\n\n\n\n\n\n\n\n\n\n\n\n\nB-11. (B-10)에서 세운 모형(mod_ns2)으로는 “예측되지 않는 자전거 대여수(lregistered)”인 잔차를 구해 데이터에 추가한 후, (15)\n\n다음과 같이 달(month)에 따른 “잔차 평균”의 변화를 bar 플롯으로 살펴보고, 이 플롯이 의미하는 바를 기술해보세요.\n날씨(weathersit)에 따른 “잔차 평균”의 변화도 Bar 플롯으로 살펴본 후, 플롯이 의미하는 바를 기술해보세요.\n\n날씨(weathersit) 변수에 대한 설명:\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB-12. 자전거 대여수(lregistered)를 예측하는 모형에 예측변수를 추가하는 소위 “위계적 분석”을 다음과 같이 수행합니다. 예측변수들은 앞서 분석한 결과를 바탕으로 적절히 변형해서 모형에 추가합니다. (10)\n\nmod1: 시간(hr)만 포함한 모형    natural spline으로 추가(cr(hr, df=10))\nmod2: 시간(hr)과 근무일 여부(workingday)를 포함한 모형    상호작용 여부을 고려해서 추가\nmod3: 시간(hr), 근무일 여부(workingday), 온도(temp)를 포함한 모형    온도는 2차 다항함수로, 상호작용 여부을 고려해서 추가\n\n\n모형의 예측값을 원래 단위인 자전거 대여 수로 환산한 후, 원래 단위로 각 모형에 대한 \\(R^2\\)값을 표시하고,\n\n예측변수의 추가에 따른 \\(R^2\\)값의 증가분을 살펴보고, 각 예측변수들의 공헌도에 대해 간략히 기술하세요.\n\n\n\nB-13. Casual biker(lcasual)들에 대해서도 동일한 분석을 하는데, 모형은 B-12에서 registered bikers에 대한 모형과 동일하다고 가정하고, (5)\n\n모형의 예측값을 원래 단위인 자전거 대여 수로 환산한 후, 원래 단위로 각 모형에 대한 \\(R^2\\)값을 표시하고,\n예측변수의 추가에 따른 \\(R^2\\)값의 증가분을 살펴보고, 각 예측변수들의 공헌도에 대해 간략히 기술하세요.\n\n\n\nB-14. (B-12)과 (B-13)의 결과를 바탕으로 registered bikers와 casual bikers들 간에 자전거 이용에 대해 어떠한 차이점이 있을지 추측해보고, 간략히 기술해보세요. (5)"
  },
  {
    "objectID": "contents/actfinal512.html",
    "href": "contents/actfinal512.html",
    "title": "Final Exam",
    "section": "",
    "text": "총점 250"
  },
  {
    "objectID": "contents/actfinal512.html#a.-다음은-houses-in-saratoga-county-2006-saratogahouses를-이용한-질문들입니다.",
    "href": "contents/actfinal512.html#a.-다음은-houses-in-saratoga-county-2006-saratogahouses를-이용한-질문들입니다.",
    "title": "Final Exam",
    "section": "A. 다음은 Houses in Saratoga County (2006) SaratogaHouses를 이용한 질문들입니다.",
    "text": "A. 다음은 Houses in Saratoga County (2006) SaratogaHouses를 이용한 질문들입니다.\n\nhouses = sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").data\n\n# 변수 설명\n# print(sm.datasets.get_rdataset(\"SaratogaHouses\", \"mosaicData\").__doc__)\n\n\n직접 다운로드 링크: SaratogaHouses.csv"
  },
  {
    "objectID": "contents/actfinal512.html#b.-다음은-bike-sharing-in-washington-d.c.-dataset를-이용한-질문들입니다.",
    "href": "contents/actfinal512.html#b.-다음은-bike-sharing-in-washington-d.c.-dataset를-이용한-질문들입니다.",
    "title": "Final Exam",
    "section": "B. 다음은 Bike Sharing in Washington D.C. Dataset를 이용한 질문들입니다.",
    "text": "B. 다음은 Bike Sharing in Washington D.C. Dataset를 이용한 질문들입니다.\nData: hour.csv, day.csv\nDescriptions: Bike Sharing in Washington D.C. Dataset\n아래 코드를 이용해 cleaning 후\nbikes는 hourly 데이터셋, bikes_daily는 daily 데이터셋으로 사용합니다.\n\nbikes = pd.read_csv(\"../data/hour.csv\")\nbikes_daily = pd.read_csv(\"../data/day.csv\")\n\ndef clean_data(df):\n    df.rename({\"dteday\": \"date\", \"cnt\": \"count\"}, axis=1, inplace=True)\n\n    df = df.assign(\n        date=lambda x: pd.to_datetime(x[\"date\"]),  # datetime type으로 변환\n        year=lambda x: x[\"date\"].dt.year.astype(str),  # year 추출\n        day=lambda x: x[\"date\"].dt.day_of_year,  # day of the year 추출\n        month=lambda x: x[\"date\"].dt.month_name().str[:3],  # month 추출\n        wday=lambda x: x[\"date\"].dt.day_name().str[:3],  # 요일 추출\n    )\n    \n    return df\n\nbikes = clean_data(bikes)  # hourly data\nbikes_daily = clean_data(bikes_daily)  # daily data\n\n\nB6 이후부터는 2012년 hourly 데이터셋(bikes)에 대해, 자전거 대여수를 log 변환한 값을 다음과 같이 가공한 후 진행합니다.\n\nbikes = (\n    bikes.query('year == \"2012\"')  # 2012년 데이터만 사용\n    .assign(\n        lregistered = np.log2(bikes[\"registered\"] + 1),  # log2 변환\n        lcasual = np.log2(bikes[\"casual\"] + 1),          # log2 변환\n    )\n)\n\n\n\n\n\n\n\nrangeplot()\n\n\n\n\n\ndef rangeplot(df, x, y, color=None, marker=\"&lt;\"):\n\n    return (\n        so.Plot(df, x=x, y=y, color=color)\n        .add(so.Range(), so.Est(errorbar=(\"pi\", 50)), so.Dodge())\n        .add(so.Dot(pointsize=8, marker=marker), so.Agg(\"median\"), so.Dodge())\n        .scale(color=\"Dark2\")\n        .theme({**sns.axes_style(\"whitegrid\")})\n    )"
  }
]